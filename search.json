[{"title":"C++元编程教程00：模板基础回顾","url":"/2025/04/09/cpp-metaprogramming-00/","content":"C++模版基础回顾\r\n前言：为什么我们需要模版？\r\n在使用编程语言进行程序设计的时候，有这样一个概念会反复的出现，这就是抽象（abstraction）。以一个简单的返回两数最大值的函数max为例：\r\nauto max(int lhs, int rhs) -&gt; int &#123;\treturn lhs &gt; rhs ? lhs : rhs;&#125;\r\n这个函数可以看作对一类实际的表达式2 &gt; 1 ? 2 : 1进行了抽象，将1、2这样实际的数值抽象为了函数形参lhs、rhs，从而将代码逻辑从具体的表达式值中解耦。抽象的强大之处在于它能极大地减少重复代码，如果一段代码逻辑在我们所写的代码中反复的出现，将其抽象为一个函数可以有效的减少我们的工作量。\r\n在使用max函数时，我们可能会发现我们也需要对浮点数求最大值，这时候我们可能会想到通过重载函数的方式添加支持：\r\nauto max(float lhs, float rhs) -&gt; float &#123;\treturn lhs &gt; rhs ? lhs : rhs;&#125;auto max(double lhs, double rhs) -&gt; double &#123;\treturn lhs &gt; rhs ? lhs : rhs;&#125;\r\n添加一两个重载的工作量尚可接受，但是如果我们还想支持更多的数据类型，甚至是不同的数据类型之间的混合比较的时候，那么需要编写的代码很快就会增长到无法接受的程度。为了避免代码重复，我们就需要继续使用抽象，类似于前文将逻辑与具体的表达式值解耦类似，我们进一步将逻辑与具体的类型解耦。为此，我们需要使用模板(template)：\r\ntemplate &lt;typename T&gt;auto max(T lhs, T rhs) -&gt; T &#123;\treturn lhs &gt; rhs ? lhs : rhs;&#125;\r\n与函数参数类似，我们通过让类型也成为可变的形参来完成抽象。每当需要在某个类型的值上使用max函数（严谨来说，现在应该叫做函数模板）时，我们除了需要传入lhs、rhs之外，还需要传入它们的类型：\r\nauto main() -&gt; int &#123;\tint result1 = max&lt;int&gt;(1, 2);\tdouble result2 = max&lt;double&gt;(1.3d, 4.2d);\tchar result3 = max&lt;char&gt;(&#x27;a&#x27;, &#x27;c&#x27;);&#125;\r\n这个简单的max函数，精准地反映了泛型编程（Generic\r\nProgramming）的核心思想：不为每种类型重复编写逻辑相同的代码，而是将类型本身参数化，让编译器在编译期自动生成不同类型的版本。这种思想，不仅直接催生了C++标准容器库的产生，更成为现代C++的基石之一。\r\n函数模板\r\n基本语法\r\n函数模板（Function template）的一般定义方式为：\r\ntemplate &lt;模板形参列表&gt;函数声明 以前文中的max函数模板为例 template &lt;typename T&gt;auto max(T lhs, T rhs) -&gt; T &#123;\treturn lhs &gt; rhs ? lhs : rhs;&#125;\r\n可以看到，max具有一个模板形参typename T，typename表示T是一个类型形参，所以当在调用max的时候，T只能是一个类型（如int）而不能是一个值（如42）。我们也可以使用class T来声明类型形参，这种写法完全等价于typename T，但笔者认为使用typename T更加清晰（使用class容易让人认为T只能说一个类，但实际上T能是任意类型）。\r\n函数模板实例化\r\n函数模板并不是实际的函数，单纯的定义一个函数模板并不会在编译后生成任何代码，为了使用函数模板，我们需要实例化（instantiate）模板：\r\ntemplate &lt;typename T&gt;auto max(T lhs, T rhs) -&gt; T &#123;\treturn lhs &gt; rhs ? lhs : rhs;&#125;auto main() -&gt; int &#123;\t//显式地提供模板实参\tint result1 = max&lt;int&gt;(114, 514);\t//如果模板实参能从上下文推导出来，那么我们不必提供它\tdouble result2 = max&lt;&gt;(4.2, 1.3);\t//我们也可以直接省略尖括号对\tdouble result3 = max(4.2, 1.3);\tstd::println(&quot;&#123;&#125; &#123;&#125; &#123;&#125;&quot;, result1, result2, result3);&#125; Compiler\r\nExplorer链接\r\n我们可以将实例化看作编译器自动地将函数定义中对模板形参的使用全部替换为模板实参的过程，上面的代码在编译时，编译器会实例化产生两个max函数，我们将这些生成的函数成为函数模板的实例：\r\nauto max(int lhs, int rhs) -&gt; int &#123;\treturn lhs &gt; rhs ? lhs : rhs;&#125;auto max(double lhs, double rhs) -&gt; double &#123;\treturn lhs &gt; rhs ? lhs : rhs;&#125; 注意：\r\n\r\n同一个函数模板实例化出的不同实例间没有任何关系，如同两个相互独立的函数。\r\n如果一个函数模板已经对某些模板实参完成实例化，那么再次用这些模板实参来调用函数模板将不会再次实例化，而是使用之前已经产生的实例。（注意在上面的例子中，我们在T=double的情况下使用了两次max，而实际只实例化生成了一个函数）\r\n\r\n函数模板的实例化可以分为显式实例化（Explicit\r\nTemplate Instantiation）和隐式实例化（Implicit Template\r\nInstantiation）两种，继续以max模板为例： template &lt;typename T&gt;auto max(T lhs, T rhs) -&gt; T &#123;\treturn lhs &gt; rhs ? lhs : rhs;&#125;//显式实例化templateauto max&lt;int&gt;(int, int) -&gt; int;//如果模板实参能够从上下文推导，则我们可以将其省略templateauto max&lt;&gt;(short, short) -&gt; short;//当然也可以连尖括号一起省略templateauto max(long, long) -&gt; long;auto main() -&gt; int &#123;\t//如果我们使用了一个函数模板而该函数模板还没有被实例化过，则会发生隐式实例化：\t//max&lt;double&gt;没有实例化，故此处发生隐式实例化\tdouble result1 = max(4.2, 1.3);\t//max&lt;int&gt;已经被实例化过了，则此处不会发生任何实例化\tint result2 = max&lt;int&gt;(114, 514);&#125;\r\n与函数形参类似，我们也可以为模板形参指定默认值： template &lt;typename T = int&gt;auto foo(T t) -&gt; T &#123;\treturn t;&#125;auto main() -&gt; int &#123;\t//此时默认T = int\tint result1 = foo(1);\t//注意，当推导出的实参与形参默认值不一致时，以推导出的实参为准\tauto result2 = foo(4.2f);\tstatic_assert(not std::same_as&lt;decltype(result2), int&gt;);&#125;\r\n非类型模板形参\r\n既然我们有只接受类型的类型模板形参，自然也有非类型模板形参（NTTP，Nontype\r\nTemplate\r\nParameter），NTTP接受一个对象的值： template &lt;int N&gt;auto foo() -&gt; int &#123;\treturn N;&#125;auto main() -&gt; &#123;\tstd::println(&quot;&#123;&#125;&quot;, foo&lt;42&gt;());&#125;\r\nNTTP的类型可以不是具体类型： template &lt;auto N&gt;auto print() -&gt; void &#123;\tstd::println(&quot;&#123;&#125;&quot;, N);&#125;auto main() &#123;\tprint&lt;114514&gt;();\tprint&lt;4.2f&gt;();\tprint&lt;true&gt;();\t\t//这行代码没法通过编译，详细解释见下\tprint&lt;std::string&#123;&quot;awa&quot;&#125;&gt;();&#125;\r\n为什么在上面的例子中，我们不能使用std::string来作为非类型模板实参呢？这是由于C++为了保证相同的模板实参列表所对应的模板实例是唯一的，换而言之就是需要判断模板实参之间的相等性。对于类型模板实参而言，其相等性判断非常容易实现（即判断两个类型是否为同一类型）。但是对于非类型模板实参，判断其相等性可能会较为复杂，同样以std::string为例，假设我们有两个std::string对象：\r\nstd::string str1&#123;&quot;awa&quot;&#125;;std::string str2&#123;&quot;awa&quot;&#125;; 在忽略短字符串优化（SSO，Short\r\nString\r\nOptimization）和字符串常量池优化的情况下，这两个std::string对象的内部指针会指向两个不同的内存地址，但是两个字符串的存储内容是相同的，这就会造成相等性判断时的歧义：单从类型的定义上看，编译器没法知道你需要比较两个std::string的存储内容，只能从最简单的比较相应成员变量的方式来判断相等性，然而这会导致编译器认为这两个对象是不相等的，从而导致以下两行代码（假设它们能通过编译）：\r\nprint&lt;str1&gt;();print&lt;str2&gt;();\r\n所指的实际上是不同的模板实例，这显然与我们的预期不一致。为了解决这个问题，C++规定用于NTTP的对象的类型必须是结构化类型（Structural\r\nType），结构化类型的定义较为复杂，此处不展开讲解，详细定义参见CppReference上相关页面。\r\n至于NTTP的进一步的引用场景，我们会在后续章节详细讲解。\r\n简写函数模板\r\n对于一些简单的函数模板，如上文例子中所用的max，C++20标准引入了一种更为简洁的写法，称为简写函数模板（Abbreviated\r\nFunction Template）： auto max(auto lhs, auto rhs) &#123;\treturn (lhs &gt; rhs) ? lhs : rhs;&#125; 这种写法相当于： template &lt;typename T1, typename T2&gt;auto max(T1 lhs, T2 rhs) &#123;\treturn (lhs &gt; rhs) ? lhs : rhs;&#125;\r\n简写函数模板这个特性非常直观且容易理解，故此处不再展开。\r\n转发引用与引用折叠\r\n在使用模板时，我们经常会想要保留原实参的表达式的值类别，通俗得讲就是左值和右值（关于什么是左值和右值不在本教程的范围之内，如果你不了解，推荐阅读HackingCpp上关于移动语义的这一章），这个时候就要用到转发引用（Forwarding\r\nReference）： template &lt;typename T&gt;auto foo(T &amp;&amp;t) -&gt; void &#123;&#125;auto main() -&gt; int &#123;\t//传递右值，T推导为int，函数形参为int &amp;&amp;t\tfoo(4);\t//传递左值，T推导为int&amp;，函数形参为int &amp; &amp;&amp;t，引用折叠后变为int &amp;t\tint a = 42;\tfoo(a);&#125; 引用折叠规则（Reference\r\nCollapsing\r\nRule）可以用一句话来总结：右值引用的右值引用折叠成右值引用，所有其他组合均折叠成左值引用。\r\nusing LRef = int&amp;;using RRef = int&amp;&amp;;auto main() -&gt; int &#123;\tint n = 42;\tLRef &amp;ref1 = n; //LRef &amp;折叠为int &amp;\tLRef &amp;&amp;ref2 = n;//LRef &amp;&amp;折叠为int &amp;\tRRef &amp;ref3 = n; //RRef &amp;折叠为int &amp;\tRRef &amp;&amp;ref4 = 1;//RRef &amp;&amp;折叠为int &amp;&amp;&#125;\r\n如果我们想将转发引用传递给另一个函数，并让转发引用继续保持其值类别，就需要使用标准库中的转发函数std::forward：\r\ntemplate &lt;typename T&gt;auto foo(T &amp;&amp;t) -&gt; void &#123;\t//将t完美转发给bar函数\tbar(std::forward&lt;T&gt;(t));&#125;\r\n模板形参包\r\n在实际场景中，我们常常会遇到需要编写变长参数的函数的情景，我们可以通过C++模板来实现这种函数。以一个能够接收任意多参数并计算所有参数的和并返回的函数sum为例：\r\nint sum_result = sum(1, 4, 6, 8);std::println(&quot;&#123;&#125;&quot; sum_result);    //输出19\r\n为了实现这样的函数，我们需要用到模板形参包（Template\r\nParameter Pack）： template &lt;typename ... ArgTypes&gt;auto sum(ArgTypes ... args) -&gt; std::common_type_t&lt;ArgTypes...&gt; &#123;\treturn (args + ...);&#125;\r\n其中，typename ... ArgTypes声明了一个类型模板形参包（Type\r\nTemplate Parameter\r\nPack，我不说你应该也能猜到还有非类型模板形参包），并通过这个模板形参包声明了一个函数形参包（Function\r\nParameter\r\nPack）ArgTypes ... args。ArgTypes包含了所有函数形参的类型，而args则包含了函数所有的参数。这些包能够接受任意数量的实参，比如：\r\nauto main() -&gt; int &#123;\tint result1 = sum&lt;int, int, int, int&gt;(1, 3, 6, 9);\t//模板形参包当然能够被推导，所以上面这行代码也可以写为\tint result2 = sum(1, 3, 6, 9);\t//不同的类型也可以\tauto result3 = sum(1, 4.2f, 6ull);\t\tstd::println(&quot;&#123;&#125; &#123;&#125; &#123;&#125;&quot;, result1, result2, result3);&#125; Compiler\r\nExplorer链接 以上几行代码会导致以下实例的产生： templateauto sum(int, int, int, int) -&gt; int;template auto sum(int, float, unsigned long long) -&gt; float;\r\n除了声明形参包之外，sum函数还用到了两个特性：包展开（Pack\r\nExpansion）和折叠表达式（Fold\r\nExpression），我们逐一讲解这些特性。\r\n包展开\r\n想要使用一个形参包里的内容，就需要用到包展开。所谓包展开，就是将包在代码中按照特定的模式展开成为一个列表，以sum函数的返回类型为例：\r\nstd::common_type_t&lt;ArgTypes...&gt;\r\n其中，std::common_type_t是C++标准元编程库中的一个类型特征（Type\r\nTrait），它的作用就是找出它的模板实参的公共类型，也就是所有模板实参都能显式转换到的类型。这个类型特征的原理我们会在后面的章节讲到。而ArgTypes...就是我们所说的包展开。假设ArgTypes接受了三个类型：int, float, unsigned long long，那么包展开的结果为\r\nstd::common_type_t&lt;int, float, unsigned long long&gt;\r\n我们将ArgTypes...中的ArgTypes称为包展开的模式（Pattern），包展开会将包中的每个元素按照我们所指定的模式展开成为一个逗号分隔的列表，比如：\r\ntemplate &lt;typename ... ArgTypes&gt;//模板形参包展开为bool, float, char const*auto foo(ArgTypes ... args) -&gt; void &#123;//函数形参包展开为bool arg0, float arg1, char const *arg2\targs...;//展开为arg0, arg1, arg2\tArgTypes...;//展开为bool, float, char const *\t&amp;args...;//展开为&amp;arg0, &amp;arg1, &amp;arg2&#125;auto main() -&gt; int &#123;\tfoo(true, 4.2f, &quot;awa&quot;);&#125; 一般而言，常见的包展开方式有：\r\n\r\n递归展开：通过递归模板实例化逐个处理参数\r\n逗号运算符展开：结合逗号运算符生成初始化列表\r\n初始化列表展开：利用&#123;&#125;初始化语法展开\r\n\r\n通过这些包展开形式，我们就可以写出一些有用的函数。\r\n示例1：递归展开求和： //递归模板实例化终止条件：只有一个实参template &lt;typename T&gt;auto accumulate(T value) -&gt; T &#123;\treturn value;&#125;//递归展开参数包template &lt;typename T, typename ... Args&gt;auto accumulate(T first, Args ... args) -&gt; T &#123;\treturn first + accumulate(args...);&#125;auto main() -&gt; int &#123;\tauto result = accumulate(1, 4.3f, 5.6, 7l);\t//递归实例化步骤：\t//1. T=int, Args=&lt;float, double, long&gt;，accumulate(4.3f, 5.6d, 7l)\t//2. T=float, Args=&lt;double, long&gt;，递归调用accumulate(5.6d, 7l)\t//3. T=double, Args=&lt;long&gt;，递归调用accumulate(7l)\t//4. T=long，到达递归终止条件\tstd::println(&quot;&#123;&#125;&quot;, result);&#125; Compiler Explorer链接\r\n示例2：使用逗号运算符展开输出所有实参 template &lt;typename ... ArgTypes&gt;auto printAll(ArgTypes ... args) -&gt; void &#123;//展开为char const *arg0, char arg1, int arg2, double arg3\tint dummy_array[] = &#123;(std::println(&quot;&#123;&#125;&quot;, args), 0)...&#125;;\t//展开为&#123;(std::print(&quot;&#123;&#125;&quot;, arg0), 0), (std::print(&quot;&#123;&#125;&quot;, arg1), 0), (std::print(&quot;&#123;&#125;&quot;, arg2), 0), (std::print(&quot;&#123;&#125;&quot;, arg3), 0)&#125;&#125;auto main() -&gt; int &#123;\tprintAll(&quot;awa&quot;, &#x27;o&#x27;, 42, 114.514);&#125;\r\nCompiler Explorer链接\r\n对于这个例子，没有接触过形参包的人总是会写出这样的包展开：\r\ntemplate &lt;typename ... ArgTypes&gt;auto printAll(ArgTypes ... args) -&gt; void &#123;\t(std::println(&quot;&#123;&#125;&quot;, args)...;&#125;\r\n但实际上这是不行的，这是因为包展开需要在一定的包展开场所中进行，包展开场所的完整列表相当长，故此处不做展开，如有需要可以查看CppReference相关页面。一般而言，常用的包展开场所有函数或模板的实参列表，及大括号/圆括号包围的初始化器。\r\n示例3：多个包的同时展开\r\n我们可以在一个包展开中同时展开多个包，当然，这些包的长度必须一致，我们可以使用sizeof...(pack)来获取包的长度（这个例子里使用了模板类，但是不是很深入，如果你看不懂可以先行阅读一下下一节[[#类模板]]的基础部分）：\r\ntemplate &lt;typename ... Args1&gt;struct Zip &#123;\ttemplate &lt;typename ... Args2&gt;\tstruct With &#123;\t\t//检查两个形参包的长度是否一致，若不一致则给出报错信息\t\tstatic_assert(sizeof...(Args1) == sizeof...(Args2), &quot;The two packs must have the same length&quot;);\t\t\t\tusing type = std::tuple&lt;std::pair&lt;Args1, Args2&gt;...&gt;;\t\t//包展开结果为：\t\t//std::tuple&lt;std::pair&lt;int, bool&gt;, std::pair&lt;float, double&gt;, std::pair&lt;char, std::string&gt;&gt;\t&#125;;&#125;;auto main() -&gt; int &#123;\tZip&lt;int, float, char&gt;::With&lt;bool, double, std::string&gt;::type zipped&#123;\t\t&#123;42, true&#125;,\t\t&#123;11.4f, 5.14&#125;,\t\t&#123;&#x27;a&#x27;, &quot;wa&quot;&#125;\t&#125;;&#125; 示例4：嵌套包展开\r\n若包展开内嵌于另一个包展开中，那么包展开按从里到外的顺序依次进行：\r\ntemplate &lt;typename ... ArgTypes&gt;auto foo(ArgTypes ... args) -&gt; void &#123;\tbar(meow(args...) + args...);\t//首先将内层包展开meow(args...)展开为meow(1, 2, 3)\t//随后将内层包展开的结果进行外层包展开\t//最终结果为bar(meow(1, 2, 3) + 1, meow(1, 2, 3) + 2, meow(1, 2, 3) + 3)&#125;auto main() -&gt; int &#123;\tfoo(1, 2, 3);&#125;\r\n折叠表达式\r\n通过刚才的例子，不难发现包展开的语法较为复杂，且在处理计算的时候（如上文中的sum函数）时较为复杂。C++17标准中所引入的折叠表达式，可以让我们通过简洁的方式来完成这些包展开。在函数模板\r\ntemplate &lt;typename ... ArgTypes&gt;auto sum(ArgTypes ... args) -&gt; std::common_type_t&lt;ArgTypes...&gt; &#123;\treturn (args + ...);&#125;\r\n中，(args + ...)就是一个折叠表达式。折叠表达式的语法为：\r\n(形参包 运算符 ...)            //一元右折叠(... 运算符 形参包)            //一元左折叠(形参包 运算符 ... 运算符 初始值)//二元右折叠(初始值 运算符 ... 运算符 形参包)//二元左折叠 注意折叠表达式的括号是不可省略的。\r\n一个折叠表达式是左折叠还是右折叠，取决于...出现在形参包的左侧还是右侧。左折叠和右折叠的区别是展开后的括号结合性：\r\n\r\n一元右折叠：(E op ...)展开为(E1 op (E2 op (... op (EN-1 op EN))))\r\n一元左折叠：(... op E)展开为((((E1 op E2) op ...) op EN-1) op EN)\r\n二元右折叠：(E op ... op init)展开为\r\n(E1 op (E2 op (... op (EN-1 op (EN op init)))))\r\n二元左折叠：(init op ... op E)展开为(((((init op E1) op E2) op ...) op EN-1) op EN)\r\n\r\n以sum函数模板为例： auto main() -&gt; int &#123;\tint result = sum(1, 1, 4, 5, 1, 4);\t//折叠表达式展开为：\t//(1 + (1 + (4 + (5 + (1 + 4)))))&#125;\r\n我们也可以使用其他运算符，比如逻辑与&amp;&amp;：\r\ntemplate &lt;typename ... ArgTypes&gt;auto allTrue(ArgTypes ... args) -&gt; bool &#123;\treturn (args &amp;&amp; ...);&#125;\r\n注意，左折叠和右折叠会影响计算顺序，因此如果运算符不满足结合性，那么左折叠和右折叠的计算结果也是不同的：\r\ntemplate &lt;typename ... ArgTypes&gt;auto rightFoldSubtract(ArgTypes ... args) -&gt; std::common_type_t&lt;ArgTypes...&gt; &#123;\treturn (args - ...);&#125;template &lt;typename ... ArgTypes&gt;auto leftFoldSubtract(ArgTypes ... args) -&gt; std::common_type_t&lt;ArgTypes...&gt; &#123;\treturn (... - args);&#125;auto main() -&gt; int &#123;\tint result1 = leftFoldSubtract(1, 2, 3);\tint result2 = rightFoldSubtract(1, 2, 3);\t//result1 = ((1 - 2) - 3) = -4\t//result2 = (1 - (2 - 3)) = 2\tstd::println(&quot;&#123;&#125; &#123;&#125;&quot;, result1, result2);&#125; Compiler\r\nExplorer链接\r\n将折叠表达式与逗号运算符相结合，我们能够更加优雅的写出很多东西，比如printAll函数模板可以简化成：\r\ntemplate &lt;typename ... ArgTypes&gt;auto printAll(ArgTypes ... args) -&gt; void &#123;\t((std::println(&quot;&#123;&#125;&quot;, args), 0), ...);&#125;\r\n包索引\r\n在C++26之前，我们并没有一个直接访问包中第n个元素的方式，唯一使用形参包的方式就是将其展开。为了访问第n个元素，我们需要使用一个递归模板：\r\ntemplate&lt;size_t N, typename... Ts&gt;struct GetNthType;//终止条件：当 N=0 时，捕获第一个类型template&lt;typename T, typename... Ts&gt;struct GetNthType&lt;0, T, Ts...&gt; &#123;    using type = T;&#125;;//递归条件：递减索引 N，继续处理剩余参数包template&lt;size_t N, typename T, typename... Ts&gt;struct GetNthType&lt;N, T, Ts...&gt; &#123;    using type = typename GetType&lt;N - 1, Ts...&gt;::type;&#125;;//使用示例using ThirdType = GetNthType&lt;2, int, double, char&gt;::type; //char\r\n为了实现GetNthType，此处使用了模板特化，模板特化会在[[#模板特化与部分特化]]一节中详细讲解。\r\n这种方法不仅效率低下（尝试思考一下获取第n个元素需要实例化多少个模板），而且不够通用（显然GetNthType只能处理类型形参包）。为了解决这个问题，C++26标准引入了包索引（Pack\r\nIndexing），包索引的语法如下： 形参包...[索引]\r\n这一特性大大简化了跟包相关的模板代码。将包索引和折叠表达式结合在一起，我们能够写出非常简洁优雅的代码：\r\ntemplate &lt;std::size_t ... Indexes, typename ... ArgTypes&gt;auto sampleSum(ArgTypes ... args) -&gt; std::common_type_t&lt;ArgTypes...&gt; &#123;\treturn (args...[Indexes] + ...);//将Indexes对应的所有args中的元素相加&#125;auto main() -&gt; int &#123;\tauto result = sampleSum&lt;1, 3, 5&gt;(1, 9, 1, 9, 8, 1, 0);//result = 9 + 9 + 1\tstd::println(&quot;&#123;&#125;&quot;, result);//输出19&#125; Compiler\r\nExplorer链接\r\n类模板\r\n与函数模板相类似，我们也可以声明类模板（Class\r\nTemplate），类模板的语法与函数模板非常类似： template &lt;模板形参列表&gt;类声明\r\n类模板的一大使用案例是实现容器，标准库当中常用的std::vector、std::map等容器都是类模板。在此我们通过实现一个简单的固定容量容器FixedArray来演示类模板的使用：\r\ntemplate &lt;typename ElementType&gt;struct FixedArray &#123;\tElementType storage[16];&#125;;\r\n与函数模板类似，类模板并不是实际存在的类，在不进行实例化的情况下编译器不会为类模板生成任何代码。要使用类模板，首先要做的就是实例化：\r\ntemplate &lt;typename ElementType&gt;struct FixedArray &#123;\tElementType storage[16];&#125;;auto main() -&gt; int &#123;\t//实例化FixedArray&lt;char&gt;\tFixedArray&lt;char&gt; array1&#123;&#x27;t&#x27;, &#x27;e&#x27;, &#x27;m&#x27;, &#x27;p&#x27;, &#x27;l&#x27;, &#x27;a&#x27;, &#x27;t&#x27;, &#x27;e&#x27;&#125;;\t//C++17添加了类模板实参推导特性（Class Template Argument Deduction, CTAD）\t//在可以推导出模板实参的上下文中我们可以省略模板实参\tFixedArray array2&#123;1.3f, 2.6f, 3.9f&#125;;\tfor (auto elem : array1.storage) &#123;\t\tstd::print(&quot;&#123;&#125; &quot;, elem);\t&#125;\tstd::println(&quot;&quot;);\tfor (auto elem : array2.storage) &#123;\t\tstd::print(&quot;&#123;&#125; &quot;, elem);\t&#125;&#125; Compiler\r\nExplorer链接\r\n类模板的显式实例化较函数模板更加复杂，我们可以一次性显式实例化整个类模板，也可以单独实例化类中的成员（包括成员函数、成员类、静态成员变量）：\r\ntemplate &lt;typename T&gt;class Foo &#123;\tstatic inline int demo_static_member = 42;\t\tauto demo_function() -&gt; void &#123;&#125;\t\tclass Bar &#123;&#125;;&#125;;//实例化整个类templateclass Foo&lt;int&gt;;//实例化单个成员函数，注意模板参数列表的摆放位置templateauto Foo&lt;char&gt;::demo_function() -&gt; void;//实例化单个静态成员变量templateint Foo&lt;double&gt;::demo_static_member;//实例化单个成员类templateclass Foo&lt;bool&gt;::Bar;\r\n类模板的隐式实例化与函数模板类似，当我们需要使用一个被完整定义的类模板且该类模板没有对应显式实例化的时候，隐式实例化就会发生：\r\ntemplate &lt;typename ElementType&gt;struct FixedArray &#123;\tElementType storage[16];&#125;;templatestruct FixedArray&lt;int&gt;;auto main() -&gt; &#123;\t//已经有对应的显式实例化，故此处不发生隐式实例化\tFixedArray&lt;int&gt; array1&#123;&#125;;\t//没有对应的显式实例化，故此处发生隐式实例化\tFixedArray&lt;double&gt; array2&#123;&#125;;\t//使用一个类型的指针类型不要求该类型有完整定义，故此处不发生隐式实例化\tFixedArray&lt;char&gt;* array_ptr = nullptr;&#125;\r\n在上面这个例子中，最后一处使用FixedArray&lt;char&gt;*指针的例子尤其值得注意。由于使用一个类型的指针类型并不需要该类型有定义，故此行代码不会触发隐式实例化。\r\n\r\n这是因为C++作为静态类型语言，需要在定义一个类型的变量之前知道该类型的大小，而指针类型的大小仅由平台决定，与指针所指向的类型大小无关，故不需要指针指向的类型有完整定义。\r\n\r\n类模板实参推导\r\n类模板实参推导（CTAD，Class\r\nTemplate Argument\r\nDeduction）可以通过初始化表达式的类型来自动推导模板实参，CTAD的推导规则相当复杂，此处不进行展开说明，如有需要可以参考CppReference上的CTAD一节。\r\n对于简单的类模板，CTAD可以很方便的自动推导出对应的模板实参，但是如果类较为复杂或者有特殊处理需求，导致CTAD不起作用时，可以通过定义推导指引(Deduction\r\nGuides)来改变CTAD的行为。推导指引的语法如下： template &lt;模板形参列表&gt;模板名(形参列表) -&gt; 模板名&lt;推导结果&gt;\r\n为了演示推导指引的作用，我们通过向FixedArray加入一个int类型的非类型模板形参来使其支持存储指定长度的数组：\r\ntemplate &lt;typename ElementType, int Length&gt;struct FixedArray &#123;\tElementType storage[Length];&#125;;auto main() -&gt; int &#123;\tFixedArray&lt;int, 4&gt; array&#123;1, 2, 3, 4&#125;;&#125;\r\n很自然的，我们期望FixedArray能够通过初始化列表来自动推导Length：\r\n//这样写既简洁又不容易出错，但很可惜CTAD不支持这种情况下的模板实参推导FixedArray array&#123;1, 2, 3, 4&#125;; 为了让CTAD能自动完成类似推导，我们需要定义以下推导指引：\r\ntemplate &lt;typename ElementType, typename... TailArgTypes&gt;FixedArray(ElementType, TailArgTypes...) -&gt; FixedArray&lt;ElementType, sizeof...(TailArgTypes) + 1&gt;;\r\n推导规则的工作原理是定义一个假想的函数模板，编译器会将无法默认规则无法推导的初始化表达式作为该函数模板的实参，再通过函数模板的推导规则来获取我们定义的类模板实参推导结果。就拿FixedArray作为例子，当编译器遇到一个无法被默认推导规则推导的规则时，如：\r\nFixedArray array&#123;1, 2, 3, 4&#125;;\r\n编译器会尝试查找FixedArray的推导规则，在这个例子中，只有上文定义的一条推导规则。在编译器查找到之后，它会尝试“想象”一个函数模板调用：\r\ntemplate &lt;typename ElementType, typename... TailArgTypes&gt;FixedArray(ElementType, TailArgTypes...) /*忽略返回类型和函数体*/;FixedArray(1, 2, 3, 4);\r\n然后编译器就可以从这一调用推导出ElementType=int, TailArgTypes=&lt;int, int, int&gt;，从而成功推导出类模板的实参。\r\n当然，这一推导规则可以用我们前文所讲的[[#包索引]]简化：\r\ntemplate &lt;typename ... ArgTypes&gt;FixedArray(ArgTypes...) -&gt; FixedArray&lt;ArgTypes...[0], sizeof...(ArgTypes)&gt;;\r\n模板模板形参\r\n我们现在有了可以接受类型的类型模板形参，有了可以接受对象的非类型模板实参（即NTTP），还有什么是模板形参接受不了的呢？就是模板。为了解决这一问题，C++引入了模板模板形参（Template\r\nTemplate Parameter，很绕口就对了），模板模板形参的声明语法如下：\r\ntemplate &lt;template &lt;模板形参列表&gt; 模板类型 标识符&gt;template &lt;template &lt;模板形参列表&gt; 模板类型 标识符 = 默认值&gt;template &lt;template &lt;模板形参列表&gt; 模板类型 ... 标识符&gt;\r\n其中，模板类型可以为class、typename、concept（C++26加入）、auto（C++26加入）。我们此处只讲解typename一种。\r\ntemplate &lt;typename ElementT&gt;struct MyArray &#123;\t//something...&#125;;template &lt;typename KeyT, typename ValueT, template &lt;typename&gt; typename ContainerT&gt;struct Map &#123;\tContainerT&lt;KeyT&gt; _keys;\tContainerT&lt;ValueT&gt; _keys;&#125;;auto main() -&gt; int &#123;\tMap&lt;int, std::string, MyArray&gt; my_map;&#125;\r\n其中，template &lt;typename&gt; typename ContainerT即为一个模板模板形参。ContainerT接受一个模板类作为实参（typename ContainerT说明ContainerT是一个类型），且该模板类拥有一个类型模板形参（template &lt;typename&gt;说明Container是一个模板，且模板形参列表为&lt;typename&gt;）。我们可以将符合这一要求的类模板（比如MyArray）赋给ContainerT。\r\n成员函数模板\r\n成员函数模板与类模板的相关性不大，类模板和普通的类都可以有成员函数模板，类模板的模板形参在其成员函数模板中都可用：\r\nclass NonTemplateClass &#123;\ttemplate &lt;typename T&gt;\tauto templateFunction() -&gt; void &#123;&#125;\tauto regularFunction() -&gt; void &#123;&#125;&#125;;template &lt;typename ClassT&gt;class TemplateClass &#123;\t//类模板的模板形参在其成员函数模板中都可用\ttemplate &lt;typename T = ClassT&gt;\tauto templateFunction() -&gt; void &#123;&#125;\tauto regularFunction() -&gt; void &#123;&#125;\t//构造函数也可以是函数模板\ttemplate &lt;typename F&gt;\tTemplateClass() &#123;&#125;&#125;; 注意，一个类的析构函数不能是函数模板。\r\n成员模板函数与一般的模板函数没有太大差别，故此处不深入讲解。\r\n类模板与形参包\r\n与函数模板一样，形参包在类模板中亦有广泛的使用，其中最为知名的莫过于std::tuple。通过使用形参包，std::tuple有了在一个对象中存储不同类型对象的能力。\r\n我们用一段简单的例子来演示形参包在类模板中的使用： template &lt;typename ... ElementT&gt;struct MyTuple &#123;\tstd::tuple&lt;ElementT...&gt; _storage;\tMyTuple(ElementT&amp;&amp; ... args)\t\t: _storage(std::forward&lt;ElementT&gt;(args)...)\t&#123;&#125;&#125;;\r\n别名模板与变量模板\r\n除了函数和类之外，类型别名与变量都可以成为模板，分别称作别名模板（Alias\r\nTemplate）和变量模板（Variable\r\nTemplate）。这两类模板的作用会在后面几篇教程详细讲解，在此处我们仅做简单演示：\r\n//别名模板template &lt;typename T&gt;using Alias = T;//变量模板template &lt;typename T&gt;std::size_t size = sizeof(T);auto main() -&gt; int &#123;\tstd::println(&quot;&#123;&#125;&quot;, size&lt;Alias&lt;int&gt;&gt;);//输出4，即int类型的大小&#125; Compiler\r\nExplorer链接\r\n待决名\r\ntypename消岐义符\r\n我们使用一段代码来引入待决名（Dependent\r\nName）这一概念： template &lt;typename T&gt;auto getIterator(std::vector&lt;T&gt; const &amp;v) -&gt; void &#123;\tstd::vector&lt;T&gt;::const_iterator it = v.begin();&#125;auto main() -&gt; int &#123;\tstd::vector&lt;int&gt; v&#123;&#125;;\tgetIterator(v);&#125; Compiler Explorer链接\r\n上面这段代码在经过clang编译后会产生以下错误： &lt;source&gt;:5:2: error: missing &#x27;typename&#x27; prior to dependent type name &#x27;std::vector&lt;T&gt;::const_iterator&#x27;    5 |         std::vector&lt;T&gt;::const_iterator it = v.begin();      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~      | typename 1 error generated.\r\n报错的原因也很简单：由于std::vector&lt;T&gt;是一个依赖于模板形参T的类型，所以编译器在实例化之前并不知道std::vector&lt;T&gt;里面有什么东西。在编译器的视角中，std::vector&lt;T&gt;::const_iterator可以是：\r\n\r\n一个静态成员变量\r\n一个成员类型别名\r\n一个静态成员函数\r\n…\r\n\r\n显然，编译器并不知道应该选择哪一种来解释const_iterator，所以产生了这样的报错，我们称std::vector&lt;T&gt;::const_iterator是一个待决名。为了避免报错，我们应该显式地告诉编译器（即“消岐义”，Disambiguate），const_iterator是一个成员类型别名：\r\ntemplate &lt;typename T&gt;auto getIterator(std::vector&lt;T&gt; const &amp;v) -&gt; void &#123;\t//在前面加上typename关键字来说明这是个类型\ttypename std::vector&lt;T&gt;::const_iterator it = v.begin();&#125;auto main() -&gt; int &#123;\tstd::vector&lt;int&gt; v&#123;&#125;;\tgetIterator(v);&#125;\r\n那么，我们应该在何时使用typename进行消岐义呢？\r\n\r\n在模板（包括别名模版）的声明或定义中，不是当前实例化的成员且取决于某个模板形参的名字不会被认为是类型，除非使用关键词typename或它已经被设立为类型名（例如用typedef声明或通过用作基类名）。\r\n\r\n将这句话拆分成几个部分：\r\n\r\n在模板的声明或定义中\r\n不是当前实例化的成员且取决于某个模板形参的名字\r\n除非使用关键词typename或它已经被设立为类型名\r\n对应到上面的例子中：\r\n我们在一个函数模板getIterator的定义中，符合\r\nstd::vector&lt;T&gt;::const_iterator不是当前实例化的成员，同时取决于我们的模板形参T，符合\r\n\r\n所以编译器不会认为std::vector&lt;T&gt;::const_iterator是一个类型名，所以我们需要使用typename来进行消岐义。\r\n当然，如果名字并非待决，我们也可以在前面使用typename。通俗来讲就是如果编译器已经知道某个标识符是个类型，你也可以继续跟编译器重复说这是个类型（当然不少编译器都会给你一个warning说这不是必要的）。\r\ntemplate消岐义符\r\n我们同样以一个例子做引入： template &lt;typename T&gt;struct Foo &#123;\ttemplate &lt;typename U&gt;\tauto foo() -&gt; void &#123;&#125;&#125;;template &lt;typename T&gt;auto bar() -&gt; void &#123;\tFoo&lt;T&gt; foo_inst&#123;&#125;;\tfoo_inst.foo&lt;T&gt;();&#125; Compiler Explorer链接\r\n这段代码同样会发生编译错误，clang编译器的报错如下： &lt;source&gt;:10:11: error: use &#x27;template&#x27; keyword to treat &#x27;foo&#x27; as a dependent template name   10 |         foo_inst.foo&lt;T&gt;();      |                  ^      | template 1 error generated.\r\n与typename类似，编译器不认为foo_inst.foo&lt;T&gt;是一个模板，所以相应的，我们需要加上template消岐义符：\r\ntemplate &lt;typename T&gt;struct Foo &#123;\ttemplate &lt;typename U&gt;\tauto foo() -&gt; void &#123;&#125;&#125;;template &lt;typename T&gt;auto bar() -&gt; void &#123;\tFoo&lt;T&gt; foo_inst&#123;&#125;;\tfoo_inst.template foo&lt;T&gt;();&#125;\r\n使用template消岐义的规则与typename类似：\r\n\r\n模板定义中不是当前实例化的成员的待决名同样不被认为是模板名，除非使用消歧义关键词template，或它已被设立为模板名。\r\n\r\n对应上面的例子：\r\n\r\n我们在bar模板函数的定义中\r\n且Foo&lt;T&gt;不是当前实例化的成员\r\n\r\n故foo_inst.foo&lt;T&gt;亦是待决名，故编译器不认为它是一个模板。\r\n同样的，template消岐义符也可以用在不需要消岐义的标识符前面。\r\n绑定规则\r\n模板中标识符的绑定规则可以根据该标识符是否待决分为两类：\r\n\r\n非待决名在模板定义点查找并绑定。即使在模板实例化点有更好的匹配，也保持此绑定。\r\n待决名的绑定则延迟到查找发生时，即实例化时。\r\n\r\n我们用一段代码来理解以上规则： auto foo(double dummy) -&gt; void &#123;\tstd::println(&quot;foo(double) called&quot;);&#125;template &lt;typename T&gt;struct Bar &#123;\tauto meow() -&gt; void &#123;\t\tfoo(42);\t&#125;&#125;;auto foo(int dummy) -&gt; void &#123;\tstd::println(&quot;foo(int) called&quot;);&#125;auto main() -&gt; int &#123;\tfoo(42);\tBar&lt;void&gt; bar_inst;\tbar_inst.meow();&#125; Compiler Explorer链接\r\n运行这段代码，输出结果为： foo(int) calledfoo(double) called\r\n为什么会出现这个结果呢？\r\nmain函数中对foo(42)函数的调用很直观的绑定到了foo(int)，但是在Bar&lt;T&gt;::meow\r\n中的foo(42)是一个非待决名，非待决名在模板的定义点查找并绑定，而非在实例化点绑定。所以当编译器编译到了meow函数时，它会立刻开始查找目前已经定义的foo函数，而由于此时编译器还没有看到后面的foo(int)，所以此处只能绑定到已知的foo(double)。但如果在meow中foo是一个待决名，那么绑定会在实例化的时候发生。\r\n### 查找规则 auto bar() -&gt; void &#123; std::println(&quot;::bar() called&quot;); &#125;template &lt;typename T&gt;struct Base &#123;\tauto bar() -&gt; void &#123; std::println(&quot;Base::bar() called&quot;); &#125;&#125;;template &lt;typename T&gt;struct Foo : public Base&lt;T&gt; &#123;\tauto dependent() -&gt; void &#123;\t\tthis-&gt;bar();\t&#125;\tauto independent() -&gt; void &#123;\t\tbar();\t&#125;&#125;;auto main() -&gt; int &#123;\tFoo&lt;void&gt; foo;\tfoo.dependent();\tfoo.independent();&#125; Compiler Explorer链接\r\n运行上面的代码，输出结果为： Base::bar() called::bar() called\r\n首先我们明确在dependent和independent函数中，由于我们没有使用任何的作用域解析操作符::，对bar进行的查找都是无限定名字查找（UDL，Unqualified\r\nName\r\nLookup）。同时，在dependent中，bar是一个依赖于Base&lt;T&gt;的待决名，而在independent中，bar是一个非待决名。非待决名和待决名的查找规则有所不同，高度概括地说：\r\n\r\n非待决名在模版定义的时候就进行无限定名字查找。\r\n待决名的名字查找会推迟到模板实例化时。\r\n\r\n这称作二阶段名字查找（Two-phase Name Lookup）\r\n所以对于independent中对bar的调用，编译器在该类模板定义时就会进行查找。按照UDL的查找顺序，编译器首先会在本类中查找（注意不会查找父类Base&lt;T&gt;），在本类中查找不到bar后编译器会进一步查找全局命名空间，随后就会查找到在全局命名空间中的bar。\r\n而对于dependent中对bar的调用，编译器会在实例化，即main函数中Foo&lt;void&gt;一行时进行查找，由于所有模板形参都已确定，编译器可以对父类进行查找，所以会查找到父类中对bar。\r\n模板特化与部分特化\r\n模板全特化\r\n模板特化（Template\r\nSpecialization），说的直白点就是让模板能够对某些模板实参进行特殊处理，以前文中的FixedArray为例（为了简洁我们将其长度固定为8）：\r\ntemplate &lt;typename ElementT&gt;struct FixedArray &#123;\tElementT storage[8];&#125;;\r\n我们都知道在C++中，一个bool需要占用完整的一个字节，所以FixedArray&lt;bool&gt;就会占用8个字节。但是bool的信息存储只需要用到一个字节中的一位，所以我们想通过某种方式来优化FixedArray&lt;bool&gt;，来使其只占用一个字节，这时候我们就要用到模板全特化（又称模板显式特化，Explicit\r\n(Full) Template Specialization）： //模板主定义template &lt;typename ElementT&gt;struct FixedArray &#123;\tElementT storage[8];&#125;;//针对bool特化模板template &lt;&gt;struct FixedArray&lt;bool&gt; &#123;\tstd::int8_t storage;&#125;;auto main() -&gt; int &#123;\tFixedArray&lt;int&gt; array1;\tFixedArray&lt;bool&gt; array2;\tstd::println(&quot;&#123;&#125; &#123;&#125;&quot;, sizeof(array1), sizeof(array2));&#125; Compiler Explorer链接\r\n可以看到，array2的大小仅为一个字节，说明我们的模板特化起了作用。\r\n全特化函数模板\r\n对于函数模板的全特化来说，其语法如下： template &lt;&gt;函数定义（或声明）\r\n如果模板实参能从函数参数列表中推导，那么我们可以省略模板实参：\r\n//模板主定义template &lt;typename T&gt;auto foo(T t) -&gt; T &#123;\treturn t;&#125;//T=int的全特化template &lt;&gt;auto foo&lt;int&gt;(int t) -&gt; int &#123;\treturn 42;&#125;//T=double的全特化，由于可以推导模板实参所以不必指定template &lt;&gt;auto foo&lt;double&gt;(double t) -&gt; double &#123;\treturn 114.514;&#125;auto main() -&gt; int &#123;\tstd::println(&quot;&#123;&#125; &#123;&#125; &#123;&#125;&quot;, foo(true), foo(1), foo(4.2));&#125; Compiler\r\nExplorer链接\r\n注意一点，所有的模板特化都应该位于在第一次会引起隐式实例化的使用前，比如下面的代码不能通过编译：\r\n//模板主定义template &lt;typename T&gt;auto foo(T t) -&gt; T &#123;\treturn t;&#125;auto main() -&gt; int &#123;\t//引发foo&lt;int&gt;隐式实例化\tstd::println(&quot;&#123;&#125;&quot;, foo(1));&#125;//T=int的全特化，由于特化位于隐式实例化之后，所以编译不通过template &lt;&gt;auto foo&lt;int&gt;(int t) -&gt; int &#123;\treturn 42;&#125;\r\n注意要分清楚全特化与显式实例化之间的差异，两者的语法很接近，但特化的函数模板与模板主定义之间没有关系，而显式实例化是基于模板主定义进行的实例化：\r\n//模板主定义template &lt;typename T&gt;auto foo(T t) -&gt; T &#123;\treturn t;&#125;//这是一个特化，注意template后面带有一对空尖括号template &lt;&gt;auto foo(int t) -&gt; int &#123;\treturn 42;&#125;//这是一个显式实例化，注意template后面直接跟着函数声明template auto foo(double) -&gt; double;\r\n注意，特化的函数模板可以跟主模板具有不同的说明符（inline/constexpr/constinit/conseval）：\r\ntemplate &lt;typename T&gt;auto foo(T) -&gt; void &#123;&#125;template &lt;&gt;inline auto foo(int) -&gt; void &#123;&#125; template &lt;typename T&gt;inline auto bar(T) -&gt; T &#123;&#125;template &lt;&gt;auto bar(int) -&gt; int &#123;&#125;         // OK，没有内联\r\n全特化类模板\r\n类模板全特化的语法与函数模板类似： template &lt;&gt;类定义（或声明）\r\n一个简单的小例子（虽说这里value应该是constexpr变量，但是由于我们还没有讲到相关知识，故省略）：\r\n//类模板主定义template &lt;typename T&gt;struct isVoid &#123;\tinline static bool value = false;&#125;;//T=void全特化template &lt;&gt;struct isVoid&lt;void&gt; &#123;\tinline static bool value = true;&#125;;auto main() -&gt; int &#123;\tstd::println(&quot;&#123;&#125; &#123;&#125;&quot;, isVoid&lt;int&gt;::value, isVoid&lt;void&gt;::value);&#125; Compiler\r\nExplorer链接\r\n注意，类模板的特化相当于一个全新的，与原类模板主声明无关的类。我们可以随意向类模板的特化中添加成员：\r\n//类模板主定义template &lt;typename T&gt;struct MathUtils &#123;\tauto abs(T t) -&gt; T &#123; \t\tstd::println(&quot;MathUtils&lt;WhatSoEver&gt;::abs() called&quot;);\t\treturn std::abs(t); \t&#125;&#125;;//T=int全特化template &lt;&gt;struct MathUtils&lt;int&gt; &#123;\tauto abs(int t) -&gt; int &#123; \t\tstd::println(&quot;MathUtils&lt;int&gt;::abs() called&quot;);\t\treturn t &lt; 0 ? -t : t; \t&#125;\tauto sqrt(int t) -&gt; int &#123; return std::sqrt(t); &#125;&#125;;auto main() -&gt; int &#123;\tMathUtils&lt;double&gt; utils_double;\tMathUtils&lt;int&gt; utils_int;\tutils_double.abs(-4.2);\tutils_int.abs(-114514);\t//sqrt只存在于MathUtils&lt;int&gt;特化中，故下面这行代码无法通过编译\t//utils_double.sqrt(4.2);\tutils_int.sqrt(4);&#125; Compiler\r\nExplorer链接\r\n全特化类模板成员\r\n在类体外定义显式特化的类模板的成员时，不需要使用template&lt;&gt;前缀，除非它是某个被特化为类模板的显式特化的成员类模板的成员：\r\n- 成员函数及成员函数模板： //类模板主定义template &lt;typename T&gt;struct Foo &#123;\tauto bar() -&gt; void &#123;&#125;\tauto meow() -&gt; void &#123;&#125;\ttemplate &lt;typename U&gt;\tauto templated() -&gt; void &#123;&#125;&#125;;//T=void全特化template &lt;&gt;struct Foo&lt;void&gt; &#123;\t//可以在类内定义\tauto bar() -&gt; void &#123;&#125;\tauto meow() -&gt; void;\ttemplate &lt;typename U&gt;\tauto templated() -&gt; void;&#125;;//也可以在类内声明，类外定义auto Foo&lt;void&gt;::meow() -&gt; void &#123;&#125;//注意无论是类内还是类外定义，三个函数都不需要使用template &lt;&gt;做前缀//这是由于三个函数（模板）都在全特化的类模板中声明，故都可以看作一个独立类（即类模板的特化）的成员，故不需要额外的特化前缀template &lt;typename U&gt;auto Foo&lt;void&gt;::templated() -&gt; void &#123;&#125; - 成员类及成员类模板：\r\ntemplate &lt;typename T&gt;struct Foo &#123;\t//成员类\tstruct Bar &#123;&#125;;\t//成员类模板\ttemplate &lt;typename U&gt;\tstruct Meow &#123;&#125;;&#125;;template &lt;&gt;struct Foo&lt;int&gt; &#123;\tstruct Bar;\ttemplate &lt;typename U&gt;\tstruct Meow &#123;&#125;;&#125;;//同样的，由于都已在类模板的全特化中声明，Bar和Meow均不需要template &lt;&gt;前缀struct Foo&lt;int&gt;::Bar &#123;&#125;;\r\n我们还可以通过对类模板的隐式实例化进行特化来单独特化类模板中的部分成员，此时我们才需要用到template &lt;&gt;前缀：\r\ntemplate &lt;typename T&gt;struct Foo &#123;\tauto foo() -&gt; void &#123;\t\tstd::println(&quot;Unspecialized foo()&quot;);\t&#125;\tauto bar() -&gt; void &#123;\t\tstd::println(&quot;Unspecialized bar()&quot;);\t&#125;&#125;;//注意，为了对类模板的隐式实例化进行特化，这里我们需要加上template &lt;&gt;前缀template &lt;&gt;auto Foo&lt;int&gt;::foo() -&gt; void &#123;\tstd::println(&quot;Specialized foo()&quot;);&#125;auto main() -&gt; int &#123;\tFoo&lt;void&gt; foo_void;\tFoo&lt;int&gt; foo_int;\tfoo_void.foo();\tfoo_void.bar();\tfoo_int.foo();\tfoo_int.bar();&#125; Compiler\r\nExplorer链接\r\n模板特化也可以进行嵌套，比如特化一个类模板中的特定成员函数模板：\r\ntemplate &lt;typename T&gt;struct Foo &#123;\ttemplate &lt;typename U&gt;\tauto bar() -&gt; void &#123;&#125;&#125;;template &lt;&gt;template &lt;&gt;auto Foo&lt;int&gt;::bar&lt;double&gt;() -&gt; void &#123;&#125;//针对Foo&lt;int&gt;::bar&lt;double&gt;进行特化 #### 全特化变量模板\r\n全特化变量模板会在后续章节中深入讲解，此处仅提供一个简单的例子说明：\r\n//变量模板主定义template &lt;typename T&gt;std::string_view getTypeName = &quot;&lt;unknown type&gt;&quot;;//T=int全特化template &lt;&gt;std::string_view getTypeName&lt;int&gt; = &quot;int&quot;;//T=double全特化template &lt;&gt;std::string_view getTypeName&lt;double&gt; = &quot;double&quot;;auto main() -&gt; int &#123;\tstd::println(&quot;&#123;&#125; &#123;&#125; &#123;&#125;&quot;, getTypeName&lt;void&gt;, getTypeName&lt;int&gt;, getTypeName&lt;double&gt;);&#125; Compiler\r\nExplorer链接\r\n模板部分特化\r\n模板全特化，是提供一个定义给一组具体的模板实参。而所谓模板部分特化（又译模板偏特化，Partial\r\nTemplate\r\nSpecialization）是将一个定义提供给一组具有某些特征的模板实参。我们以一个例子做引入：\r\ntemplate &lt;typename T&gt;std::string_view typeCategory = &quot;&lt;unknown&gt;&quot;;//对所有指针进行部分特化template &lt;typename T&gt;std::string_view typeCategory&lt;T *&gt; = &quot;&lt;pointer&gt;&quot;;//对所有左值引用进行部分特化template &lt;typename T&gt;std::string_view typeCategory&lt;T &amp;&gt; = &quot;&lt;Lvalue Reference&gt;&quot;;//对所有右值引用进行部分特化template &lt;typename T&gt;std::string_view typeCategory&lt;T &amp;&amp;&gt; = &quot;&lt;Rvalue Reference&gt;&quot;;auto main() -&gt; int &#123;\tstd::println(\t\t&quot;&#123;&#125; &#123;&#125; &#123;&#125; &#123;&#125; &#123;&#125;&quot;,\t\ttypeCategory&lt;int&gt;,//&lt;unknown&gt;\t\ttypeCategory&lt;int *&gt;,//&lt;pointer&gt;\t\ttypeCategory&lt;int const *&gt;,//&lt;pointer&gt;\t\ttypeCategory&lt;int &amp;&gt;,//&lt;Lvalue Reference&gt;\t\ttypeCategory&lt;int &amp;&amp;&gt;//&lt;Rvalue Reference&gt;\t);&#125; Compiler\r\nExplorer链接\r\n在有多个模板形参时，完全指定一部分模板实参也是偏特化的一种：\r\n//类模板主定义template &lt;typename T1, typename T2&gt;struct Foo &#123;\tstatic auto foo() -&gt; void &#123;\t\tstd::println(&quot;Foo&lt;T1, T2&gt;::foo() called&quot;);\t&#125;&#125;;//对T1=int进行偏特化，T2不做指定template &lt;typename T2&gt;struct Foo&lt;int, T2&gt; &#123;\tstatic auto foo() -&gt; void &#123;\t\tstd::println(&quot;Foo&lt;int, T2&gt;::foo() called&quot;);\t&#125;&#125;;auto main() -&gt; int &#123;\tFoo&lt;void, double&gt;::foo();\tFoo&lt;float, int&gt;::foo();\t//偏特化匹配所有第一个模板实参为int的实例化\tFoo&lt;int, void&gt;::foo();\tFoo&lt;int, char&gt;::foo();&#125; Compiler\r\nExplorer链接\r\n注意，与模板全特化不同，模板偏特化只能特化类模板与变量模板。\r\n模板偏特化这一特性十分强大，这相当于允许我们对模板实参进行模式匹配，并根据不同的匹配结果进行不同的处理，这一特性直接构成了我们后面要讲的模板元编程的核心工具之一。\r\n模板偏特化的语法与全特化大致相同，故此处不再赘述。\r\n","tags":["C++","Metaprogramming","Tutorial","Template"]},{"title":"Principles of Database Systems (H) Lecture 2 Notes","url":"/2025/09/19/dbh-02/","content":"Introduction to SQL\r\nTo manage data in a database, a full-blown language will introduce\r\ntoo much complexity to both users and database systems. What we need is\r\na special DSL that specializes in data query and manipulation, hence SQL\r\nwas invented. The SQL is composed of four parts:\r\n\r\nData Manipulation Language, DML\r\nData Definition Language, DDL\r\nData Query Language, DQL\r\nData Control Language, DCL\r\n\r\nThe most common used one is the DQL, or more specifically, the\r\nSELECT statement: SELECT column_name FROM table_name WHERE conditions The DDL, deals with tables\r\nand other database objects. There are three statements:\r\nCREATE, ALTER, and DROP:\r\n-- Create a tableCREATE TABLE Users (\tid INT AUTO_INCREMENT PRIMARY KEY,\tusername VARCHAR(50) NOT NULL,\temail VARCHAR(100) NOT NULL,\tbirthdate DATE,\tis_active BOOLEAN DEFAULT TRUE);ALTER TABLE table_name-- Add columnADD COLUMN birth_date DATE;-- Modify column's datatypeMODIFY COLUMN salary DECIMAL(10,2);-- Change column's nameCHANGE COLUMN old_column_name new_column_name datatype;-- Delete a columnDROP COLUMN birth_date;-- Add a primary keyADD PRIMARY KEY (column_name);-- Add a foreign keyADD CONSTRAINT fk_customerFOREIGN KEY (customer_id)REFERENCES customers (customer_id);-- Rename the tableRENAME TO staff; The DML includes INSERT, UPDATE,\r\nand DELETE: INSERT INTO table_name (column1, column2, ...) VALUES (value1, value2, ...);UPDATE table_name SET column=value WHERE conditions;DELETE FROM table_name WHERE conditions\r\n\n  \n    \n      Caution\r\n\n    \n    \n      The SQL language lacks standardization, and multiple variants exist.\r\nIn this course, we will stick to the one that PostgreSQL uses.\r\n\n    \n  \r\n\n  \n    \n      Caution\r\n\n    \n    \n      The principle of “all rows are distinct” can be enforced for tables\r\nin SQL, but not enforced for query results in SQL.\r\n\n    \n  \r\n\n  \n    \n      Caution\r\n\n    \n    \n      The SQL keyword and identifier are case-insensitive.\r\n\n    \n  \r\nText datatypes:\r\n\r\nchar(length): Fixed-length strings.\r\nvarchar(max_length): Variable-length strings that are\r\nlimited by a maximum length.\r\nvarchar2(max_length): Only appears in Oracle SQL.\r\ntext: Stores big text (~10KB).\r\nclob: Stores much bigger texts (up to 4GB).\r\nnchar/nvarchar: Unicode equivalent of\r\nchar/varchar.\r\n\r\nNumerical datatypes:\r\n\r\nint: 4-byte integer type.\r\nfloat: IEEE-compliant 4-byte floating point type.\r\nnumeric(precision, scale): precision is\r\nthe number of digits, and the scale is the number of these\r\ndigits after the decimal point.\r\n\r\nData datatypes:\r\n\r\ndate: Date and times (down to seconds with Oracle, not\r\nwith others).\r\ndatatime: Also date and times, down to seconds (other\r\nthan Oracle, DB2, and PostgreSQL)\r\ntimestamp: down to  second, replace datetime in\r\nDB2/PostgreSQL.\r\n\r\nBinary datatypes\r\n\r\nraw(max_length): Fixed-length binaries.\r\nvarbinary(max_length): Variable-length binaries that\r\ncan have a maximum length of max_length bits.\r\nblob: Binary equivalent of clob.\r\nbytea: Same as blob, used in\r\nPostgreSQL.\r\n\r\nBy default, columns are permitted to have a null as it\r\nvalue. To prohibit this, use NOT NULL decorator when\r\ncreating the column.\r\nSome DBMS systems support adding comments to database objects,\r\ne.g. in PostgreSQL: COMMENT ON object_type object_name is string|null\r\nConstraints\r\nFor example, we want create a table for storing personal information:\r\nCREATE TABLE people(\tpeopleid int,\tfirst_name varchar(30),\tsurname varchar(30),\tborn numeric(4),\tdied numeric(4)) But this will allow that rows contain arbitrary\r\nnull data, so we need to add not null\r\nconstraints for mandatory data: CREATE TABLE people(\tpeopleid int not null,\tfirst_name varchar(30),\tsurname varchar(30) not null,\tborn numeric(4),\tdied numeric(4)) And we need to add a\r\nprimary key, so we use the primary key constraint:\r\nCREATE TABLE people(\tpeopleid int not null\t\tprimary key,\tfirst_name varchar(30),\tsurname varchar(30) not null,\tborn numeric(4),\tdied numeric(4)) Note that the primary key constraint implies\r\nnot null. SQL will not warn about redundant constraints.\r\nAnother potential problem is the capitalization. For example, the three\r\nstrings:\r\n\r\n\"FOO\"\r\n\"foo\"\r\n\"Foo\"\r\n\r\nis considered to be not equal in Oracle, PostgreSQL, and\r\nDB2, but they are equal in SQL Server, MySQL, and SQLite. So we\r\nneed to standardize the names, using unique constraint for\r\nuniqueness and check for standardization: CREATE TABLE people(\tpeopleid int not null\t\tprimary key,\tfirst_name varchar(30),\t\tcheck(first_name = upper(first_name)),\tsurname varchar(30) not null,\t\tcheck(surname = upper(surname)),\tborn numeric(4),\tdied numeric(4),\tunique(first_name, surname))\r\n","tags":["Principles of Database Systems","Notes"]},{"title":"Principles of Database Systems (H) Lecture 1 Notes","url":"/2025/09/12/dbh-01/","content":"Lecture 1\r\nIntroduction to Databases\r\nA modern database system is a complex software\r\nsystem whose task is to manage a large, complex collection of data.\r\nThe central idea was to have one system doing once and for all the\r\nboring data storage/retrieval part.\r\nA bit of history\r\nIn early days database applications were built directly on top of\r\nfile systems. However, it suffers from many issues, including:\r\n\r\nData redundancy and inconsistency\r\nDifficulty in accessing data\r\nData isolation\r\nIntegrity problems\r\nAtomicity of updates\r\nConcurrent access by multiple users\r\nSecurity problems\r\n\r\n1980s:\r\n\r\nResearch relational prototypes evolve into commercial systems\r\n\r\nSQL becomes industrial standard\r\n\r\nParallel and distributed database systems\r\n\r\nWisconsin, IBM, Teradata\r\n\r\nObject-oriented database systems\r\n\r\n1990s:\r\n\r\nLarge decision support and data-mining\r\napplications\r\nLarge multi-terabyte data warehouses\r\nEmergence of Web commerce\r\n\r\n2000s:\r\n\r\nBig data storage systems\r\n\r\nGoogle BigTable, Yahoo PNuts, Amazon\r\nNoSQL systems\r\n\r\nBig data analysis: beyond SQL\r\n\r\nMap Reduce, etc.\r\n\r\n\r\n2010s:\r\n\r\nSQL reloaded\r\n\r\nSQL front end to Map Reduce systems\r\nMassively parallel database systems\r\nMulti-core main-memory databases\r\n\r\n\r\nRelational Database\r\nThe relational database is based on the relational model of\r\ndata, it:\r\n\r\norganizes data into one or more tables\r\nrows are also called records or tuples\r\ncolumns are also called attributes\r\n\r\nEach column in the table stores a piece of data, and one row\r\nrepresents a “known fact”. For example:\r\n\r\n\r\n\r\n\r\nName\r\nBirthday\r\nBirthplace\r\n\r\n\r\n\r\n\r\nPerson1\r\n\r\n\r\n\r\n\r\n\r\nPerson2\r\n\r\n\r\n\r\n\r\n\r\nPerson3\r\n\r\n\r\n\r\n\r\n\r\n\r\nAll the pieces of data in a row are related, hence “relational”.\r\nKeys\r\nThe relational database prohibits different records. Since there are\r\nno duplicate, we need to identify what allows you to differentiate one\r\nrow from another. It may be one column, or one set of columns, known\r\ncollectively as a key.\r\nIt may happen that several different keys are available. One of them\r\nis (arbitrarily) singled out and called the primary key. We\r\nusually choose the simplest one. In practice, to simplify the issue, we\r\noften add a numerical attribute (often a increasing integer) as the\r\nprimary key.\r\nNormalization\r\nOne common problem with databases is that data may be written at\r\ndifferent time by different people. If you let them write data in the\r\nway they want, it will make data retrieval very difficult because when\r\nthey search, computers may compare the data literally. So we\r\nneed to standardize your data, a process also known as\r\nnormalization.\r\n\n  \n    \n      First Normal Form\r\n\n    \n    \n      First Normal Form (1NF): each column should only contain one\r\npiece of information.\r\n\r\nEvery Column Should Have Single Values\r\nAll Values in a Column Should Be of the Same Type\r\nEvery Column Must Have a Unique Name\r\nThe Order of Data Doesn’t Matter\r\n\r\n\n    \n  \r\n\n  \n    \n      Second Normal Form\r\n\n    \n    \n      Second Normal Form (2NF): Every non-key attribute must\r\nprovide a fact about the key. A database conforms to the 2NF if:\r\n\r\nIt is already in 1NF.\r\nIt has no partial dependency: Every non-key attribute must depend on\r\nthe entire primary key.\r\n\r\n\n    \n  \r\n\n  \n    \n      Third Normal Form\r\n\n    \n    \n      Third Normal Form (3NF): Every non-key attribute must\r\nprovide a fact about the key, the whole key, and\r\nnothing but the key. A database conforms to the 3NF if:\r\n\r\nIt is already in 2NF.\r\nIt has no transitive dependency: No non-key attribute should depend\r\non another non-key attribute. All non-key attributes must depend only on\r\nthe primary key.\r\n\r\n\n    \n  \r\n","tags":["Principles of Database Systems","Notes"]},{"title":"Principles of Database Systems (H) Lecture 6 Notes","url":"/2025/10/21/dbh-06/","content":"Postprocessing Queries\r\nSorting\r\nWe have a single construct in SQL that sorts the result set:\r\nSELECT title, year_released\tFROM movies\tORDER BY year_released This will return all movies, starting with the oldest\r\none. The default order is ascending, we can also use the keywords\r\nDESC and ASC to control it: SELECT c.country_name, m.title, m.year_released\tFROM movies m\t\tINNER JOIN countries c\t\t\tON c.country_code = m.country\tWHERE m.movie IN\t\t(SELECT DISTINCT c.movieid\t\t\tFROM credits c\t\t\t\tINNER JOIN people p\t\t\t\t\tON p.peopleid = c.peopleid\t\t\tWHERE c.credited_as = 'A'\t\t\t\tAND p.birth_year &gt;= 1970)\tORDER BY c.country_name ASC, m.year_released DESC, m.title ASC\r\nThe specific ordering depends on the data type, e.g. the strings are\r\nsorted lexicographically. But it can be troublesome when\r\nNULLs are in the comparison. In this case, the ordering\r\ndepends on specific behaviour of DBMS:\r\n\r\nNULL is smaller than everything: SQL Server, MySQL,\r\nSQLite\r\nNULL is greater than everything: DB2, Oracle,\r\nPostgreSQL\r\n\r\nSometimes we may found the orderings of texts vary across different\r\nlanguages or cultures. To solve this, we need to use collation:\r\n-- For PostgreSQL, SQL Server, MySQLCREATE TABLE table_name(\tsome_text_column varchar(100)\t\tCOLLATE collation_name NOT NULL)-- For OracleORDER BY nls_sort(some_text_column, 'collation_name')\r\nAnother problem is that when we want orderings other than the\r\nascending and descending behavior. For example, to order the credit of\r\npeople in movies: ORDER BY\tCASE credited_as\t\tWHEN 'D' THEN 1\t\tWHEN 'P' THEN 2\t\tWHEN 'A' THEN 3\tEND\r\nAnother problem is that we may need to retrieve only a “slice” of\r\nsorted data, e.g. successive pages on websites. We can leverage the\r\nLIMIT construct to achieve this: -- PostgreSQL, MySQL, SQLiteSELECT title, country, year_released\tFROM movies\tORDER BY title\tLIMIT 10\t-- DB2, Oracle, PostgreSQLSELECT title, country, year_released\tFROM movies\tORDER BY title\tFETCH FIRST 10 ROWS ONLY\t-- SQLServerSELECT TOP 10 title, country, year_released\tFROM movies,\tORDER BY title If we want\r\nto “skip” the first few elements, we can use the OFFSET\r\nkeyword: -- PostgreSQL, MySQL, SQLiteSELECT title, country, year_released\tFROM movies\tORDER BY title\tLIMIT 10 OFFSET 20\t-- DB2, Oracle, PostgreSQLSELECT title, country, year_released\tFROM movies\tORDER BY title\tOFFSET 20\tFETCH FIRST 10 ROWS ONLY\r\nThere are also many cases when plain ordering isn’t satisfying. For\r\nexample, messages in a forum thread can form a very complex hierarchy,\r\ni.e. tree-like data. DBMS is known to struggle when dealing with data\r\nlike this. This is the well-known BOM (Bill Of Materials) problem.\r\nOne way to solve this problem is by using the “materialized path”,\r\nturing the “ancestor” into a attribute.\r\nOracle has implemented another solution by dynamic ordering:\r\nSELECT message\tFROM forum_posts\tCONNECT BY answered_postid = prior postid\tSTART WITH answered_postid IS NULL\t\tAND topicid = ...\tORDER siblings BY postid\r\nMost DBMSs handle hierarchies through recursive queries.\r\nWindow Functions\r\nWe have seen so far two kinds of functions:\r\n\r\nScalar functions that operate on values in the current\r\nrow\r\nAggregate function that operate on sets of rows.\r\n\r\nThe problem with aggregate functions is that they ignore details of\r\ndata. If I ask for the year of the oldest movie per country, the\r\ndatabase will only return a country and a year, and nothing else.\r\nIf we want more details with aggregate functions, the only option is\r\nto join their output to the same table: SELECT a.country, a.title, a.year_released\tFROM movies a\t\tINNER JOIN\t\t\t(SELECT country, min(year_released) minyear\t\t\t\tFROM movies\t\t\t\tGROUP BY country) b\t\t\tON b.country = a.country\t\t\t\tAND b.minyear = a.year_released Window functions\r\nsolve this problem by returning a result for a single row and computing\r\nthe result on several rows. With DBMS products that support window\r\nfunctions, every aggregate function can be used as a window function:\r\nSELECT country, title, year_released,\tmin(year_released)\t\tOVER (PARTITION BY country) earliest_year\tFROM movies Thus, this query returns two years for every movie:\r\n\r\nthe release year\r\nearliest movie for the same country was released\r\n\r\nBut this will return many unnecessary results. We need to limit\r\noutput to those movies for which the year of release happens to be the\r\nsame as the earliest one for their country: SELECT m.country, m.title, m.year_released\tFROM \t\t(SELECT country, title, year_released,\t\t\tmin(year_released)\t\t\t\tOVER(PARTITION BY country) earliest_year\t\t\tFROM movies) m\tWHERE m.year_released = m.earliest_year Be careful\r\nthat if we use filters in the inner query: SELECT m.country, m.title, m.year_released\tFROM \t\t(SELECT country, title, year_released,\t\t\tmin(year_released)\t\t\t\tOVER(PARTITION BY country) earliest_year\t\t\tFROM movies\t\t\tWHERE title != 'example') m\tWHERE m.year_released = m.earliest_year it will return\r\nthe second earliest movie! This is because the window functions are a\r\npostprocessing function on the result set. The corrected\r\nversion is SELECT m.country, m.title, m.year_released\tFROM \t\t(SELECT country, title, year_released,\t\t\tmin(year_released)\t\t\t\tOVER(PARTITION BY country) earliest_year\t\t\tFROM movies) m\tWHERE m.year_released = m.earliest_year\t\tAND title != 'example'\r\nAlso note that we cannot “flatten” the nested query into one query.\r\nThis is mainly because the logical processing order of SQL:\r\n\r\nFROM/JOIN\r\nWHERE\r\nGROUP BY\r\nHAVING\r\nWindow Functions\r\nSELECT\r\nORDER BY\r\n\r\nThen if we try to move the window function to the outer query:\r\nSELECT country, title, year_released\tFROM movies\tWHERE year_released = MIN(year_released) OVER(PARTITION BY country) Since window functions are processed after the\r\nWHERE clause, this will cause the WHERE clause\r\ndepends on unavailable data!\r\nWindow functions always operate against rows that belong to a result\r\nset. One related characteristics is that they can only appear after the\r\nSELECT, not in the WHERE clause, and there is\r\nnothing with them similar to HAVING with aggregate\r\nfunctions.\r\nWindow functions+OVER can be rewrite by using\r\nGROUP BY+JOIN: SELECT a.country, a.title, a.year_released\tFROM movies a\t\tINNER JOIN\t\t\t(SELECT country, min(year_released) earliest_year\t\t\t\tFROM movies\t\t\t\tGROUP BY country) b\t\t\tON b.country = a.country\t\t\t\tAND b.earliest_year = a.year_released\tWHERE a.title != 'example'\r\nSimilar to that we can have an aggregate function without a\r\nGROUP BY clause when we want only one result for the whole\r\ntable, we can have an empty OVER clause to indicate that\r\nthe window function should compute the result over all rows selected.\r\nThis is frequently used in operations such as computing a value as a\r\npercentage of the total: SELECT country_name, cnt as number_of_movies,\t   round(100 * cnt / sum(cnt) over (), 0) as percentage\tFROM \t\t(SELECT c.country_name, coalesce(m.cnt, 0) cnt\t\t\tFROM countries c\t\t\t\tLEFT OUTER JOIN \t\t\t\t\t(SELECT country, count(*) cnt\t\t\t\t\t\tFROM movies\t\t\t\t\t\tGROUP BY country) m\t\t\t\t\tON m.country = c.country_code) q\tORDER BY country_name\r\n\n  \n    \n      Tip\r\n\n    \n    \n      When there is an ORDER BY clause we cannot start\r\nreturning rows before you have seen all of them, so we may count them\r\ntoo when sorting, and the marginal cost of the window function is near\r\nzero.\r\n\n    \n  \r\nWe can rewrite the query by using a CROSS JOIN:\r\nSELECT country_name, cnt as number_of_movies,\t   round(100 * cnt / t.movie_count, 0) as percentage\tFROM \t\t(SELECT c.country_name, coalesce(m.cnt, 0) cnt\t\t\tFROM countries c\t\t\t\tLEFT OUTER JOIN \t\t\t\t\t(SELECT country, count(*) cnt\t\t\t\t\t\tFROM movies\t\t\t\t\t\tGROUP BY country) m\t\t\t\t\tON m.country = c.country_code) q\t\tCROSS JOIN\t\t\t(SELECT count(*) movie_count\t\t\t\tFROM movies) t\tORDER BY country_name\r\nThere are\r\n","tags":["Principles of Database Systems","Notes"]},{"title":"Principles of Database Systems (H) Lecture 5 Notes","url":"/2025/10/10/dbh-05/","content":"Advanced Queries, Part 2\r\nJOIN\r\nOuter JOIN\r\nAn example of using JOIN is to look for the directors of\r\nAmerican movies released in 2018:\r\nSELECT m.year_released, m.title, p.first_name, p.surname\tFROM movies m\tJOIN credits c\t\tON c.movieid = m.movieid\tJOIN people p\t\tON p.peopleid = c.peopleidWHERE c.credited_as = 'D'\tAND m.country = 'us'\tAND year_released = 2018\r\nJOIN conditions can also be interpreted as filtering\r\nconditions: SELECT m.year_released, m.title, p.first_name, p.surname\tFROM movies m, credits c, people pWHERE c.movieid = m.movieid\tAND p.peopleid = c.peopleid\tAND c.credited_as = 'D'\tAND m.country = 'us'\tAND year_released = 2018 Since the boolean operators have\r\nshort-circuiting behavior, the two statements are equivalent.\r\nBut one issue with the statement is that it will only return the\r\nmovies that have known directors in the database.\r\nInner JOIN\r\nIf we want to see all 2018 American movies in the database, we need\r\nto resort to an extended kind of JOIN, known as the outer\r\nJOIN (the regular JOIN is also known as the\r\ninner JOIN). An outer JOIN build the result\r\nset with NULLs if we can’t find a match in the outer joined\r\ntable.\r\nThere are three kinds of outer JOINs:\r\n\r\nLeft outer JOIN\r\nRight outer JOIN\r\nFull outer JOIN The mostly commonly used one is the\r\nleft outer JOIN. A right outer JOIN can always\r\nbe expressed in a left one, and the full outer JOINs are\r\nrarely used.\r\n\r\nFor example, let try to count how many movies we have per country. We\r\nmay have no movies from smaller countries, or newer countries that\r\nhaven’t produced one movie yet.\r\nWe start by counting in movies how many movies we have\r\nper country. This of course will only return countries that have movies.\r\nIf we use an inner JOIN, they will be the only ones we’ll\r\nsee. SELECT c.country_name, x.number_of_movies\tFROM countries c\tINNER JOIN\t(SELECT country as country_code, count(*) as number_of_movies\t\tFROM movies\t\tGROUP BY country) x\t\tON x.country_code = c.country_code With a left outer JOIN, we’ll see all\r\ncountries in the countries table appear. Note that the\r\ntable that we want to see listed in full is always with a left outer\r\nJOIN and is the first one after the FROM\r\nkeyword: SELECT c.country_name, x.number_of_movies\tFROM countries c\tLEFT OUTER JOIN\t(SELECT country as country_code, count(*) as number_of_movies\t\tFROM movies\t\tGROUP BY country) x\t\tON x.country_code = c.country_code If we don’t want to see the NULL’s,\r\nwe can use a CASE construct: SELECT c.country_name,\t\tCASE\t\t\tWHEN x.number_of_movies IS NULL then 0\t\t\tELSE x.number_of_movies\t\tEND number_of_movies\tFROM countries c\tLEFT OUTER JOIN\t(SELECT country as country_code, count(*) as number_of_movies\t\tFROM movies\t\tGROUP BY country) x\t\tON x.country_code = c.country_code Or we can use\r\nthe more concise coalesce() function that takes an\r\nindeterminate number of parameters and returns the first one that isn’t\r\nNULL: SELECT c.country_name, coalesce(x.number_of_movies, 0) number_of_movies\tFROM countries c\tLEFT OUTER JOIN\t(SELECT country as country_code, count(*) as number_of_movies\t\tFROM movies\t\tGROUP BY country) x\t\tON x.country_code = c.country_code\r\nLet see a comparison: SELECT count(*)FROM movies m\tINNER JOIN countries c\tON m.country = c.country_code\tSELECT count(*)FROM movies m\tLEFT OUTER JOIN countries c\tON m.country = c.country_code\tSELECT count(*)FROM movies m\tLEFT OUTER JOIN countries c\tON m.country != c.country_code\t The three statements will\r\nreturn:\r\n\r\nThe row count of movies\r\nThe row count of movies\r\n\r\n\r\nThe inner JOIN and outer JOIN here are\r\nequivalent because we have a foreign key constraint (the condition will\r\nalways not NULL).\r\nLet see a more complicate example: find all British movie titles with\r\ndirector names when available. The only place we may lose information is\r\nin the credits table, since the referential integrity\r\nguarantee that every row in credits will have a matching\r\nrow both in movies and people. From this we\r\nknow that the credits must be outer JOINed:\r\nSELECT  m.year_released,\t\tm.title,\t\tm.country,\t\tp.first_name,\t\tp.surname,\t\tc.credited_asFROM movies m\tLEFT OUTER JOIN credits c\t\tON c.movieid = m.movieid\tINNER JOIN people p\t\tON p.peopleid = c.peopleid But there is an issue: the condition on the last line will\r\nfial if we return a NULL from credits. So we\r\nneed a second outer JOIN to return NULL from\r\npeople when there is no row in credits:\r\nSELECT a.year_released, a.title, a.first_name, a.surnameFROM (SELECT m.year_released, m.title, m.country, \t\t\t p.first_name, p.surname, c.credited_as\t  FROM movies m\t\t  LEFT OUTER JOIN credits c\t\t\t  ON c.movieid = m.movieid\t\t  LEFT OUTER JOIN people p\t\t\t  ON p.peopleid = c.peopleid) aWHERE a.credited_as = 'D'\tAND a.country = 'gb' But if the left outer JOIN returns\r\nNULL, then credited_as cannot be\r\n'D' and the movie will disappear. The corrected answer is:\r\nSELECT m.year_released, m.title, p.first_name, p.surnameFROM (SELECT movieid, year_released, title\t  FROM movies\t  WHERE country = 'gb') mLEFT OUTER JOIN (SELECT movieid, peopleid\t\t\t\t FROM credits\t\t\t\t WHERE credited_as = 'D') c\t\t\t   ON c.movieid = m.movieidLEFT OUTER JOIN people p\tON p.peopleid = c.peopleid\r\nSet Operators\r\nThe most common set operator is UNION that takes two\r\nresult sets and combine them into a single result set. For example:\r\nSELECT movieid, title, year_released, countryFROM moviesWHERE country = 'us'\tAND year_released BETWEEN 1940 AND 1949UNIONSELECT movieid, title, year_released, countryFROM moviesWHERE country = 'gb'\tAND year_released BETWEEN 1940 AND 1949 (Using an OR would be more efficient than\r\ntwo separate queries here)\r\nUsing UNION requires two conditions:\r\n\r\nThe two result sets must return the same number of columns, and\r\nthe data types of corresponding columns must match. For example:\r\nSELECT 'regular' AS class,\tmovieid, title, year_releasedFROM moviesUNIONSELECT 'premium' AS class,\tmovieid, title, year_releasedFROM premium_movies If we are sure that the two sets have no duplicates, we\r\ncan use the UNION ALL to avoid the unnecessary duplicates\r\nremoving step:\r\n\r\nSELECT 'regular' AS class,\tmovieid, title, year_releasedFROM moviesUNION ALLSELECT 'premium' AS class,\tmovieid, title, year_releasedFROM premium_movies\r\nOther set operations:\r\n\r\nINTERSECT\r\nEXCEPT/MINUS (in Oracle)\r\n\r\nThe INTERSECT is equivalent to an inner\r\nJOIN, and the EXCEPT is equivalent to an outer\r\nJOIN. \n  \n    \n      Tip\r\n\n    \n    \n      Always prefer JOIN operations.\r\n\n    \n  \r\nLet’s take an example and find country codes that are both in\r\nmovies and countries: SELECT country_codeFROM countriesINTERSECTSELECT DISTINCT countryFROM movies Or the\r\nequivalent version using inner JOIN: SELECT c.country_codeFROM countries c\tINNER JOIN (SELECT DISTINCT country FROM movies) m\t\tON m.country = c.country_code\r\nAnother example: find countries for which we haven’t any movie.\r\nSELECT country_codeFROM countriesEXCEPTSELECT DISTINCT countryFROM movies Or the equivalent version using outer JOIN:\r\nSELECT c.country_codeFROM countries c\tLEFT OUTER JOIN (SELECT DISTINCT country FROM movies) m\t\tON m.country = c.country_codeWHERE m.country IS NULL\r\nSubqueries\r\nOne way to perform a JOIN is to use a nested loop that\r\nscans a table and for each row look for mathcing values in the column of\r\nthe other table on which the JOIN is performed. So when we\r\nexecute this statement: SELECT m.title, m.year_released, c.country_nameFROM movies m\tJOIN countries c\t\tON c.country_code = m.countryWHERE m.country != 'us' for every row in\r\nmovies the code in column country will be\r\nchecked, and, if it’s not 'us', the corresponding movie\r\ntitle, year_released, and\r\ncountry_name will be fetched from\r\ncountries.\r\nWe can also write the query like this: SELECT m.title, m.year_released,\t(SELECT c.country_name\t FROM countries c\t WHERE c.country_code = m.country) as country_nameFROM movies mWHERE m.country != 'us' The condition\r\nin the subquery is provided by the value in the row from\r\nmovies that is currently inspected. The subquery is said to\r\nbe correlated with the outer query. This is slower because the\r\nsubquery is fired for every returned row.\r\nNote though that this isn’t exactly equivalent to a\r\nJOIN. If we don’t find the country name. the subquery would\r\nreturn NULL. Strictly speaking, a subquery after the\r\nSELECT is more like a left outer JOIN.\r\n\n  \n    \n      Tip\r\n\n    \n    \n      A subquery in the FROM clause cannot be correlated, a\r\nsubquery after SELECT is usually correlated.\r\n\n    \n  \r\nThe IN operator is handy for handling the implicit list\r\ngenerated by a subquery: SELECT country, year_released, titleFROM moviesWHERE country IN\t(SELECT country_code\t FROM countries\t WHERE continent = 'EUROPE') Here the subquery is\r\nuncorrelated.\r\nSome DBMSs even allow comparing a set of column values to the result\r\nof a subquery: (col1, col2) IN (SELECT col3, col4 FROM t WHERE ...) \n  \n    \n      Caution\r\n\n    \n    \n      Using IN on subqueries poses an implicit\r\nDISTINCT constraint in the subquery result.\r\n\n    \n  \r\n\n  \n    \n      Tip\r\n\n    \n    \n      Don’t use DISTINCT if the result set of a query is\r\ndemonstrably unique.\r\n\n    \n  \r\nThe NULL Problem:\r\nArithmetic operators:\r\n\r\ncol+NULL=NULL\r\ncol-NULL=NULL\r\ncol*NULL=NULL\r\ncol/NULL=NULL\r\n\r\nRelational operators:\r\n\r\ncol&gt;NULL=NULL\r\ncol&lt;NULL=NULL\r\ncol&gt;=NULL=NULL\r\ncol&lt;=NULL=NULL\r\ncol!=NULL=NULL\r\ncol is NULL=TRUE or FALSE\r\n\r\nLogical operators:\r\n\r\nTRUE and NULL=NULL\r\nFALSE and NULL=FALSE\r\nTRUE or NULL=TRUE\r\nFALSE or NULL=NULL\r\n\r\ncol IN ('a', 'b', NULL)\r\n\r\nIf col is 'a' or 'b', then\r\nthe result is TRUE.\r\nOtherwise the result is NULL.\r\n\r\ncol NOT IN ('a', 'b', NULL)\r\n\r\nIf col is 'a' or 'b', then\r\nthe result is FALSE.\r\nOtherwise the result is NULL.\r\n\r\nSo a subquery that returns a NULL in a\r\nNOT IN() will always give a NULL condition,\r\nand the result will vanish. For example the following statement will\r\nreturn nothing: SELECT *FROM peopleWHERE first_name NOT IN\t(SELECT first_name\t FROM people\t WHERE born &lt; 1950) We need to explicitly reject the\r\nNULL values: SELECT *FROM peopleWHERE first_name NOT IN\t(SELECT first_name\t FROM people\t WHERE born &lt; 1950\t\t AND first_name IS NOT NULL)\r\nEXISTS\r\nSo far we have only seen uncorrelated subqueires in the\r\nWHERE clause. Correlated queries in the WHERE\r\nclause are used with the EXISTS construct.\r\n\n  \n    \n      Caution\r\n\n    \n    \n      Never use a correlated subquery in IN!\r\n\n    \n  \r\nFor example the movies with at least one actor born in 1970 or later:\r\nSELECT DISTINCT m.titleFROM movies mWHERE EXISTS\t(SELECT NULL\t FROM credits c\t\t INNER JOIN people p\t\t\t ON p.peopleid = c.peopleid\t WHERE c.credited_as = 'A'\t\t AND p.born &gt;= 1970\t\t AND c.movieid = m.movieid) The SELECT NULL is used to emphasize that we\r\nare not interested in whom, or when precisely they where born. We just\r\nwant to check that whether there is such a person. In fact we could turn\r\nthe query into a uncorrelated one: SELECT DISTINCT m.titleFROM movies mWHERE m.movieid IN\t(SELECT DISTINCT c.movieid\t FROM credits c\t\t INNER JOIN people p\t\t\t ON p.peopleid = c.peopleid\t WHERE c.credited_as = 'A'\t\t AND p.born &gt;= 1970)\r\n","tags":["Principles of Database Systems","Notes"]},{"title":"Principles of Database Systems (H) Lecture 4 Notes","url":"/2025/09/29/dbh-04/","content":"Advanced Queries\r\nDISTINCT\r\nTo obtain a valid relation, we must ensure that there are no\r\nduplicate identifiers in the result set. For example, the naive query\r\nbelow SELECT country FROM movies WHERE year_released=2000 This query will return a result set that contains\r\nmany identical rows. In other words, we may obtaining a table, but it’s\r\nnot a relation because many rows cannot be distinguished.\r\n\n  \n    \n      Tip\r\n\n    \n    \n      If we need to do some operations with the query result, always try to\r\naccomplish it by using database.\r\n\n    \n  \r\nIf we are only interested in the different countries, we can use the\r\nkeyword DISTINCT SELECT DISTINCT country FROM movies WHERE year_released=2000 Now the result set is a\r\nvalid relation.\r\nIf there are multiple columns after the keyword\r\nDISTINCT, them it will eliminate those rows where all\r\nselected fields are identical. SELECT DISTINCT country, year_released FROM movies WHERE year_released IN (2000,2001) The selected combination\r\n(country, year_released) will be identical.\r\nAggregate Functions\r\nThe aggregate function will aggregate all rows that share a feature\r\nand return a characteristic of each group of aggregated rows. It will be\r\nclearer with an example. To compute an aggregate result, we’ll first\r\nretrieve data. And then regroup all the data according to the values in\r\none or several columns.\r\nFor example, if we want to group by country, and for each country the\r\naggregate function COUNT(*) says how many movies we have.\r\nSELECT country,\tCOUNT(*) number_of_movies\tFROM movies\tGROUP BY country We can also group on several columns. SELECT country, year_released\tCOUNT(*) number_of_movies\tFROM movies\tGROUP BY country, year_released Every\r\ncolumn that isn’t an aggregate function and appears after\r\nSELECT must also appear after GROUP BY.\r\n\n  \n    \n      Caution\r\n\n    \n    \n      Beware of some performance implication. When we apply a simple\r\nWHERE filter, the DBMS can start returning rows as soon as\r\na match is found. With a GROUP BY, the rows must be\r\nregrouped before the aggregate can take place. In other words, we need a\r\npreparatory phase that may take time, even if few rows are returned in\r\nthe end.\r\n\n    \n  \r\nCommon aggregate functions: -\r\nCOUNT(*)/COUNT(col) - MIN(col) -\r\nMAX(col) - AVG(col) -\r\nSTDDEV()\r\nFor example, to find the earliest release year by country:\r\nSELECT country,\tMIN(year_released) oldest_movie\tFROM movies\tGROUP BY country Since the result returned by MIN will be a\r\nrelation, we can apply another relational operation to the result:\r\nSELECT *\tFROM (\t\tSELECT country,\t\t\tMIN(year_released) oldest_movie\t\t\tFROM movies\t\t\tGROUP BY country\t) earliest_movies_per_country\tWHERE oldest_movie &lt; 1940 There is a short-hand that makes nesting queries\r\nunnecessary. We can have a condition on the result of an aggregate with\r\nHAVING: SELECT country,\tMIN(year_released) oldest_movie\tFROM movies\tGROUP BY country\tHAVING MIN(year_released) &lt; 1940 The two following queries are both\r\nvalid in SQL and behaviorally equivalent: SELECT country,\tMIN(year_released) oldest_movie\tFROM movies\tGROUP BY country,\tHAVING country='us'\tSELECT country,\tMIN(year_released) oldest_movie\tFROM movies\tWHERE country='us'\tGROUP BY country, But the\r\nefficient way is the second one. This is because it will first filter\r\nthe result then aggregate them.\r\n\n  \n    \n      Caution\r\n\n    \n    \n      The query optimizers in different DBMS are very likely different. We\r\nmust not rely the optimization that depends on a specifc DBMS. The most\r\nefficient query should always be chosen.\r\n\n    \n  \r\nThe aggregate ignores NULL. In the following query, the\r\nWHERE condition changes nothing to th result: SELECT MAX(died) most_recent_death\tFROM people\tWHERE died IS NOT NULL\r\nWe can also count distinct columns: SELECT country,\tCOUNT(DISTINCT year_released) number_of_years\tFROM movies\tGROUP BY country\r\nA complex example: find out how many people that are both actors\r\n(represented by the kind 'A') and directors (represented by\r\nthe kind 'D'). SELECT peopleid, COUNT(*) AS number_of_roles\tFROM(\t\tSELECT DISTINCT peopleid, credited_as\t\t\tFROM credits\t\t\tWHERE credited_as IN ('A', 'D')\t)\tGROUP BY peopleid\tHAVING count(*)=2\r\nRetrieving Data from\r\nMultiple Tables\r\nIn the building of complex queries, a important invariant that we\r\nneed to maintain is that the result set return by those queries are\r\nvalid relations, with no duplicates and a column that could be used as a\r\nkey. This must hold at every stage in a query.\r\nTo relate data from multiple tables, we need to use\r\nJOIN. For example, to retrieve country names by using the\r\ncountry column in the table movies, we can\r\nuse: SELECT title, country_name, year_released\tFROM movies JOIN countries\tON country_code=country The JOIN operation will create a\r\nvirtual table with all combinations between rows in the two tables. If\r\ntable 1 has  rows, and table 2\r\nhas  rows, then the result\r\nvirtual table will have  rows.\r\nIn the query above, we use an ON filter to filter out\r\nunrelated rows to make a much smaller virtual table.\r\nIf the two tables joined contain columns that has the same name, then\r\nit will be ambiguous to join on these columns. There is something called\r\nnatural join (unsupported by SQLServer) that basically means\r\n“if a column has the same name, then we should join on it”. But It can\r\nbe quite counterintuitive because it is based purely on names.\r\nWe can solve the issue by USING (not supported by\r\nSQLServer either): SELECT DISTINCT first_name, surname\tFROM people JOIN credits USING (peopleid)\tWHERE credited_as = 'D' A much better solution is to\r\nexplicitly write out the table names SELECT DISTINCT first_name, surname\tFROM people JOIN credits\tON credits.peopleid = people.peopleid\tWHERE credited_as = 'D' We can create short\r\nalias to shorten the query: SELECT DISTINCT first_name, surname\tFROM people AS p JOIN credits AS c\tON c.peopleid = p.peopleid\tWHERE credited_as = 'D' \n  \n    \n      Caution\r\n\n    \n    \n      Most DBMS accept people AS p instead of\r\npeople p, except Oracle.\r\n\n    \n  \r\nMore about JOIN\r\nA simple example of self join is if, for actor families, each row\r\nwere containing the identifiers of the father and mother if they are in\r\nthe database. You can display child and father. SELECT c.first_name || ' ' || c.surname as person,\t   f.first_name || ' ' || f.surname as father\tFROM people AS c JOIN people AS f\tON f.peopleid = c.fatherid A\r\nJOIN can as well be applied to a virtual table returned\r\nfrom a query, as long as the result is a valid relation.\r\nWe can also chain joins the same way we chain filtering conditions\r\nwith AND. JOINs between 10 or 15 tables aren’t\r\nuncommon, and queries generated by programs often do much worse.\r\nSELECT ... FROM table1 \tJOIN table2\t\tON ...\t...\tJOIN tablen\t\tON ...  Let’s write a relatively simple query, say we want to\r\nretrieve all British movie titles with director names. Finding all the\r\ntables involved is simple enough. But trouble begins as soon as we start\r\nwriting column names after SELECT: SELECT m.title, p.surname - The\r\ntitle comes from movies, and the\r\nsurname comes from people. But is it a key? -\r\nConsider when two brothers co-direct a film, then the title and surname\r\nare definitely not unique. How to handle the situation of a movie with\r\ntwo directors? A easy and naive solution is to use\r\nDISTINCT: SELECT DISTINCT m.title, p.surname But it will lose much information.\r\nA better solution is to return everything that is required to be certain\r\nabout uniqueness: SELECT m.year_released, m.title, p.first_name, p.surname\tFROM movies AS m\t\tINNER JOIN credits AS c\t\t\tON c.movieid = m.movieid\t\tINNER JOIN people AS p\t\t\tON p.peopleid = c.peopleid\t\tWHERE c.credited_as = 'D' AND m.country='gb' One important thing is that it better\r\nto start with the table for which we can select efficiently fewer rows\r\nbefore starting joining. Though in most situations this would not cause\r\nany performance differences, but we cannot rely on the specific behavior\r\nof the DBMS.\r\n","tags":["Principles of Database Systems","Notes"]},{"title":"Principles of Database Systems (H) Lecture 3 Notes","url":"/2025/09/26/dbh-03/","content":"Retrieving Data from a Table\r\nSELECT\r\nTo display the full content of a table, we can use SELECT * FROM table_name But\r\nthis should not be used in a program, this is because:\r\n\r\nIf our program depends on some specific columns, altering the\r\ndatabase will probably crashes our program.\r\nRetrieving the whole table could waste unnecessary hardware resource\r\nand bandwidth.\r\n\r\nAnd in most circumstances, we are only interested in a specific part\r\nof the database. So we need to filter the data by the\r\nWHERE clause. The conditions are usually expressed by a\r\ncolumn name followed by a comparison operator and the value to which the\r\ncontent of the column is compared. e.g. SELECT * FROM movies WHERE country = 'us' Only the rows\r\nwhere the expression is true will be returned.\r\nYou can compare to a number, a string constant, another column or the\r\nresult of a function.\r\nIf the original table doesn’t contain duplicates and forms a valid\r\nrelation. Then the filtering result will also not contain duplicates and\r\nform a valid relation. Hence we can treat is as a “virtual table”, e.g.\r\nSELECT * FROM \t(SELECT * FROM movies WHERE country = 'us') AS us_movies\tWHERE year_released BETWEEN 1940 AND 1949 Note that reversing the filters will result in a\r\nbehaviorally equivalent statement (but not necessarily equivalent in\r\nperformance): SELECT * FROM \t(SELECT * FROM movies WHERE year_released BETWEEN 1940 AND 1949) \t\tAS movies_from_40_to_49\tWHERE country = 'us' Or for a simple query like this, we can just\r\nuse a AND to combine the conditions: SELECT * FROM movies\tWHERE country = 'us' AND year_released BETWEEN 1940 AND 1949\r\n\n  \n    \n      Caution\r\n\n    \n    \n      AND has higher precedence than OR. Always\r\nprecedence where they can help improve readability, even if they are\r\nunnecessary.\r\n\n    \n  \r\nRelational operators:\r\n\r\nequal =\r\nnot equal &lt;&gt; or !=\r\nless than &lt;\r\nless or equal &lt;=\r\ngreater than &gt;\r\ngreater or equal &gt;=\r\n\r\n\n  \n    \n      Caution\r\n\n    \n    \n      Beware that most database products implicitly convert one of the\r\nsides in a comparison between values of differing types, and the result\r\ncan be quite unintuitive.\r\n\n    \n  \r\n\n  \n    \n      Tip\r\n\n    \n    \n      We can test a expression’s output by using SELECT expression AS VALUE;\r\n\n    \n  \r\nThe IN operator can simplify chained OR\r\nconditions: SELECT * FROM movies\tWHERE country IN ('us', 'gb') AND year_released BETWEEN 1940 AND 1949\r\nFor filtering strings, we can use a regex-like pattern. The\r\nLIKE operator will filter all strings that fit the\r\nspecified pattern. The pattern can have two wildcard:\r\n\r\n% means “any number of character, including none”,\r\nand\r\n_ means “one and only one character”.\r\n\r\nFor example, to select all movies that not have a ‘A’ in their name:\r\nSELECT * FROM movies\tWHERE title NOT LIKE '%A%' and title NOT LIKE '%a%' Notice that the behaviorally equivalent statement\r\nSELECT * FROM movies\tWHERE upper(title) NOT LIKE '%A%' is not good from a performance perspective.\r\n\n  \n    \n      Caution\r\n\n    \n    \n      It is not good to use function with columns in conditions.\r\n\n    \n  \r\nDates\r\nBeware that the dates have three common formats: -\r\nDD/MM/YYYY - MM/DD/YYYY -\r\nYYYY/MM/DD To avoid possible issues, always\r\nexplicity convert data types before using them.\r\n-- badSELECT * FROM forum_posts WHERE post_date&gt;= '2018-03-12';-- goodSELECT * FROM forum_posts WHERE post_date&gt;=date('2018-03-12');SELECT * FROM forum_posts WHERE post_date&gt;=date('12 March, 2018');\r\nAnother frequent mistake is the datetime value. If we\r\ncompare the datetime s to dates. The SQL\r\nengine will not understand that the date part of the\r\ndatetime value should be equal to the date\r\nvalue.\r\nAlso be aware that the comparisons involving dates and times can be\r\nsomewhat unintuitive: -- GoodWHERE issued &gt;= &lt;March 12&gt;WHERE issued &gt;= &lt;March 12 00:00:00&gt;-- Probably badWHERE issued &lt;= &lt;March 16&gt;WHERE issued &lt;= &lt;March 16 00:00:00&gt;-- The possible intentionWHERE issued &lt;= &lt;March 16 23:59:59&gt;\r\nDate arithmetic can also be problematic, e.g. adding 30 days and\r\nadding one month are different. And the syntax of different DBMS is also\r\ndifferent. For example, adding a month to the specified date:\r\n\r\nMicrosoft SQLServer: dateadd(month, 1, date_col)\r\nIBM DB2: date_col + 1 month\r\nPostgreSQL: date_col + interval'1 month'\r\nMySQL: date_add(date_col, interval 1 month)\r\nOracle: add_months(date_col, 1) or\r\ndate_col + decimal_number\r\nSQLite: date(date_col, '1 month')\r\n\r\nNULL\r\nUnlike in the most programming languages, in SQL, NULL\r\nis not a value. For example, the conditions WHERE column_name = NULLWHERE column_name != NULL This\r\ntwo conditions will never be true. This can be intuitively\r\nunderstand as we are comparing the columns to a value we don’t\r\nknow, so it will never be true. To test whether the column contains\r\na value or not, we need to use the IS operator:\r\nWHERE column_name IS NULLWHERE column_name IS NOT NULL\r\nSome Functions\r\nWe can of course query some specific columns instead all of them from\r\nthe database: SELECT title, year_released FROM movies WHERE country = 'us' And we need to know the structure of the\r\ndatabase to do this. We can see the description of the database by: -\r\ndesc movies in Oracle, and MySQL -\r\ndescribe table movies in IBM DB2 - \\d movies\r\nin PostgreSQL - .schema movies in SQLite\r\nOne important feature of DBMS is that we can retrieve transformed\r\ndata through some operators and functions. For example the string\r\nconcatenation: - 'hello' || 'world' in IBM DB2, Oracle,\r\nPostgreSQL, and SQLite - 'hello' + 'world' in SQLServer -\r\nconcat('hello', 'world') in MySQL SELECT title || ' was released in ' || cast(year_released AS varchar) \tAS movie_release\tFROM movies\tWHERE country = 'us'\r\n\n  \n    \n      Caution\r\n\n    \n    \n      For performance reasons that will be explained later, there is no\r\nissue applying a function (or transformation) to data that is returned\r\nor constants that were input. You should avoid, though, applying them to\r\ncolumns that are used for comparison.\r\n\n    \n  \r\nSome useful functions:\r\n\r\ntrunc(num, digits)\r\nround(num, digits)\r\nfloor(num)\r\nceiling(num)\r\nupper(string)\r\nlower(string)\r\nsubstr(string, start, count)\r\ntrim(string)\r\nreplace(string, needle, replace)\r\ncast(expr as type)\r\n\r\nCASE...END\r\nThe CASE construct is similar to the switch\r\nin Java or other pattern matching constructs. For example CASE upper(color)\tWHEN 'Y' THEN 'Color'\tWHEN 'N' THEN 'B&amp;W'\tELSE '?'END AS color\r\nIn a CASE statement, the value is tested against various\r\nvalues. If a match is found, the corresponding value is returned, If\r\nthere is no match and there is no ELSE clause, a\r\nNULL is returned.\r\nSame as the WHERE clauses, the NULL values\r\ncannot be tested directly in a WHEN clause. We need to use\r\nthe following instead: CASE\tWHEN died IS NULL THEN 'alive and kicking'\tELSE 'passed away'END AS status\r\n","tags":["Principles of Database Systems","Notes"]},{"title":"Discrete Mathematics (H) Lecture 1-3 Notes","url":"/2025/09/11/discrete-math-lec-1/","content":"Logic\r\n\n  \n    \n      Definition: Proposition\r\n\n    \n    \n      A proposition is a declarative statement\r\nthat is either true or false.\r\n\n    \n  \r\nUsually, propositions are condition-based. We can use\r\nlogical connectives to build more complex propositions:\r\n\r\nNegation \r\nConjunction \r\nDisjunction \r\nExclusive or \r\nImplication \r\nBiconditional \r\n\r\n\n  \n    \n      Definition: Truth Table\r\n\n    \n    \n      A truth table displays the relationships between truth\r\nvalues of different propositions. The rows of a truth table list out all\r\npossible values of all elementary propositions.\r\n\n    \n  \r\n\n  \n    \n      Definition: Negation\r\n\n    \n    \n      Let  be a proposition. The\r\nstatement “It is not the case that ” is called the negation of\r\n, denoted by  and read as “not ”.\r\n\n    \n  \r\n\n  \n    \n      Definition: Conjunction\r\n\n    \n    \n      Let  and  be propositions. The\r\nconjunction of  and , denoted by , is true when both  and  are true and is false otherwise.\r\n\n    \n  \r\n\n  \n    \n      Definition: Disjunction\r\n\n    \n    \n      Let  and  be propositions. The\r\ndisjunction of  and , denoted by , is false when both  and  are false and is true otherwise.\r\n\n    \n  \r\n\n  \n    \n      Definition: Exclusive Or\r\n\n    \n    \n      Let  and  be propositions. The exclusive\r\nor of  and , denoted by , is true when exactly one of\r\n and  is true and is false otherwise.\r\n\n    \n  \r\n\n  \n    \n      Definition: Implication\r\n\n    \n    \n      Let  and  be propositions. The conditional\r\nstatement (a.k.a. implication) , is the proposition “if , then ”, is false when  is true and  is false, and true otherwise. In ,  is called the hypothesis and\r\n is called the\r\nconclusion.\r\n\n    \n  \r\n is read in a variety of\r\nequivalent ways:\r\n\r\nif  then \r\n implies \r\n is sufficient for\r\n\r\n is necessary for\r\n\r\n follows from \r\n unless \r\n only if \r\n\r\nThe converse of \r\nis .\r\nThe contrapositive of  is .\r\nThe inverse of \r\nis .\r\n\n  \n    \n      Definition: Biconditional\r\n\n    \n    \n      Let  and  be propositions. The biconditional\r\nstatement (a.k.a. bi-conditional) , is the proposition\r\n“ if and only if ”, is true when  and  share the same truth value, and is\r\nfalse otherwise.\r\n\n    \n  \r\n\r\nA compound proposition that is always true for all possible\r\ntruth values is called a tautology.\r\nA compound proposition that is always false for all\r\npossible truth values is called a contradiction.\r\nA compound proposition that is neither tautology nor contradiction\r\nis called a contingency.\r\n\r\n\n  \n    \n      Definition: Logical Equivalence\r\n\n    \n    \n      Two proposition are equivalent if they always have\r\nthe same truth value.\r\nAlternative definition: The propositions  and  are called logically\r\nequivalent if  is a\r\ntautology, denoted by  or .\r\n\n    \n  \r\nLogical Equivalence Laws\r\n\n  \n    \n      Distributive Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      De Morgan’s Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Identity Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Domination Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Idempotent Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Double Negation Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Commutative Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Distributive Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Absorption Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Negation Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Useful Law\r\n\n    \n    \n      \r\n\n    \n  \r\nComputer\r\nRepresentation of True and False\r\nA bit is sufficient to represent two possible values: 0\r\n(false) or 1 (true). A variable that takes on values 0 and 1 is called a\r\nBoolean variable. A bit string is a sequence of zero\r\nor more bits, the length of such bit strings is the number of\r\nbits in the string.\r\nLimitations of Propositional\r\nLogic\r\nPropositional logic is the world described in terms of\r\nelementary propositions and their logical combinations. Elementary\r\nstatements typically refer to objects, their properties and\r\nrelations. But there exists some limitations of it, for example: -\r\nRepeated statements for many objects. - Statements that define the\r\nproperty of a group of objects. To solve this, we need to: - explicitly\r\nmodels objects and their properties, and - allows to make statements\r\nwith variables and quantify them.\r\nPredicate Logic\r\nBasic building blocks of the predicate logic (or\r\nfirst-order logic):\r\n\r\nConstant\r\nVariable\r\nPredicate\r\n\r\n\n  \n    \n      Definition: Predicate\r\n\n    \n    \n      A predicate is a statement  that contains  variables  and becomes a\r\nproposition when specific values are substituted for the\r\nvariables .\r\nThe universe (or domain)  of the predicate variables is\r\nthe set of all values that may be substituted in place of the\r\nvariables.\r\nThe truth set of  is the set of all\r\nvalues of the predicate variables s.t. the proposition  is\r\ntrue.\r\n\n    \n  \r\nThe logical connectives and corresponding rules in propositional\r\nlogic are all usable in predicate logic.\r\nThere are two types of quantified statements:\r\nuniversal and existential.\r\n\n  \n    \n      Definition: Universal Quantification\r\n\n    \n    \n      The universal quantification of  is the proposition “ is true for all values of\r\n in the universe of discourse.”\r\nDenoted by , and is\r\nexpressed as “for every , .”\r\n\n    \n  \r\n\n  \n    \n      Definition: Existential Quantification\r\n\n    \n    \n      The existential quantification of  is the proposition “ is true for some values of\r\n in the universe of discourse.”\r\nDenoted by , and is\r\nexpressed as “for some , .”\r\n\n    \n  \r\nSuppose that the elements in the universe can be numerated as , then: -  is true iff . -  is true iff .\r\n\n  \n    \n      Caution\r\n\n    \n    \n      The truth values of  and \r\ndepend on both the propositional function  and the universe.\r\n\n    \n  \r\n\n  \n    \n      Caution\r\n\n    \n    \n      The quantifier has higher precedence than the logical\r\nconnectives.\r\n\n    \n  \r\n\n  \n    \n      Tip\r\n\n    \n    \n      The universal quantifier \r\nis often paired with the implication :  The\r\nseemingly correct  would\r\nmean that everyone is a SUSTech student and is smart! The\r\nexistential quantifier  is\r\noften paired with the conjunction :  while\r\n would be\r\ntrue even if  is not a SUSTech\r\nstudent!\r\n\n    \n  \r\n\n  \n    \n      De Morgan’s Law for Quantifiers\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Tip\r\n\n    \n    \n      The order of the quantifiers generally does matter. if the types of\r\nthe nested quantifiers are the same, the order will have no matter at\r\nchanging.\r\n\n    \n  \r\nTheorems and Proofs\r\n\n  \n    \n      Definition: Axiom\r\n\n    \n    \n      An axiom and postulate is a statement or\r\nproposition which is regarded as being established, accepted, or\r\nself-evidently true.\r\n\n    \n  \r\n\n  \n    \n      Definition: Theorem\r\n\n    \n    \n      A theorem is a statement that can be proved to be true.\r\n\n    \n  \r\n\n  \n    \n      Definition: Lemma\r\n\n    \n    \n      A lemma is a statement that can be proved to be true, and is\r\nused in proving a theorem or proposition.\r\n\n    \n  \r\nTo show the truth value of a statement follows from other statements,\r\nwe need to provide a correct supporting argument, a.k.a.\r\nproof.\r\nTypically, a theorem looks like this:  \n  \n    \n      Definition: Proof\r\n\n    \n    \n      A proof provides an argument supporting the validity of the\r\nstatement, and may use premises, axioms, lemmas, results of other\r\ntheorems, etc.\r\n\n    \n  \r\nIn formal proofs, steps follow logically from the set of\r\npremises, axioms, lemmas, and other theorems.\r\nSome basic proof methods are:\r\n\r\nDirect proof\r\nProof by contrapositive\r\nProof by contradiction\r\nProof by cases\r\nProof of equivalence\r\n\r\nConsider the truth table of the implication:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nT\r\nT\r\nT\r\n\r\n\r\nT\r\nF\r\nF\r\n\r\n\r\nF\r\nT\r\nT\r\n\r\n\r\nF\r\nF\r\nT\r\n\r\n\r\n\r\nThen:\r\n\r\ndirect proof proceeds by showing the first row.\r\nproof by contradiction proceeds by showing the second row.\r\nproof by contrapositive proceeds by showing the fourth row.\r\n\r\nThe proof by cases is based on the following logical equivalence:\r\n ### Inference Rules of Propositional Logic Every inference\r\nrule represent a tautology that allows us to infer new true statements\r\nfrom existing ones.\r\n\n  \n    \n      Modus Ponens\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Modus Tollens\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Hypothetical Syllogism\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Disjunctive Syllogism\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Addition\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Simplification\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Conjunction\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Resolution\r\n\n    \n    \n      \r\n\n    \n  \r\nInference Rules for\r\nQuantified Statements\r\n\n  \n    \n      Universal Instantiation\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Universal Generalization\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Existential Instantiation\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Existential Generalization\r\n\n    \n    \n      \r\n\n    \n  \r\n","tags":["Notes","Discrete Mathematics"]},{"title":"Discrete Mathematics (H) Lecture 6 Notes","url":"/2025/10/15/discrete-math-lec-6/","content":"Algorithms and Complexity\r\nAsymptotic Notation\r\nBig-\r\nNotation\r\n\n  \n    \n      Definition: Big- Notation\r\n\n    \n    \n      Let  and  be functions from set of integers or\r\nthe set of real numbers to the set of real numbers. We say that , if there exist some\r\npositive constants  and\r\n s.t. , whenever .\r\n\n    \n  \r\n(for more precise definition and applications see notes for lecture 2 of Data Structures and\r\nAlgorithm Analysis (H))\r\n\n  \n    \n      Tip: Big- Estimates for\r\nPolynomials\r\n\n    \n    \n      Let ,\r\nwhere  are\r\nreal numbers. Then .\r\n\n    \n  \r\nProof. Assuming ,\r\nwe have  So we can conclude that for polynomials, the leading term\r\n dominates its growth.\r\nBig- estimates for some\r\nfunctions:\r\n\r\n\r\n\r\n\r\n for integer \r\n for integers \r\n for an integer \r\n\r\n\n  \n    \n      Tip: Big- for Summation of\r\nFunctions\r\n\n    \n    \n      If  and  then .\r\n\n    \n  \r\nProof. By definition, there exist constants  s.t.  when  and  when . Then  where \r\n\n  \n    \n      Tip: Big- for Product of\r\nFunctions\r\n\n    \n    \n      If  and  then .\r\n\n    \n  \r\nProof. When ,  where .\r\nBig- Notation\r\n\n  \n    \n      Definition: Big-\r\nNotation\r\n\n    \n    \n      Let  and  be functions from the set of integers\r\nor the set of real numbers to the set of real numbers. We say that , if there exists some\r\npositive constants  and  and s.t. , whenever .\r\n\n    \n  \r\n\n  \n    \n      Tip\r\n\n    \n    \n      Big- gives an upper\r\nbound on the growth of a function, while Big- gives a lower bound.\r\n\r\n\n    \n  \r\nBig- Notation\r\n\n  \n    \n      Definition: Big-\r\nNotation\r\n\n    \n    \n      Two functions  have the\r\nsame order growth if \r\nand . In this case, we\r\nsay that , which\r\nis the same as .\r\n\n    \n  \r\nAlgorithms\r\n\n  \n    \n      Definition: Algorithm\r\n\n    \n    \n      An algorithm is a finite sequence of precise\r\ninstructions for performing a computation or for solving a\r\nproblem.\r\n\n    \n  \r\n\n  \n    \n      Definition: Computational Problem\r\n\n    \n    \n      A computational problem is a specification of the desired\r\ninput-output relationship.\r\n\n    \n  \r\n\n  \n    \n      Definition: Problem Instance\r\n\n    \n    \n      An instance of a problem is all the inputs needed to compute\r\na solution to the problem.\r\n\n    \n  \r\nA correct algorithm halts with the correct output for\r\nevery input instance. We then say that the algorithm\r\nsolves the problem.\r\nTime and Space Complexity\r\n\n  \n    \n      Definition: Time Complexity and Space Complexity\r\n\n    \n    \n      The number of machine operations needed in an algorithm is\r\nthe time complexity of the algorithm, and the amount of\r\nmemory needed is the space complexity of the\r\nalgorithm.\r\n\n    \n  \r\nExample: Horner’s Algorithm\r\nConsider the evaluation of  Direct computation will take 3 additions and 6\r\nmultiplications. A more optimal way is to rearrange the expression as\r\n which takes 3 additions and 3 multiplications. This is the\r\ncore idea of the Horner’s Algorithm.  The\r\nnumber of operations needed in this algorithm is clearly  So the time complexity of this algorithm is .\r\nExample: Insertion Sort\r\n\r\nBest-case\r\nTo analyze for the best-case, we constrain the input so that is will\r\nresult in the fastest possible running time for the given input size.\r\nFor a sorting algorithm, the best-case is when the input array is\r\nalready sorted.\r\nIn this case the number of comparison needed is \r\nWorst-case\r\nTo analyze for the best-case, we constrain the input so that is will\r\nresult in the slowest possible running time for the given input size.\r\nFor a sorting algorithm, the best-case is when the input array is\r\nalready sorted in reverse order. \r\nAverage-case\r\nWe average running time over every possible type of input for the\r\ngiven size, this usually involve probabilities of different types of\r\ninput.\r\nThe complexity of insertion sort is  assuming that each of the\r\n instances are equally\r\nlikely.\r\nDealing with Hard Problems\r\nShowing that a problem has an efficient algorithm is relatively easy,\r\nsince all we need to do is to demonstrate an algorithm. But showing that\r\nthere is no efficient algorithm exists for a particular problem is\r\nrather difficult, since in this case we need to prove the non-existence\r\nof such algorithm among the countless possible algorithms. A handy way\r\nto approach this question is through the NP-Complete\r\nproblems.\r\nThe NP-Complete problems is a very large class of thousands of\r\npractical problems for which it is not known if the problems have\r\nefficient solutions. Or more precisely, have polynomial-time solutions.\r\nIt is known that if any one of the NP-Complete problems has an efficient\r\nsolution then all of the NP-Complete problems can be efficient\r\nsolutions. Since the innumberable man-years trying to find the efficient\r\nsolutions, NP-Complete problems are very likely to be hard.\r\nEncoding the Input of\r\nProblems\r\nComplexity of a problem is a measure w.r.t. the size of\r\ninput. In order to formally discuss how hard a problem is, we need to be\r\nmuch more formal than before about the input size of a problem.\r\n\n  \n    \n      Definition: Input Size\r\n\n    \n    \n      The input size of a problem is the minimum number\r\nof bits () needed to encode\r\nthe input of the problem.\r\n\n    \n  \r\nBut the exact input size ,\r\ndetermined by an optimal encoding method, is hard to compute in most\r\ncases. However, we do not need to determine  exactly in most cases.\r\nFor most problems, it is sufficient to choose some natural and\r\n(usually) simple, encoding and use the size  of this encoding.\r\nInput Size Example: Composite\r\n\n  \n    \n      Example: Composite Number\r\n\n    \n    \n      Given a positive integer , are\r\nthere integers  s.t. ? (i.e., is  a composite number?)\r\n\n    \n  \r\nSolution. Any integer  can be represented in the binary\r\nnumber system as a string  of length . Thus, a natural\r\nmeasure of input size is  (or just )\r\nInput Size Example: Sorting\r\n\n  \n    \n      Example: Sorting\r\n\n    \n    \n      Sort  integers .\r\n\n    \n  \r\nSolution. Using fixed length encoding, we write  as a binary string of length .\r\nThis coding gives an input size .\r\nComplexity in terms of Input\r\nSize\r\n\n  \n    \n      Example: Composite Number\r\n\n    \n    \n      The naive algorithm for determining whether  is composite compares  with the first  numbers to see if any of them divides\r\n.\r\n\n    \n  \r\nAt the first glance, this algorithm makes  comparisons, so it might seem\r\nlinear and very efficient. But, not that the input size of this problem\r\nis , so the\r\nnumber of comparisons performed is actually  which is exponential.\r\n\n  \n    \n      Definition: Type of Functions\r\n\n    \n    \n      Two positive functions  and\r\n are of the same type\r\nif  for all large , where  are some positive\r\nconstants.\r\n\n    \n  \r\nInput Size Example:\r\nInteger Multiplication\r\n\n  \n    \n      Example: Integer Multiplication\r\n\n    \n    \n      Given two integers  and , compute .\r\n\n    \n  \r\nThe minimum input size of this problem is  A natural choice is to use  since .\r\nComplexity\r\nDecision Problems\r\nand Optimization Problems\r\n\n  \n    \n      Example: Decision Problem\r\n\n    \n    \n      A decision problem is a question that has two possible\r\nanswers: yes and no.\r\n\n    \n  \r\nIf  is the problem, and  is the input, we will often write  to denote a yes answer\r\nand  to denote a\r\nno answer.\r\n\n  \n    \n      Example: Optimization Problem\r\n\n    \n    \n      An optimization problem requires an answer that is an\r\noptimal configuration.\r\n\n    \n  \r\nAn optimization problem usually has a corresponding\r\ndecision problem.\r\nExample: Knapsack\r\nvs. DKnapsack\r\nWe have a knapsack of capacity \r\n(a positive integer) and  objects\r\nwith weights  and values\r\n, where  and  are positive integers.\r\nThen\r\n\r\nOptimization problem (Knapsack):\r\n\r\nFind the largest value  of any subset  that\r\nfits in the knapsack, i.e, .\r\n\r\nDecision problem (DKnapsack):\r\n\r\nGiven , is there a subset of\r\nthe objects that fits in the knapsack and has total value at least ?\r\n\r\n\r\nRelationship Between the\r\ntwo Problems\r\nGiven a subroutine for solving the optimization problem, solving the\r\ncorresponding decision problem is usually trivial: First we solve the\r\noptimization problem, then check the decision problem. If it does,\r\nanswer yes, otherwise no.\r\nThus, if we prove that a given decision problem is hard to solve\r\nefficiently, then it is obvious that the optimization problem must be\r\n(at least as) hard.\r\nComplexity Classes\r\n\n  \n    \n      Definition: Polynomial-Time Algorithms\r\n\n    \n    \n      An algorithm is polynomial-time if its running time is , where  is a constant independent of , and  is the input size of the\r\nproblem that the algorithm solves.\r\n\n    \n  \r\n\n  \n    \n      Tip\r\n\n    \n    \n      Whether we use  or  as the input size, it will not affect\r\nthe conclusion of whether an algorithm is polynomial-time.\r\n\n    \n  \r\n\n  \n    \n      Definition: Nonpolynomial-Time Algorithms\r\n\n    \n    \n      An algorithm is nonpolynomial-time if the running time is\r\nnot  for any fixed .\r\n\n    \n  \r\nNonpolynomial-time algorithms are very often impractical.\r\n\n  \n    \n      Example: Polynomial-Time Solvable Problems\r\n\n    \n    \n      A problem is solvable in polynomial time (or simply in\r\npolynomial time) if there exists an algorithm which solves the\r\nproblem in polynomial time (a.k.a. tractable).\r\n\n    \n  \r\n\n  \n    \n      Definition: Complexity Class \r\n\n    \n    \n      The class  consists of\r\nall decision problems that are polynomial time. That is, there exists an\r\nalgorithm that will decide in polynomial time if any given input is a\r\nyes-input or a no-input.\r\n\n    \n  \r\n\n  \n    \n      Definition: Certificate\r\n\n    \n    \n      A certificate is a specific object corresponding to a\r\nyes-input, such that it can be used to show that the input is indeed a\r\nyes-input.\r\n\n    \n  \r\nGiven a presumed yes-input and its corresponding certificate, by\r\nmaking used of the given certificate, we verify that the input is\r\nactually a yes-input.\r\n\n  \n    \n      Definition: Complexity Class \r\n\n    \n    \n      The class  consists of\r\nall decision problems s.t., for each yes-input, there exists a\r\ncertificate allows us to verify in polynomial time that the input is\r\nindeed a yes-input.\r\n\n    \n  \r\nWe can intuitively observe that , but  still remains\r\nas a biggest problem in computer science.\r\nReduction\r\n\n  \n    \n      Definition: Reduction\r\n\n    \n    \n      We say that a problem  can be\r\nreduced to  if every\r\ninstance of  can be\r\n“rephrased” to an instance of .\r\n\n    \n  \r\n\n  \n    \n      Example\r\n\n    \n    \n      Let\r\n\r\n: multiplying two positive\r\nnumbers\r\n: adding two numbers\r\n\r\nThen  can be reduced to  via a logarithmic transformation\r\n\r\n\n    \n  \r\nWe say that  is “no harder to\r\nsolve” than  if  can be reduced to .\r\n\n  \n    \n      Definition: Polynomial-Time Reductions\r\n\n    \n    \n      Let  and  be two decision problems. A\r\npolynomial-time reduction from  to  is a transformation  with the following two properties:\r\n\r\n transform an input  for  into an input  for  s.t. a yes-input of  maps to a yes-input of , and a no-input  maps to a no-input of .\r\n is computable in\r\npolynomial time in .\r\n\r\nIf such an  exists, we say that\r\n is polynomial-time\r\nreducible to , and write\r\n.\r\n\n    \n  \r\nGiven two decision problems  and an algorithm \r\nfor the decision problem , we\r\ncan develop an algorithm  to\r\nsolve  trivially: \r\n\n  \n    \n      Theorem\r\n\n    \n    \n      If  and , then .\r\n\n    \n  \r\nProof.  means\r\nwe have a polynomial-time algorithm  for . Since , we have a polynomial-time\r\ntransformation  mapping input\r\n for  to an input for .\r\nCombining these, we get the following polynomial-time algorithm for\r\nsolving :\r\n\r\ntake input  for  and compute \r\nrun algorithm  on input\r\n, and return the answer.\r\n\r\nBoth steps take polynomial time. So the combined algorithm takes\r\npolynomial time. Hence .\r\n\n  \n    \n      Lemma: Transitivity of Reduction\r\n\n    \n    \n      If  and , then .\r\n\n    \n  \r\nNP-Completeness and Its\r\nProperties\r\n\n  \n    \n      Definition: Complexity Class \r\n\n    \n    \n      The class  of\r\nNP-Complete problems consists of all decision problems  s.t.\r\n\r\n\r\nfor every ,\r\n\r\n\r\n\n    \n  \r\nIntuitively,  consists\r\nof all the hardest problems in .\r\n\n  \n    \n      Theorem\r\n\n    \n    \n      Let  be an arbitrary problem in\r\n.\r\n\r\nIf there is a polynomial-time algorithm for , then there is a polynomial-time\r\nalgorithm for every .\r\nIf there is no polynomial-time algorithm for , then there is no polynomial-time\r\nalgorithm for every .\r\n\r\n\n    \n  \r\nEither all NP-Complete problems are polynomial time\r\nsolvable, or all NP-Complete problems are not polynomial time\r\nsolvable.\r\nProof. For 1., if  and , then from the 2. of the\r\ndefinition of NP-Completeness, all problems in  will also belong to .\r\nFor 2., assume that for some problem  that has a\r\npolynomial-time algorithm while all other  don’t have. Since , we have\r\n for all other , i.e.,  is also solvable in polynomial time.\r\nBut this contradicts with our assumption.\r\n","tags":["Notes","Discrete Mathematics"]},{"title":"Discrete Mathematics (H) Lecture 4 Notes","url":"/2025/09/23/discrete-math-lec-4/","content":"Basic Discrete Mathematics\r\nObjects\r\nSets\r\nIntroduction to Sets\r\n\n  \n    \n      Definition: Sets\r\n\n    \n    \n      A set is an unordered collection of objects. These objects\r\nare called elements or members.\r\n\n    \n  \r\nWe can represent a set by\r\n\r\nlisting the elements, or\r\ndefine it by property, e.g.\r\n\r\n Important sets:\r\n\r\nNatural numbers: \r\nIntegers: \r\nPositive integers: \r\nRationals: \r\nReal numbers: \r\nComplex numbers: \r\n\r\nInterval notation: -  -  -  - \r\nTwo sets  are equal\r\nif and only if \r\n\n  \n    \n      Russell’s Paradox\r\n\n    \n    \n      Let , is a set\r\nof sets that are not members of themselves. Is  or ?\r\n\r\nIf , then  does not satisfy the condition, so\r\n.\r\nIf , then  is included in the set , so .\r\n\r\nThe most common solution of this paradox in modern mathematics is the\r\naxiomatic set theory.\r\n\n    \n  \r\nTwo special sets:\r\n\r\nThe universal set is the set of all objects under\r\nconsideration, denoted as .\r\nThe empty set is the set that contains no elements, denoted\r\nas  or .\r\n\r\nVenn Diagrams and Subsets\r\n\n  \n    \n      Definition: Subsets\r\n\n    \n    \n      A set  is called a\r\nsubset of  iff every\r\nelement of  is also an element of\r\n, denoted as . \r\n\n    \n  \r\n\n  \n    \n      Definition: Proper Subsets\r\n\n    \n    \n      If , but , then we say  is a proper subset of , denoted by . \r\n\n    \n  \r\n\n  \n    \n      Theorem: Empty Set\r\n\n    \n    \n      .\r\n\n    \n  \r\nProof: we substitute  into the definition of subset\r\nrelation  Since the premise  is always false, the\r\nstatements holds.\r\n\n  \n    \n      Theorem: Reflexivity of Subset Relation\r\n\n    \n    \n      .\r\n\n    \n  \r\nProof: we substitute  into the definition of subset\r\nrelation  This trivially holds.\r\nThe set relations can be visualized by using Venn diagrams,\r\ne.g. : \r\nCardinality\r\n\n  \n    \n      Definition: Cardinality\r\n\n    \n    \n      Let  be a set. If there are\r\nexactly  distinct elements in\r\n where  is a nonnegative integer, we say that\r\n is a finite set and  is the cardinality of , denoted by .\r\n\n    \n  \r\nA set if infinite if it is not finite.\r\nPower Set\r\n\n  \n    \n      Definition: Power Set\r\n\n    \n    \n      Given a set , the power\r\nset of  is the set of all\r\nsubsets of the set , denoted by\r\n.\r\n\n    \n  \r\n\n  \n    \n      Example\r\n\n    \n    \n      \r\n\r\n\r\n\r\nWe can observe that .\r\n\n    \n  \r\nTuples\r\n\n  \n    \n      Definition: Tuple\r\n\n    \n    \n      The ordered n-tuple  is the ordered\r\ncollection that has  its -th element. A -tuple is also called as an ordered\r\npair.\r\n\n    \n  \r\n\n  \n    \n      Definition: Cartesian Product\r\n\n    \n    \n      Let  and  be sets. The Cartesian product\r\nof  and , denoted as , is the set of all ordered\r\npairs , where  and . Hence \r\nWe can easily generalize the Cartesian product to  sets as follows: \r\n\n    \n  \r\nIt easy to observe that\r\n\r\n\r\n\r\n\r\n\n  \n    \n      Definition: Relations\r\n\n    \n    \n      A subset of the Cartesian product  is called a relation from the set  to the set .\r\n\n    \n  \r\nSet Operations\r\n\n  \n    \n      Definition: Union\r\n\n    \n    \n      Let  and  be sets. The union of the sets\r\n amd , denoted by , is the set .\r\n\n    \n  \r\n\n  \n    \n      Definition: Intersection\r\n\n    \n    \n      The intersection of the set  and , denoted by , is the set .\r\n\n    \n  \r\n\n  \n    \n      Definition: Complement\r\n\n    \n    \n      If  is a set, then the\r\ncomplement of the set \r\nw.r.t. the universal set , denoted\r\n is the set , .\r\n\n    \n  \r\n\n  \n    \n      Definition: Difference\r\n\n    \n    \n      Let  and  be sets. The difference of\r\n and , denoted by , is the set containing the elements\r\nof  that are not in , i.e. .\r\n\n    \n  \r\n\n  \n    \n      Definition: Disjoint Sets\r\n\n    \n    \n      Two sets  and  are called disjoint if their\r\nintersection is empty, i.e. .\r\n\n    \n  \r\n\n  \n    \n      Theorem: The Principle of Inclusion and Exclusion\r\n\n    \n    \n      \r\n\n    \n  \r\nThis is very similar to the principle of inclusion and exclusion in\r\nprobability.\r\nSet Identities\r\n\n  \n    \n      Identity Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Domination Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Idempotent Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Identity Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Complementation Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Commutative Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Associative Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Distributive Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Associative Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Absorbtion Laws\r\n\n    \n    \n      \r\n\n    \n  \r\n\n  \n    \n      Complement Laws\r\n\n    \n    \n      \r\n\n    \n  \r\nThe set identities are extremely similar to that ones of logic. The\r\nproofs for them are also very close to the ones of logic.\r\nFor example, to prove . Proof by membership tablesProof by equivalenceProof by logical equivalencesProof by membership tables:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n1\r\n1\r\n0\r\n0\r\n0\r\n0\r\n\r\n\r\n1\r\n0\r\n0\r\n1\r\n1\r\n1\r\n\r\n\r\n0\r\n1\r\n1\r\n0\r\n1\r\n1\r\n\r\n\r\n0\r\n0\r\n1\r\n1\r\n1\r\n1\r\n\r\n\r\nProof by equivalence: showing that Proof by logical equivalences:  ### Generalized\r\nUnions and Intersections The union of a collection of sets is\r\nthe set that contains those elements that are members of at least one\r\nset in the collection .\r\nThe intersection of a collection of sets is the set that\r\ncontains those elements that are members of all sets in the collection\r\n.\r\nComputer Representation of\r\nSets\r\nA naive way to represent sets in computers is to explicitly store all\r\nthe elements in a list. But this will make common set\r\noperations like lookup/insertion/deletion inefficient. A better solution\r\nis to assign a bit in a bit string to each element in the universal set\r\nand set the bit to 1 if the element is in the set, otherwise 0. By using\r\nthis representation, each set of cardinality  can be represented as a bit string\r\n. And the common set\r\noperations can be implemented easily and efficiently. The only problem\r\nis that this representation can waste memory.\r\nFunctions\r\n\n  \n    \n      Definition: Functions\r\n\n    \n    \n      Let  and  be two sets. A function from\r\n to , denoted by , is an assignment of exactly\r\none element of  to each\r\nelement of . We write  if  is the unique element of  assigned by the function  to the element  of .\r\n\n    \n  \r\nTo represent a function, we can either\r\n\r\nexplicitly state the assignments between elements of the two sets,\r\nor\r\nby a formula.\r\n\r\n\n  \n    \n      Definition: Important Sets of Functions\r\n\n    \n    \n      Let  be a function from  to . We say that  is the domain of  and  is the codomain of . If ,  is called the image of  and  is a preimage of . The range of  is the set of all images of elements of\r\n, denoted by . We also say  maps  to .\r\n\n    \n  \r\n\n  \n    \n      Example\r\n\n    \n    \n      For a function:  \r\n\r\n is the image of \r\n is a preimage of \r\nthe domain of  is \r\nthe codomain of  is \r\nthe range of  is \r\n\r\n\n    \n  \r\n\n  \n    \n      Definition: Image of a Subset\r\n\n    \n    \n      For a function  and\r\n, the image of\r\n is a subset of  that consists of the images of the\r\nelements of , denoted by , .\r\n\n    \n  \r\n\n  \n    \n      Definition: Injective Function\r\n\n    \n    \n      A function  is called\r\none-to-one or injective, iff  implies  for all  in the domain of . In the case,  is called an injection.\r\nAlternatively, a function is one-to-one iff  whenever .\r\n\n    \n  \r\n\n  \n    \n      Definition: Surjective Function\r\n\n    \n    \n      A function  is called\r\nonto or surjective, iff for every  there is an element  s.t. . In this case,  is called a surjection.\r\nAlternatively, a function is onto iff all codomain elements are\r\ncovered, i.e. .\r\n\n    \n  \r\n\n  \n    \n      Definition: Bijective Function\r\n\n    \n    \n      A function  is called\r\nbijective, iff it is both injective and surjective.\r\n\n    \n  \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nProperties to prove\r\nMethod to prove\r\n\r\n\r\n\r\n\r\nTo show  is injective\r\nShow that \r\n\r\n\r\nTo show  is not injective\r\nShow that \r\n\r\n\r\nTo show  is surjective\r\nShow that \r\n\r\n\r\nTo show  is not\r\nsurjective\r\nShow that \r\n\r\n\r\n\r\n\n  \n    \n      Tip\r\n\n    \n    \n      For a function  with\r\n,  is injective iff  is surjective.\r\n\n    \n  \r\nProof.\r\n\r\nOnly if: suppose that  is injective. Let  be elements of\r\n. Then  for . Therefore, . But\r\n and . Therefore, .\r\nIf: suppose that  is\r\nsurjective. Let  be a listing of\r\nthe elements of . Suppose that\r\n for some . Then, . But\r\n, a contradiction.\r\n\r\nBut note that this conclusion cannot be straightforwardly generalize\r\nto functions with infinite domain and codomain.\r\nLet  and  be functions from  to . Then  and  are also functions from  to  defined for all \r\nby \r\n\n  \n    \n      Definition: Inverse Function\r\n\n    \n    \n      Let  be a bijection. The\r\ninverse of  is the\r\nfunction that assigns to an element  belonging to  the unique element  in  s.t. , denoted by . Hence,  when . In this case,  is called an invertible\r\nfunction.\r\n\n    \n  \r\n\n  \n    \n      Caution\r\n\n    \n    \n      If  is not a bijection, it is\r\nimpossible to define the inverse function of .\r\n\n    \n  \r\n\n  \n    \n      Definition: Composition of Functions\r\n\n    \n    \n      Let  be a function from  to  and let  be a function from  to . The composition of the\r\nfunctions  and , denoted by , is defined by .\r\n\n    \n  \r\nSuppose that  is a bijection\r\nfrom  to . Then  and , where  denote the identity\r\nfunctions on the sets  and\r\n, respectively.\r\n","tags":["Notes","Discrete Mathematics"]},{"title":"Discrete Mathematics (H) Lecture 5 Notes","url":"/2025/09/29/discrete-math-lec-5/","content":"Basic Discrete\r\nMathematics Objects (Continued)\r\nSequence\r\nDefinitions and Notations\r\n\n  \n    \n      Definition: Sequence\r\n\n    \n    \n      A sequence is a function from a subset of the set of\r\nintegers to a set . We use the\r\nnotation  to denote the image of\r\nthe integer , and  represents the ordered list .\r\n\n    \n  \r\n\n  \n    \n      Definition: Arithmetic Progression\r\n\n    \n    \n      An arithmetic progression is a sequence of the form , where\r\nthe initial term  and\r\ncommon difference  are\r\nreal numbers.\r\n\n    \n  \r\n\n  \n    \n      Definition: Geometric Progression\r\n\n    \n    \n      A geometric progression is a sequence of the form , where the\r\ninitial term  and the\r\ncommon ratio  are real\r\nnumbers.\r\n\n    \n  \r\n\n  \n    \n      Definition: Recursively Defined Sequences\r\n\n    \n    \n      The -th element of the sequence\r\n is defined\r\nrecursively in terms of the previous elements of the sequence and\r\nthe initial elements of the sequence.\r\n\n    \n  \r\n\n  \n    \n      Example\r\n\n    \n    \n      \r\nArithmetic progression: \r\nGeometric progression: \r\nRecursively defined sequence: \r\n\r\n\n    \n  \r\nSummations\r\n\n  \n    \n      Definition: Summation of Sequences\r\n\n    \n    \n      The summation of the terms of a sequence is \r\nThe variable  is referred to as\r\nthe index of summation and the choice of the letter  is arbitrary.  is the lower limit of the\r\nsummation, and  is the upper\r\nlimit of the summation.\r\n\n    \n  \r\nTwo useful properties of summations:\r\n\r\n\r\n\r\n\r\nThe sum of the first  terms of\r\nthe arithmetic progression  is  The sum of the first \r\nterms of the geometric progression  is \r\n\n  \n    \n      Example\r\n\n    \n    \n      \r\n\r\n\r\n\r\n\r\n\r\n\r\n\n    \n  \r\nSolution 1Solution 2Solution 3Solution 4Solution 5Solution 1. Using the summation formula for the arithmetic\r\nprogression: Solution 2. Again, using the summation formula for the\r\narithmetic progression: Solution 3. The inner sum for fixed  is  Then Solution 4. Using the summation formula for the geometric\r\nprogression: Solution 5. The inner sum for fixed  is  Then the outer sum is \r\nInfinite Series\r\nInfinite geometric series  can be computed in the\r\nclosed form for : \r\nSome Useful Summation\r\nFormulas\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSum\r\nClosed Form\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nProof 1Proof 2Proof 3Proof 4Proof 5Proof 6Proof for formula 1. Let , then , then Proof for formula 2. A useful trick is the write the sum\r\ntwice, once in increasing order and once in decreasing order  Adding both expressions: Proof for formula 3. We use the telescoping method. Consider\r\nthe identity  And we write this for   Adding all these equations up, we get  Substitute :  Simplify the equation we get Proof for formula 4. We can notice that \r\nThen we only need to calculate :\r\nProof for formula 5. Proof for formula 6. Notice that  Then the closed form is \r\nCardinality of Sets, Redux\r\n\n  \n    \n      Definition: Cardinality (Continued)\r\n\n    \n    \n      The sets  and  have the same cardinality if\r\nthere is a one-to-one correspondence between elements in  and .\r\nIf there is a one-to-one function from  to , the cardinality of  is less than or the same as the\r\ncardinality of , denoted by . Moreover, when  and  and  have different cardinalities, we say\r\nthat the cardinality of  is less\r\nthan the cardinality of , denoted\r\nby .\r\n\n    \n  \r\n\n  \n    \n      Cantor-Bernstein Theorem\r\n\n    \n    \n      Assume  are two (possibly\r\ninfinite) sets, then \r\n\n    \n  \r\nProof. If  and  are both finite, then the proof is\r\ntrivial. If  and  are both infinite. ### Countable Sets\r\n\n  \n    \n      Definition: Countable Sets\r\n\n    \n    \n      A set that is either finite or has the same\r\ncardinality as the set of positive integers  is called countable.\r\nA set that is not countable is called uncountable.\r\n\n    \n  \r\n\n  \n    \n      Example\r\n\n    \n    \n      Prove that the set of even numbers  is countable.\r\n\n    \n  \r\nProof. We define a function , then we need to prove\r\n is a bijection: - Injective: If\r\n, then . - Surjective:  is the\r\npreimage in .\r\n\n  \n    \n      Theorem\r\n\n    \n    \n      The set of integers  is\r\ncountable.\r\n\n    \n  \r\nProof. We can list a sequence:  or define a bijection from  to : \r\n\n  \n    \n      Theorem\r\n\n    \n    \n      The set of (positive) rational numbers  is countable.\r\n\n    \n  \r\nProof. We construct a list by first list all  with , then list all  with , etc. \r\n\n  \n    \n      Lemma\r\n\n    \n    \n      If  is a countable set, then\r\n is also a\r\ncountable set.\r\n\n    \n  \r\n\n  \n    \n      Theorem\r\n\n    \n    \n      The set of finite string  over\r\na finite alphabet  is countably\r\ninfinite. (Assume an alphabetical ordering of symbols in )\r\n\n    \n  \r\nProof. We show that the strings can be listed in a sequence\r\nby follows:\r\n\r\nFirst list all the strings of length  in alphabetical order,\r\nthen all the strings of length  in lexicographic order,\r\netc.\r\n\r\nThis implies a bijection from  to .\r\nUncountable Sets\r\n\n  \n    \n      Theorem\r\n\n    \n    \n      The set of real numbers  is uncountable.\r\n\n    \n  \r\nProof. Assume that  is countable, then every subset of  is countable, in particular,\r\nthe interval  is countable.\r\nThis implies that the elements of this set can be listed as , where  all . We want to\r\nshow that not all read numbers in the interval  are in the list. We do this by form\r\na new number called , where  if , and  if . We claim that  is different from each number in the\r\nlist. Each expansion is unique, if we exclude an infinite\r\nstring of ’s.  and  differ in the -th decimal place for all . This proof method is the famous\r\nCantor diagonalization argument.\r\n\n  \n    \n      Theorem\r\n\n    \n    \n      The set  is\r\nuncountable.\r\n\n    \n  \r\nProof. Assume that  is countable. This implies that the elements of\r\nthis set can be listed as , where , and each  can be uniquely by the bit string\r\n, where\r\n if  and  if . Then we can construct the\r\nlist  with all . We form a new set\r\ncalled , where  if , and  if . We claim that  is different from each set in the list.\r\nEach bit string is unique, and \r\nand  differ in -th bit for all .\r\nComputable vs. Uncomputable\r\n\n  \n    \n      Definition: Computability\r\n\n    \n    \n      We say that a function is computable (or\r\nTuring-computable) if we can use a Turing machine to find the\r\nvalues of this function. If a function is not computable, we\r\nsay it is uncomputable. Intuitively, a function is computable\r\nif there is a computer program that finds the values of this\r\nfunction.\r\n\n    \n  \r\n\n  \n    \n      Theorem\r\n\n    \n    \n      There are functions that are not computable.\r\n\n    \n  \r\nProof. Since we know that the set of finite strings on\r\nfinite alphabet is countable, it is trivial to prove that the set of\r\ncomputer programs is countably finite. To prove this theorem,\r\nwe only need to prove the set of functions is uncountable. We\r\nassume that the set of functions is countable, then the set of functions\r\nfrom  to the set  is countable. This implies we can\r\nlist all the functions in the set as a sequence  We define a new function  We claim that  is\r\ndifferent from all functions in the sequence. We prove this by\r\ncontradiction. Assume that \r\nfor some , then  This is a contradiction, since . So the set of functions\r\nis uncountable.\r\nCantor’s Theorem\r\n\n  \n    \n      Theorem\r\n\n    \n    \n      If  is a set, then .\r\n\n    \n  \r\n","tags":["Notes","Discrete Mathematics"]},{"title":"Data Structures and Algorithms (H) Lecture 1 Notes","url":"/2025/09/11/dsaa-lec-1/","content":"Getting Started\r\n\n  \n    \n      Definition: Algorithm\r\n\n    \n    \n      An algorithm is a well-defined computational procedure that\r\ntakes some input and produces some output.\r\n\n    \n  \r\nThis means:\r\n\r\nIt is a tool for solving a well-specified computational\r\nproblem. e.g. the sorting problem:\r\nInput: a sequence of \r\nnumbers .\r\nOutput: a permutation  of the input sequence\r\ns.t. \r\n\r\nA sequence like  is called a instance of the sorting\r\nproblem. We expect the algorithm to solve all instances of a\r\nspecific problem.\r\nTo define the algorithms precisely, we use a abstract language,\r\npseudocode.\r\nAn ideal algorithm should be\r\n\r\ncorrect\r\nefficient\r\neasy to read, cheap to design\r\netc.\r\n\r\n\n  \n    \n      Definition: Correctness of Algorithm\r\n\n    \n    \n      An algorithm is correct if for every input instance it halts\r\nwith the corrent output. A correct algorithm solves the\r\nproblem.\r\n\n    \n  \r\nTo show the correctness of algorithms, we need to make mathematical\r\nreasoning.\r\nNext, to measure the running time of a given algorithm, we need a\r\nmodel that provides a good level of abstraction:\r\n\r\nGives a good idea about the time an algorithm needs.\r\nAllows us to compare different algorithms.\r\nWithout getting bogged down with details.\r\n\r\n\n  \n    \n      Definition: Random-access Machine Model\r\n\n    \n    \n      A generic random-access machine, where instructions are executed one\r\nafter another, with no concurrent operations. The elementary operations\r\nare:\r\n\r\nArithmetic operations\r\nLogical operations, shifts, comparisons\r\nData movement: variable assignments\r\nControl instructions: loops, subroutine/method calls\r\n\r\nThe RAM model assumes that each elementary operations takes the same\r\namount of time (a constant time).\r\n\n    \n  \r\nThen to measure the runtime, we only need to count the number of\r\nelementary operations in the RAM model, this is known as the common\r\ncost model.\r\nBut we often abstract from the constants and concrete operation\r\nnumbers, and focus on the asymptotic growth of runtime with\r\nproblem size.\r\nExample: Insertion Sort\r\n\r\n\r\ninsertion-sort\r\n\r\nCorrectness\r\nTo prove the correctness of the above algorithm, we use proof by\r\nloop invariants, a popular way of proving correctness of algorithms\r\nwith loops.\r\n\n  \n    \n      Definition: Loop Invariant\r\n\n    \n    \n      A loop invariant is a statement that is always true and that\r\nreflects the progress of the algorithm towards producing a correct\r\noutput.\r\n\n    \n  \r\n\r\nInitialization: the loop invariant is true at\r\ninitialization.\r\n\r\nThis part is often trivial.\r\n\r\nMaintenance: if the loop invariant is true after  iteration, it is also true after  iterations.\r\n\r\nEssentially the same as the proof by induction.\r\n\r\nTermination: when the algorithm terminates, the loop\r\ninvariant tells that the algorithm is correct.\r\n\r\nApplying this to the insertion sort, we get:\r\n\r\nLoop invariant: At the start of each iteration of the for\r\nloop of lines 1-9, the subarray  consists of the elements originally in , but in sorted order.\r\nInitialization: For  the subarray  is the original  and it is sorted (trivial).\r\nMaintenance: The while loop moves  one position to the\r\nright and inserts  at the\r\ncorrect position . Then  contains the original , but in sorted order.\r\nTermination: The for loop ends when . Then the loop invariant  says that the array contains the\r\noriginal  in sorted\r\norder.\r\n\r\nRuntime of Insertion\r\nSort (In a naive way)\r\nA naive way of calculating runtime of insertion sort is:\r\n\r\nAssume that line  take  time to run once (cost).\r\nCount the number the line  has\r\nexecuted.\r\nSum up the product of cost and execution time.\r\n\r\nDefine  as the number of\r\ntimes the while loop is executed for that :\r\n\r\n\r\n\r\nLine\r\nCost\r\nTimes\r\n\r\n\r\n\r\n\r\n1\r\n\r\n\r\n\r\n\r\n2\r\n\r\n\r\n\r\n\r\n3\r\n\r\n\r\n\r\n\r\n4\r\n\r\n\r\n\r\n\r\n5\r\n\r\n\r\n\r\n\r\n6\r\n\r\n\r\n\r\n\r\n7\r\n\r\n\r\n\r\n\r\n8\r\n\r\n\r\n\r\n\r\n9\r\n\r\n\r\n\r\n\r\n\r\nThen the runtime of the algorithm is:\r\n - Best case: the array is already sorted, in that\r\ncase, :\r\n So it’s linear w.r.t. the input size. - Worse\r\ncase: the array is inversely ordered, in that case, :\r\n So it’s quadratic w.r.t. the input size\r\n","tags":["Notes","Data Structures and Algorithms"]},{"title":"Data Structures and Algorithms (H) Lecture 3 Notes","url":"/2025/09/27/dsaa-lec-3/","content":"Divide-and-Conquer\r\nIntroduction\r\nProblem: Find a number\r\nin a sorted array\r\nConsider the problem that we need to find whether a specific number\r\nis in a sorted array. A naive approach is searching the array\r\nfrom the start to the end. The worst time would then be  since its a linear search.\r\nWhat if we always check the midpoint of the array and discarding the\r\nwrong half? Then the time would reduce to . We call this kind of sort\r\nthe binary search. By dividing the problem by half at each\r\nstep, we have reduced the time from linear to logarithmic.\r\nDesign Paradigms\r\nThe insertion sort we discussed before uses an incremental\r\napproach:\r\n\r\nhaving sorted the subarray , we inserted  into its proper place, yielding the\r\nsorted subarray .\r\nthe idea is to incrementally build up a solution to the\r\nproblem.\r\n\r\nAn alternative design approach is the\r\ndivide-and-conquer:\r\n\r\ndivide: break the problem into smaller subproblems, smaller\r\ninstances of the original problem.\r\nconquer: solve these problems recursively.\r\ncombine the solutions to subproblems into the solution for\r\nthe original problems.\r\n\r\nMerge Sort\r\nThe merge sort is a famous sorting algorithm that uses\r\ndivide-and-conquer. The algorithm will\r\n\r\ndivide the -element\r\nsequence to be sorted into two subsequences of  elements each, and then\r\nconquer them by sort the two subsequences\r\nrecursively using merge sort, and finally\r\ncombine them by merge the two subsequences into\r\none to produce the final answer.\r\n\r\nThe recursion stops when the subarray has only 1 element. The key of\r\nthis algorithm is the merging part. One of the tedious bit is copying\r\nelements between arrays.\r\n\r\n\r\nFirst we assume subarrays  and  are sorted.\r\nThen we copy these subarrays to new arrays  and .\r\nBoth  and  contain an additional element  at the end, call the\r\nsentinel, so we don’t have to check for end of array.\r\nMerge  and  back into  by comparing  and . The pseudocode of merge is\r\npresented below: \r\n\r\nThe runtime of the merging is quite easy to determine. Since there\r\nare no nested loops and all loops are bounded directly by the index. The\r\nruntime will then be  Next we check the correctness of the merge algorithm by loop\r\ninvariant:\r\n\r\nLoop invariant: at the start of the iteration of the last\r\nfor loop,\r\n\r\nthe subarray  contains\r\nthe  smallest elements of  and , in sorted order and\r\n and  are the smallest elements of their\r\narrays that have not been copied back to .\r\n\r\nInitialization: the loop starts with , hence  is empty and contains the  smallest elements of . As ,  and  are the smallest uncopied\r\nelements.\r\nMaintenance: suppose . Then  is the\r\nsmallest element not copied back.  contains the  smallest elements, and after copying\r\n into ,  contains the  smallest elements. Incrementing\r\n and  reestablishes the loop condition. The\r\nargument for  is\r\nsimilar.\r\nTermination: at termination, . By the loop invariant,  contains the  smallest elements of\r\n and , in sorted order. That’s all\r\nelements in  and  apart from the two .\r\n\r\nThe Complete Algorithm\r\n\r\nFirst we prove the correctness of this algorithm. Since the algorithm\r\nis recursive, we use the proof by induction to show its\r\ncorrectness.\r\nProof. We assume that MergeSort sorts correctly arrays of\r\nsize  and show that it sorts\r\ncorrectly an array of size . -\r\nBase case: . The\r\nalgorithm will return at line 1 with the sorted array of a single\r\nelement. - Inductive step: by the induction hypothesis lines 3\r\nand 4 return two subarrays sorted correctly. We have already proved the\r\nMerge is correct hence after its execution the algorithm will return the\r\narray  sorted.\r\nNext we deal with the runtime of MergeSort. We assume for simplicity\r\nthat  is an exact power of 2 and\r\nuse  to denote the time for\r\nMergeSort to sort  elements. Then\r\nwe have the runtime for each line: \r\nThis yields a recurrence equation where  depends on :\r\n\r\nIf , then , and the algorithm terminates in\r\nconstant time  \r\n“The time for MergeSort to sort  elements is twice the time for\r\nMergeSort to sort  elements plus\r\n time for Merge.”\r\nOtherwise we have , where\r\n\r\n is the time to\r\ndivide into subproblems, \r\n is the time to solve\r\n subproblems each of size , \r\n is the time to\r\nconquer (to combine the obtained sub-solutions), \r\n\r\n\r\nTo solve a recurrence equation like this, we have several\r\nmethods:\r\n\r\nSubstitution method: guess a solution and verify using\r\ninduction.\r\nDraw a recursion tree, add times across the tree.\r\nUse the Master Theorem to solve a general recurrence\r\nequation in the shape of\r\n\r\n\r\nThe recursion tree of the MergeSort:  The\r\ntree obviously has a depth of . And we add the runtimes of the nodes at the same level to\r\nget the runtime for each level. It is easy to see that all levels except\r\nthe leaf level have a runtime of . So the runtime is \r\nComparison with Insertion\r\nSort\r\n\r\nThe MergeSort always runs in time . This is way better than\r\nthe worst case and average case of  for InsertionSort.\r\nBut it is worse than the best-case time  of InsertionSort. So the\r\nInsertionSort might be faster if the array is almost sorted.\r\nMergeSort needs more space than InsertionSort:\r\n\r\nMergeSort always stores  elements outside the\r\ninput.\r\nInsertionSort only needs \r\nadditional space.\r\nWe say that InsertionSort sorts in place.\r\n\r\n\r\n\n  \n    \n      Definition: In-place Sorting\r\n\n    \n    \n      A sorting algorithm sorts in place if it only uses  additional space.\r\n\n    \n  \r\nThe Master Theorem\r\nThe master theorem provides a “cookbook” method for solving\r\nrecurrences of the form  where  and .\r\n\r\n is called the driving\r\nfunction, and\r\n is called the master\r\nrecurrence.\r\n\r\nThe master recurrence \r\ndescribes the running time of a divide-and-conquer algorithm that\r\ndivides a problem of size  into\r\n subproblems each of size , i.e., the algorithm solves each\r\nsubproblem in time .\r\nThe driving function \r\ndescribes the cost of dividing the problem before the recursion, as well\r\nas the cost of combining the results together.\r\nImportant term:  is\r\ncalled the watershed function.\r\n\n  \n    \n      The Master Theorem\r\n\n    \n    \n      Let  and  be constants, and let  be non-negative for large enough\r\n. Then, the solution of the\r\nrecurrence function defined over   has\r\nthe following asymptotic behavior:\r\n\r\nIf there exists a constant  s.t. , then .\r\nIf there exists a constant  s.t. , then .\r\nIf there exists a constant  s.t. , and if\r\n additionally satisfies the\r\nregularity condition  for some constant  and all sufficiently large , then .\r\n\r\n\n    \n  \r\n\n  \n    \n      Caution\r\n\n    \n    \n      The master theorem is inexhaustive and does not apply to all\r\npossible recurrence equations. But in practice it does cover the vast\r\nmajority of the recurrence equations we may face.\r\n\n    \n  \r\nThe master theorem allows us to state the master recurrence  without floors and ceilings even\r\nwhen we don’t have problems of exactly the same size. e.g.,  The three cases can be understood intuitively as follows:\r\n\r\nCase 1: the watershed function must grow polynomially\r\nfaster than , by at least\r\na factor  for\r\nsome constant .\r\nCase 2: watershed and driving functions grow asymptotically\r\nnearly at the same rate (the growth are the same for ).\r\nCase 3: the watershed function must grow polynomially\r\nslower than , by at least\r\na factor  for\r\nsome constant , and\r\nthe regularity condition must be satisfied.\r\n\r\nMergeSort Example\r\nThe runtime of the MergeSort is  Then we have  It is easy to see that the case 2 hold by  So the solution is \r\nFurther Examples 1\r\nFor the following recurrence equation  we have  The case 1 holds because  So the solution is \r\nFurther Examples 2\r\nFor the following recurrence equation  we have  The case 3 holds because  and  So the solution is \r\n","tags":["Notes","Data Structures and Algorithms"]},{"title":"Data Structures and Algorithms (H) Lecture 2 Notes","url":"/2025/09/18/dsaa-lec-2/","content":"Runtime and Asymptotic\r\nNotation\r\nIn the previous lecture, we showed a naive way to analyze the runtime\r\nof algorithms. But this naive approach is very heavy and can be\r\nparticularly troublesome when comparing algorithms that have complex\r\nruntime expressions. To simplify this, we introduce the asymptotic\r\nnotation. A high-level intuition of the asymptotic notation is that\r\nit captures, and only captures the growth speed of\r\nfunctions.\r\nThe running time of every instance is sandwiched between the best\r\ncase and the worst case running time. The worst case is often more\r\nimportant because:\r\n\r\nit guarantees that the algorithm will never take longer time.\r\nfor some algorithms, the worst case is quite frequent.\r\noften the average case is as bad as the base case.\r\n\r\nWhere the average case is defined as the performance on “average”\r\ninput. e.g. for sorting it means that we assume that every possible\r\npermutation is equally likely.\r\nSome key observations:\r\n\r\nthe biggest-order term (e.g.  vs. ) dominates the runtime as  grows.\r\nand how the runtime scales with  is more important than constant factors\r\n(for sufficiently large ).\r\nadditive smaller order terms become irrelevant for large\r\n.\r\nwe care about large  because\r\nsmall problems are always easy to solve.\r\n\r\nAsymptotic Notation: \r\n\n  \n    \n      Definition:  Notation\r\n\n    \n    \n      For a given non-negative function  we denote by  the set of\r\nfunctions  A function \r\nbelongs to the set  if\r\nit can be “sandwiched” between  and  for sufficiently large . By the convention of computer science\r\nliterature, we write  instead of . We say that  is an asymptotically tight\r\nbound of .\r\n\n    \n  \r\nIntuitively, \r\nmeans that captures  grows\r\nessentially the same speed as .\r\n\n  \n    \n      Example\r\n\n    \n    \n      \r\n\n    \n  \r\n expresses tight upper and\r\nlower bounds on . There are\r\nother two notations:\r\n\r\nUse  if we only want to\r\nexpress an upper bound.\r\nUse  if we only want to\r\nexpress a lower bound.\r\n\r\nThe formal definition of these two new notations are similar:\r\n\n  \n    \n      Definition: \r\nNotation\r\n\n    \n    \n      For a given non-negative function  we denote by  the set of functions\r\n For a given non-negative\r\nfunction  we denote by  the set of\r\nfunctions \r\n\n    \n  \r\n\n  \n    \n      Tip\r\n\n    \n    \n      \r\n\n    \n  \r\nWe also have  and  indicate strictly slower and\r\nfaster growth, respectively: \r\nAn overview of asymptotic notation:\r\n\r\n\r\n\r\nNotation\r\nMeaning\r\nAnalogy\r\n\r\n\r\n\r\n\r\n\r\n grows at most as fast as\r\n\r\n\r\n\r\n\r\n\r\n grows at least as fast as\r\n\r\n\r\n\r\n\r\n\r\n grows as fast as \r\n\r\n\r\n\r\n\r\n grows slower than \r\n\r\n\r\n\r\n\r\n grows faster than \r\n\r\n\r\n\r\n\r\n\n  \n    \n      Caution\r\n\n    \n    \n      Always remember that  is a “incorrect” convention\r\nfor expressing .\r\n\n    \n  \r\nBy using the asymptotic notation, we can now describe the runtime of\r\ninsertion sort as:  ### Common runtimes\r\n\r\n\r\n\r\nMathematical expression\r\nName\r\n\r\n\r\n\r\n\r\n\r\nlogarithmic time\r\n\r\n\r\n\r\nlinear time\r\n\r\n\r\n\r\nquadratic time\r\n\r\n\r\n\r\ncubic time\r\n\r\n\r\n for \r\npolynomial time\r\n\r\n\r\n\r\nexponential time\r\n\r\n\r\n\r\n\n  \n    \n      Tip\r\n\n    \n    \n      \r\nEvery polynomial of \r\ngrows strictly slower than every polynomial of .\r\nEvery polynomial of  grows\r\nstrictly slower than every exponential function.\r\n\r\n\n    \n  \r\nHow to find \r\n\r\nIt is often helpful to divide by , e.g.  Then try \r\nsandwiching the constant term.\r\nRemember that .\r\nAlso remember that inequalities need to hold for all .\r\nNo need to invest time to find the best possible constants.\r\n\r\nPractical rules\r\nto make runtime analysis simple\r\nFor two non-negative functions  and :\r\n\r\nSlower functions can be ignored: \r\nAsymptotic times can be multiplied: \r\n\r\nAsymptotic notation:\r\ncomparing sets\r\nIs  true or false? (Tip: think of  as a placeholder for an\r\nanonymous function from the set  of all functions that grow\r\nlinearly in .)\r\nSuch statement is true if no matter how the anonymous functions are\r\nchosen on the left of the equal sign, there is a way to choose the\r\nanonymous functions on the right of the equal sign to make the equation\r\nvalid.\r\n\n  \n    \n      Example\r\n\n    \n    \n      \r\n is true because\r\n.\r\n is false,\r\ne.g.  but .\r\n\r\n\n    \n  \r\n","tags":["Notes","Data Structures and Algorithms"]},{"title":"Data Structures and Algorithms (H) Lecture 4 Notes","url":"/2025/10/01/dsaa-lec-4/","content":"HeapSort\r\nIntroduction\r\nThe idea behind HeapSort is to find the largest element and move it\r\nto the end of the array. Then we repeat this process with remaining\r\nelements until the array is sorted. This idea is similar to the\r\nSelectionSort, but SelectionSort compares lots of elements to find the\r\nlargest, while the HeapSort store knowledge gained from these\r\ncomparisons for future use. This make the HeapSort very efficient on\r\nboth runtime and storage, and it also sorts arrays in place. More\r\nspecifically, we use a clever data structure, a heap to achieve\r\nthis.\r\nHeap\r\nA heap is essentially an array imagined as being a\r\nbinary tree. The elements are arranged row by row from left to\r\nright. For example, the array  can be viewed as a heap like this  We\r\nnavigate through the array/imaginary tree using these operations:  - Max-heap property: for every node other than the\r\nroot, the parent is no smaller than the node, i.e., . In a\r\nmax-heap, the root always stores a largest element, this is\r\nexactly what we wants for the sorting algorithm. - Min-heap\r\nproperty, for every node other than the root, the parent is no\r\nlarger than the node, i.e., .\r\nProcedures Needed for the\r\nAlgorithm\r\n\r\nBuild-Max-Heap: produces a max-heap from an unordered\r\narray.\r\nMax-Heapify: maintains the max-heap property once the\r\nmaximum (the root element) has been removed.\r\nHeapSort: sorts an array in place.\r\n\r\nWe also need to introduce a new variable A.heap-size\r\nwhich indicates how many elements of  are stored in a heap. Decreasing\r\nA.heap-size by 1 effectively removes the last element from\r\nthe heap. There are analogous operations for min-heaps.\r\nMax-Heapify(A,i)\r\nFirst we assumes subtrees  and  are max-heaps, but\r\nmax-heap property might be violated in root of subtree at . Then to restore the max-heap property\r\nat , we lets the value at  “float down”, if necessary. At the\r\nend of Max-Heapify the subtree at  is a max-heap. To do this, we proceed\r\nas follows: - Compare  with all\r\nexisting children, and - if the largest child is larger than\r\n, swap and recurse on child.\r\n\r\nRuntime\r\nTo figure out the runtime of Max-Heapify, we need to\r\nfirst define the height of the node in a tree. \n  \n    \n      Definition: Height of Tree Nodes\r\n\n    \n    \n      The height of a tree node is the longest number of simple\r\ndownward edges from the node to a leaf.\r\n\n    \n  \r\nWe know that Max-Heapfity takes constant time  on each level. Then the running\r\ntime of Max-Heapify on a node of height  is . We claim that the height of a\r\nheap, i.e. the height of the root, is at most . Proof: the number  of elements in a heap of height  is - doubling on each level - at least\r\n1 node on the last level - hence in total at least we have  So the size and height are related as  So the runtime is .\r\nCorrectness\r\nWe prove the correctness by induction on the height: - Base\r\ncase: height is 0, which means we are on a leaf, this is trivial. -\r\nInductive case: assume the algorithm is work for height  and show it works for . Then the algorithm swaps  with the larger between  and  (if any) and one subtree\r\nwas already a heap and the other will be by inductive hypothesis.\r\nBuild-Max-Heap(A,n)\r\nThe idea is to use Max-Heapify repeatedly to create a\r\nheap. Since Max-Heapify assumes  and  are heaps, so we need to\r\nconstruct the heap bottom-up.\r\n\n  \n    \n      Tip\r\n\n    \n    \n      Nodes in \r\nare all leaves. Since leaves are already max-heaps, so no work is\r\nrequired on them.\r\n\n    \n  \r\n A example run of this algorithm on the following\r\ninitial array:  \r\nCorrectness\r\nWe prove the correctness of Build-Max-Heap by using loop\r\ninvariant. - Loop invariant: At the start of each iteration\r\n of the for loop, each node  is the root of a\r\nmax-heap. - Initialization: true for leaves .\r\n- Maintenance: by loop invariant, all children of  are roots of max-heaps as their indexes\r\nare larger than . Then\r\nMax-Heapify(A,i) turns the subtree at  into a max-heap. -\r\nTermination: the loop terminates at , hence node 1 is the root of a\r\nmax-heap.\r\nRuntime\r\nSince all nodes have height at most , and every call to Max-Heapify takes time . The\r\nBuild-Max-Heap calls Max-Heapify  times. So the total time is at most\r\n But this is not a tight bound, though it is sufficient for our\r\ndiscussion. The runtime can be improved to  since most of the nodes have a small\r\nheight.\r\nRefined Runtime Analysis\r\nThe key observation is that most nodes in the tree have small height.\r\nWe can show that there are at most \r\nnodes of height . We prove this by\r\ninduction:\r\nA better bound for the Build-Max-Heap can then be\r\ncalculated as  as the infinite series of  is . Two important inequalities we used: -\r\n for , from this we get for\r\n,  because\r\n. - \r\nfor .\r\nHeapSort\r\nWith all the procedures introduces above, we can now describe the\r\nidea of the HeapSort as follows: - Build a max-heap, s.t.\r\nthe root contains largest element. - Swap the root with the last element\r\nof the heap/array. - Discard the last element from the heap by reducing\r\nthe size of the heap by 1. - Call Max-Heapify(A,1) to\r\nrestore heap property at the root. The pseudocode goes as follows:  The runtime will then be \r\nCorrectness of HeapSort\r\n\r\nLoop invariant: At the start of each iteration of the for\r\nloop of line 2-5, the subarray  is a max-heap containing the\r\n smallest elements of , and the subarray  contains the  largest elements of  and is sorted.\r\nInitialization: The subarray  is empty, the invariant\r\ntrivially holds.\r\nMaintenance:  is\r\nthe largest element in  and\r\nit is smaller than the elements in . When we put it in the -th position, then  contains the largest elements,\r\nsorted. Decreasing the heap size and calling Max-Heapify\r\nturns  into a max-heap.\r\nDecrementing  sets up the\r\ninvariant for the next iteration.\r\nTermination: After the loop . This means that  is sorted and  is the smallest element in the\r\narray, which makes the array sorted.\r\n\r\n","tags":["Notes","Data Structures and Algorithms"]},{"title":"Data Structures and Algorithms (H) Lecture 6 Notes","url":"/2025/10/15/dsaa-lec-6/","content":"Randomization and Lower\r\nBounds\r\nA Randomized Version of\r\nQuickSort\r\nChoosing the right pivot element can be tricky since we have no idea\r\na priori which element can be a good pivot. A (surprisingly good)\r\nsolution is to leave it to chance: \r\nPerformance of\r\nRandomized-QuickSort\r\nFirst we assume that in the following that all elements in the array\r\nare distinct. Now we may ask: what is a worst-case input for\r\nRandomized-QuickSort? The answer is that there is no\r\nworst case for Randomized-QuickSort! The reason behind\r\nthis is simple: all inputs lead to the same runtime behavior. -\r\nThe -th smallest element is chosen\r\nwith uniform probability. - Every split is equally likely, regardless of\r\nthe input. - The runtime is random, but the random process (probability\r\ndistribution) is the same for every input.\r\nRuntime of Randomized\r\nAlgorithms\r\nFor randomized algorithms (in contrast to deterministic\r\nalgorithms) we consider the expected running time .\r\n\n  \n    \n      Tip\r\n\n    \n    \n      For more information on expectations, random variables, and\r\nprobabilities, see the notes for STA219 Probability\r\nand Statistics for Engineering.\r\n\n    \n  \r\nFor analyzing sorting algorithms the number of comparisons\r\nof elements made is an interesting quantity:\r\n\r\nFor QuickSort and other algorithms it can be used as a\r\nproxy or substitute for the overall running time.\r\n\r\nAnalyzing the number of comparisons might be easier than analyzing\r\nthe number of elementary operations.\r\n\r\nComparisons can be costly if the keys to be compared are\r\nnot numbers, but more complex object.\r\nAlgorithms making fewer comparisons might be preferable, even if the\r\noverall runtime is the same.\r\nThere is a lower bound for the running time of all sorting\r\nalgorithms that rely on comparisons only.\r\n\r\nLet  be the number of\r\ncomparisons of elements made by QuickSort. Since\r\ncomparisons are elementary operations, .\r\nFor each comparison QuickSort only makes  other operations in the\r\nfor loop. So other operations sum to . Hence we have  and thus  and we need to show  So we can analyze the number of operations as a substitute for\r\nthe runtime in the RAM model.\r\nExpected Time for\r\nRandomized-QuickSort\r\n\n  \n    \n      Theorem\r\n\n    \n    \n      The expected number of comparisons of\r\nRandomized-QuickSort is  for every input where all elements are distinct.\r\n\n    \n  \r\nProof. for ease of analysis, rename array elements to  with , i.e.  is the -th smallest element.\r\nA key observation is that each pair of elements is compared\r\nat most once. The reason is that elements are only compared\r\nagainst the pivot, and after Partition ends the pivot is\r\nnever touched again.\r\nLet  be the number of\r\ntimes  and  are compared:  Then the total number of comparisons is  Taking expectations on both sides and using linearity of\r\nexpectations gives us   is compared against\r\n when:\r\n\r\nIf pivot is  or  then the decision whether to\r\ncompare  is\r\npostponed to a recursive call.\r\nIf pivot is  or , then they are compared.\r\nIf pivot is \r\nthen  and  become separated and are never\r\ncompared.\r\n\r\nA decision is only made if . So  and  are only compared if the\r\nfirst pivot chosen amongst  is either  or\r\n. Since gthese are  values, out of which 2 lead to\r\n being compared. And since\r\nthe pivot element is chosen uniformly at random, we have  Substitute this back into the expectation expression, we have:\r\n Substituting \r\nyields \r\nRandom Input\r\nvs. Randomized Algorithm\r\nQuickSort is efficient if:\r\n\r\nThe input is random or\r\nthe pivot element is chosen randomly.\r\n\r\n\n  \n    \n      Key Insight\r\n\n    \n    \n      We have no control over the former, but we can make the latter\r\nhappen.\r\n\n    \n  \r\n\r\nDeterministic QuickSort\r\n\r\nPro: the runtime is deeterministic for each input\r\nCon: may be inefficient on some inputs\r\n\r\nRandomized QuickSort\r\n\r\nPro: same behavior on all inputs\r\nCon: runtime is random, running it twice gives different times\r\n\r\n\r\nOther Applications of\r\nRandomization\r\n\r\nRandom Sampling\r\n\r\nGreat for big data\r\nSample likely reflects properties of the set it is taken from\r\n\r\nSymmetry breaking\r\n\r\nVital for many distributed algorithms\r\n\r\nRandomized search heuristics\r\n\r\nGeneral-purpose optimizers, great for complex problems\r\n\r\n\r\nComparison Sorts\r\nSo far we have learnt the following sorting algorithms:\r\n\r\nInsertionSort\r\nSelectionSort\r\nMergeSort\r\nHeapSort\r\nQuickSort\r\n\r\nAll these algorithms proceed by comparing elements, hence the name\r\ncomparison sorts. The best runtime we have achieved on such\r\nalgorithms is  in\r\nthe worst case. In this section, we will prove that it’s impossible\r\nto do better.\r\nA Very Brief Tast of\r\nComplexity Theory\r\nComplexity theory deals with the difficulty of problems. Or\r\nmore formally, it studies the limits to the efficiency of\r\nalgorithms. The complexity theory often gives results like: every\r\nalgorithm needs at least time  in\r\nthe worst case to solve problem .\r\nThis kind of results stops us from wasting time trying to achieve the\r\nimpossible, and informs the design of efficient algorithms.\r\nA famous concept in the complexity theory is the\r\nNP-Completeness and NP-Complete Problem. This concept\r\nfirst arises from the study of Entscheidungsproblem (German for\r\ndecision problem). An example of the decision problem is\r\n\n  \n    \n      Example\r\n\n    \n    \n      Does there exist an assignment of variables that satisfies a Boolean\r\nformula?\r\n\n    \n  \r\nThe NP-complete problems:\r\n\r\ncontain &gt;3000 important problems in different shapes.\r\nis easy to verify that a given solution means “yes”.\r\nno one knows how to find a solution in polynomial\r\nworst-case time.\r\neither no NP-complete problem is solvable in polynomial time, or all\r\nof them are.\r\n\r\nTo show that a given time  is best possible, we need to\r\nfind arguments that apply to all algorithms that can ever be\r\ninvented.\r\nComparison Sorts as Decision\r\nTrees\r\nFrom the previous sections we know that we can take the number of\r\ncomparisons as lower time bound. w.l.o.g. we assume that elements  are distinct, then\r\nwe can assume that all comparisons have the form .\r\nA decision tree reflects all comparisons a particular\r\ncomparison sort makes, and how the outcome of one comparison determines\r\nfuture comparisons.\r\nIn the decision tree, a inner node  means comparing  and . The leaves are ordering  established by\r\nthe algorithm:  A leaf contains a sorted output for a particular input. The\r\nexecution of a sorting algorithm corresponds to tracing a simple path\r\nfrom the root down to a leaf.\r\nAn example of a decision tree: \r\n\n  \n    \n      Theorem\r\n\n    \n    \n      Every comparison sort requires  comparisons in the worst\r\ncase. The theorem can be also extended to an  bound for the\r\naverage-case time.\r\n\n    \n  \r\nProof. The worst-case number of comparisons equals the\r\nlength of the longest simple path fomr the root to any\r\nreachable leaf, i.e. the height  of the tree.\r\nEvery correct algorithm must be able to produce a sorted output for\r\neach of the  possible orderings\r\nof the input. So the number of the leaves of the decision tree must be\r\nat least . Since a binary tree of\r\nheight  has no more than  leaves, to accommodate  we will need . Taking logarithms gets us\r\n.\r\nSo the worst-case number of comparisons is at least . Use the fact  we get \r\n","tags":["Notes","Data Structures and Algorithms"]},{"title":"Data Structures and Algorithms (H) Lecture 7 Notes","url":"/2025/10/21/dsaa-lec-7/","content":"Sorting in Linear Time\r\nCountingSort\r\nTo circumvent the bound of comparison sort, we need to get more\r\ninformation than mere comparisons. Since elements to be sorted are often\r\nnumbers or strings, we can use this as a property that we could\r\nutilize.\r\nAssume that the input elements are integers in . And for each element , CountingSort counts the\r\nnumber of elements less than .\r\nThen we reconstruct the array using this information. This will only use\r\nlinear time. The CountingSort uses an array  for counting and an array  for writing the output.\r\n\n  \n    \n      Caution\r\n\n    \n    \n      If the input array contains duplicate elements, we must find a way to\r\nplace them into different output positions.\r\n\n    \n  \r\n\r\nThe CountingSort proceeds as follows:\r\n\r\nInitialize the counter array\r\nCount elements\r\nRunning sum elements \r\nWrite elements to output\r\n\r\nThe runtime of CountingSort is straightforward to see:\r\n If  then the\r\nruntime will be simply .\r\nCorrectness\r\n\r\nLoop invariant: At the start of each iteration  of the last for loop, the\r\nelements  are in the right\r\nposition in  and the last element\r\nin  that has not yet been copied\r\nin , with value , belongs to .\r\nInitialization: At the start of the loop  and no elements have been copied. The\r\narray  provides for each element,\r\nthe number of elements in  that\r\nare smaller or equal to it. So the last element of , , naturally goes in position .\r\nMaintenance: At iteration , the loop invariant tells us that the\r\nelement  goes in  and we copy in. Since the next\r\nelement equal to  in  that has not yet been copied in  should go in position , we decrement  re-establishing the loop\r\ninvariant.\r\nTermination: When the loop terminates . The loop invariant tells us that all\r\nthe elements of  are in the\r\nright position in  thus there are\r\nno more elements to be copied.\r\n\r\nStability in Sorting\r\n\n  \n    \n      Definition: Stable Sorting\r\n\n    \n    \n      We say that a sorting algorithm is stable iff same elements\r\nappear in the output in the same order as they do in the input\r\narray.\r\n\n    \n  \r\nThe CountingSort is stable. This property is relevant\r\nwhen satellite data is attached to keys being sorted, e.g. sorting\r\nemails according to date.\r\nAdvantages &amp; Disadvantages\r\nPros:\r\n\r\nSorts in linear time  integers\r\nin the range  if .\r\nIs stable\r\n\r\nCons:\r\n\r\nDoes not sort in place\r\nCan be slow if .\r\n\r\nRadixSort\r\nSince there are only 10 different integers in a digit of a decimal\r\nnumber (or the 52 possible letters in a English word), we can utilize\r\nthe stability of CountingSort to limit the size of  by sorting the digits:  Example of running RadixSort: \r\nThe runtime of RadixSort is clearly \r\nCorrectness\r\nThe correctness of RadixSort follows from the stability\r\nof CountingSort and induction on columns.\r\n\r\nLoop Invariant: At each iteration of the for\r\nloop, the array is sorted on the last  digits.\r\nInitialization: The array is trivially sorted on the empty\r\nset of digits for .\r\nMaintenance: The invariant tells us that the array is\r\nsorted on the last  digits. Now\r\nwe sorted the -th digit\r\nre-establishing the loop invariant, since our stable sort ensures that\r\nelements with same -th digit\r\nremain in the same order as before sorting.\r\nTermination: The loop terminates when . Then the loop invariant states\r\nthat the array is completely sorted.\r\n\r\n","tags":["Notes","Data Structures and Algorithms"]},{"title":"Probability and Statistics for Engineering Lecture 1-2 Notes","url":"/2025/09/18/pse-1/","content":"Basic Ideas in Probability\r\nBasic Concepts\r\n\n  \n    \n      Definition: Random Experiment\r\n\n    \n    \n      A random experiment, often simply called an\r\nexperiment, represents the realization or observation of a\r\nrandom phenomenon and has the following characteristics:\r\n\r\nit can be repeated under the same conditions;\r\nall possible outcomes are clearly known;\r\nexactly one of these possible outcomes occurs each time, but it\r\ncannot be determined in advance which outcome will occur.\r\n\r\n\n    \n  \r\n\n  \n    \n      Definition: Sample Point and Sample Space\r\n\n    \n    \n      Each possible fundamental outcome of a random experiment is\r\ncalled a sample point, usually denoted as . A set that includes all sample\r\npoints of the experiment is called the sample space, usually\r\ndenoted as .\r\n\n    \n  \r\n\n  \n    \n      Definition: Event\r\n\n    \n    \n      From a set theory perspective, a random event, or simply\r\nevent, is a subset of the sample space , typically denoted by uppercase\r\nletters. If the outcome is a sample point in event , then we say that event  happens.\r\n\n    \n  \r\n\r\nInclusion :  happens when  happens.\r\nSum/union /: at least one of  and  happens.\r\nProduct/intersection /:  and  both happen.\r\nDifference /:  happens and  does not happen.\r\nMutually exclusive/disjoint :  and  cannot happen at the same time.\r\nComplement /: either  or\r\n happens, denoted as  or .\r\n\r\nThe operations of events obey certain rules similar to the rules of\r\nset:\r\n\r\nCommunicative laws;\r\nAssociative laws;\r\nDistributive laws;\r\nDe Morgan’s laws:\r\n\r\n Having defined the sample space and random event, we can now\r\ndiscuss the probability of events: \n  \n    \n      Definition: Probability\r\n\n    \n    \n      Probability measure, or simply probability, is a\r\nreal-valued function defined on subsets of the sample space , satisfying the following three\r\naxioms: - Non-negativity: . - Normalization: . - Additivity: for\r\nany countable number of mutually exclusive events , we have \r\n\n    \n  \r\nFrom the three axioms, we can derive many useful properties: -  Proof: trivial. -\r\nFinite additivity: for any finite sequence of mutally exclusive\r\nevents ,  Proof: straightforward of the additivity axiom with\r\n for all .\r\n\r\nThe complement rule: . Proof:\r\nimmediate from the finite additivity with .\r\nThe numeric bound: . Proof: straightforward from the complement\r\nrule.\r\nMonotonicity: if , then  and\r\n. Proof:\r\nfrom the finite additivity, since , we have . We also know\r\nthat , so we have\r\n.\r\nThe addition law: .\r\nThe inclusion-exclusion principle: \r\n\r\nProof: We prove this by induction on . Base case: for :\r\n\r\ntrivially holds. for :\r\n\r\nis exactly the same as the addition law.\r\nInductive case: Consider events sequence , then:\r\n\r\nThen from the addition law:\r\n\r\nFrom the distributive laws:\r\n\r\nLet\r\n\r\nThen:\r\n\r\nApply the inductive hypothesis to the events :\r\n\r\nComputing Probabilities\r\n\n  \n    \n      Addition and Multiplication Principles\r\n\n    \n    \n      Addition principle: If there are  types of methods to complete a task,\r\nwith  specific methods in the\r\nth type, then the total number of\r\nspecific methods to complete this task is: \r\nMultiplication principle: If there are  steps to complete a task, with  possible methods for the th step, then the total number of\r\nmethods to complete this task is: \r\n\n    \n  \r\n\n  \n    \n      Definition: Permutation and Combination\r\n\n    \n    \n      Permutation: If \r\nelements are randomly selected without replacement from  distinct elements () and placed in order,\r\nthen the number of different permutation is \r\nCombination: The number of combinations of  elements randomly selected without\r\nreplacement from  distinct\r\nelements (), where the order\r\ndoes not matter, is given by \r\n\n    \n  \r\n\n  \n    \n      Example\r\n\n    \n    \n      Suppose a class of  students\r\nhas been allocated  () concert tickets by the\r\nuniversity. The teacher decides to distribute the tickets by drawing\r\nlots. The teacher prepares a hat containing  slips of paper, with  slips marked with 1 and the remaining\r\nslips marked with 0. The students take turns drawing slips from the hat,\r\nand those who draw a slip marked with a 1 will get a concert ticket. If\r\nyou are one of the students in the class and you really want to get a\r\nticket, would you choose to draw early or late?\r\n\n    \n  \r\nSolution. The essential question is that, suppose you are\r\nthe th to draw a slip, does the\r\nprobability depend on ?\r\nConsidering the ticket drawing as a random experiment, then each\r\npossible outcome of the drawing process can be represented by a -digit binary number, with  digits being 1. Then the number of\r\nsample points is . w.l.o.g. we\r\nassume that . Then let  denote the event that you get a ticket,\r\nso ,\r\n\n  \n    \n      Example\r\n\n    \n    \n      There are  student in a class\r\nwith student IDs . Now\r\nwe prepare  gifts numbered with\r\neach possible student IDs. Then everyone randomly select their gift from\r\na big bag. What’s the probability that at least one student get a gift\r\nwith his/her own ID?\r\n\n    \n  \r\nSolution. Each permutation of  is a sample point,\r\nrepresenting the gift numbers corresponding to students with IDs . So . Let  be the event that at least one student\r\ngets his/her own gift, then directly determining  is not so straightforward. Let  be the event that the student with ID\r\n get his/her own gift, then  The events  are very\r\nclear and their probability computation is simple:  Put these together, we have: \r\nGeometric Model of\r\nProbability\r\nThe classical model of probability assumes finite number of sample\r\npoints and equal likelihood. Another model, called the geometric\r\nmodel of probability, is a natural extension of the classical model\r\nto an infinite number of sample points while maintaining equal\r\nlikelihood. \n  \n    \n      Definition: Geometric Model of Probability\r\n\n    \n    \n      If a random experiment can be represented as randomly throwing a\r\npoint onto a bounded region ,\r\nwhere the point is equally likely to land at any position\r\nwithin the region, then the probability model of the experiment is\r\ncalled a geometric model of probability. Let  be the event that the point lands at a\r\nsubregion  of , then the probability of event\r\n is computed as \r\n\n    \n  \r\n\n  \n    \n      Example\r\n\n    \n    \n      Bertrand’s paradox: Consider a circle with radius 1. What is the\r\nprobability that a randomly draw chord of the circle is longer than the\r\nside of the inscribed equilateral triangle of the circle?\r\n\n    \n  \r\nSolution 1Solution 2Solution 3Possible solution 1. Take a radius of the circle , and randomly choose a point  on the radius, then draw a chord\r\nthrough  orthgonal to . By elementary geometry,  intersects the triangle at the\r\nmidpoint of , say . Then the sample space is all points on\r\n. For the chord to be longer than\r\nthe side of the triangle,  must\r\nfall on , therefore, the\r\nprobability is Possible solution 2. Take a point on the circle, say , draw the tangent to the circle through\r\n. Then randomly choose another\r\npoint  on the circle, and draw a\r\nchord by connecting  and  that forms a random angle  with the tangent. Then the sample\r\nspace is , and for the\r\nchord to be longer than the side of the triangle, we need . Therefore, the\r\nprobability is Possible solution 3. Randomly choose a point  within the circle and draw a chord with\r\n as the midpoint. For the chord to\r\nbe longer than the side of the inscribed equilateral triangle,  must fall within the inscribed circle\r\nof the triangle, whose radius is . Then the sample space is the\r\noriginal circle, the event of interest is the inscribed circle.\r\nTherefore, the probability is \r\nBoth of the three solutions are correct. The reason why the\r\nprobabiliy varies across different solutions is that the hypothesis “the\r\nchord is randomly drawn” is not clearly defined. Different assumptions\r\nof equal likelihood lead to different sample spaces:\r\n\r\nSolution 1: equally likely chosen on a radius, so the sample space\r\nis the radius.\r\nSolution 2: equally likely chosen on the circle, so the sample space\r\nis the circle boundary.\r\nSolution 3: equally likely chosen within the circle, so the sample\r\nspace is the whole circle area.\r\n\r\nTherefore, when computing probabilities, it is crucial to clearly\r\ndefine the sample apce first.\r\nConditional\r\nProbability and Independence\r\n\n  \n    \n      Definition: Conditional Probability\r\n\n    \n    \n      Let  and  be two random events and . Then the conditional\r\nprobability of event  given\r\nthat event  occurs is defined as\r\n\r\n\n    \n  \r\nThe idea behind this definition is that if event  has occured, the sample space becomes\r\n instead of .\r\n\n  \n    \n      Example\r\n\n    \n    \n      You are playing a poker game where you are dealt with 5 cards face\r\ndown. If one of the cards that you are dealt lands face up, showing the\r\nAce of spades, what is the probability of having a royal flush now?\r\n\n    \n  \r\nSolution. Let  be the\r\nevent of having a royal flush. The probability of  is straightforward:  Let  be the event that\r\none of the 5 cards is the Ace of spades, then the number of sample\r\npoints in  is .  is the event of a royal flush\r\nof spades. So the probability of \r\ngiven  is \r\n\n  \n    \n      Multiplication Law\r\n\n    \n    \n      Let  and  be two random events and . Then .\r\n\n    \n  \r\n\n  \n    \n      Law of Total Probability\r\n\n    \n    \n      Let  and  be two random events, then (assume that\r\n if )  More generally, if  are  mutually exclusive random\r\nevents, and ,\r\nthen we call  a\r\npartition of the sample space , and \r\n\n    \n  \r\nJust like how the addition law generalize to the inclusion-exclusion\r\nlaw, the multiplication law also has a generalized version, called the\r\nchain rule for random events. s \n  \n    \n      Chain Rule\r\n\n    \n    \n       assume that .\r\n\n    \n  \r\n\n  \n    \n      Bayes’ Theorem\r\n\n    \n    \n      Let  be random\r\nevents and  is a\r\npartition of the sample space. Then for any event  s.t. , we have \r\n is the probability of\r\n given that  occurs, also termed the posterior\r\npossibility of .  is called the prior\r\nprobability or marginal probability of , which refers to the probability\r\nvalue in the absence of any other prior information.\r\n\n    \n  \r\nA intuitive understanding of the Bayes’ rule and the law of total\r\nprobability is:\r\n\r\nthe law of total probability can be viewed as from cause to\r\neffect, since we are calculating the probability of the outcome\r\nevent based on all possible causes.\r\nthe conditional probability  obtained by the Bayes’ rule can\r\nbe viewed as the probability of the specifc cause  led to the observed outcome . Simply put, we are reasoning from\r\neffect to put.\r\n\r\n\n  \n    \n      Definition: Independence\r\n\n    \n    \n      Let  all\r\ndenote random events. -  and  are said to be independent if\r\n. -  are said to be\r\nmutually independent if for every subset of these events  (, ), we have \r\n\n    \n  \r\n\n  \n    \n      Caution\r\n\n    \n    \n      Pairwise independence does not imply mutual\r\nindependence.\r\n\n    \n  \r\nIndependence can sometimes rely on the occurrence of some random\r\nevent. This is the concept of the conditional independence.\r\n\n  \n    \n      Definition: Conditional Independence\r\n\n    \n    \n      Let  all\r\ndenote random events.  are said to be\r\nconditionally independent conditioned on  if for every subset of the events,\r\n\r\n(,\r\n), we have \r\n\n    \n  \r\n\n  \n    \n      Example\r\n\n    \n    \n      Draw three cards from a properly shuffled standard deck, with\r\nreplacement and reshuffling. Let \r\nbe the event that “card 1 and 2 have the same suit”,  be the event that “card 2 and 3 have\r\nthe same suit”,  be the event that\r\n“card 1 and 3 have the same suit”. Are  mutually independent?\r\n\n    \n  \r\nSolution. It is trivial to show that . And  are the same event “card 1,\r\n2, 3 have the same suit”, so it is also trivial that .\r\nTherefore, we have  but  So the three events are pairwise independent, but not\r\nmutually independent.\r\n","tags":["Notes","Probability and Statistics for Engineering"]},{"title":"Data Structures and Algorithms (H) Lecture 5 Notes","url":"/2025/10/12/dsaa-lec-5/","content":"QuickSort\r\nIntroduction\r\nThe quick sort is also a divide-and-conquer algorithm just like the\r\nmerge sort:\r\n\r\nDivide:\r\n\r\nPick some element called pivot.\r\nMove it to its final location in the sorted sequence s.t. all\r\nsmaller elements are to its left, larger ones are to its right.\r\n\r\nConquer:\r\n\r\nRecursively sort subarrays for smaller and larger elements.\r\n\r\nCombine:\r\n\r\nNo work needed here since after the recursion the array is\r\nsorted.\r\n\r\n\r\nQuickSort: The Algorithm\r\n\r\nInitially we will cal QuickSort(A,1,A.length).\r\nThe differences to merge sort are:\r\n\r\nWe split the array at , the\r\nposition of the pivot in sorted array, and we don’t know  in advance, instead it is revealed by\r\nPartition.\r\nThere is no combine step at the end.\r\nPartition plays a similar role to\r\nMerge.\r\n\r\nPartition(A,p,r)\r\nThe Partition function rearranges the subarray  in place using swaps,\r\nand it takes the last element \r\nas the pivot element.\r\nThe main ideas are:\r\n\r\nScan the subarray from left to right.\r\nBuild up a subarray  of\r\nelements smaller or equal to the pivot.\r\nBuild up a subarray \r\nof elements larger than the pivot.\r\nWhen reaching the end of the array, put the pivot in the right\r\nplace.\r\n\r\nThe pseudocode for Partition goes as follows: \r\nAn example run of the Partition algorithm:\r\n\r\nCorrectness\r\nWe prove the correctness of Partition by using the loop\r\ninvariant:\r\n\r\nLoop invariant: At the beginning of the -th iteration, we have  (let  denotes the pivot) and .\r\nInitialization: Since at the initialization  and , the invariant trivially\r\nholds.\r\nMaintanence:\r\n\r\nIf the if on line 4 is false, then  is greater than , and the algorithm will increate the\r\n to so that subarry  grows one element to\r\ncontain .\r\nIf the if on line 4 is true, then we swap  with  and then increase both  and  by 1.\r\nThis maintains the invariant.\r\n\r\nTermination: After the last swap in line 7,  and\r\nPartition returns the position of .\r\n\r\nRuntime\r\nThe runtime is straightforward to find:  where .\r\nWorst-case and Best-case\r\nPartitionings\r\nThe overall runtime depends on how the array is partitioned\r\nas that determines the sizes \r\nand  of the subarray to be\r\nsorted recursively.\r\nWorst-case Partitioning\r\nThe worst case is attained when Partition always produces one\r\nsubproblem with  and one with\r\n elements. This case can appear,\r\ne.g., when the array is sorted. This leads to the following recurrence:\r\n Solving this gives \r\nBest-case Partitioning\r\nThe best case is where we split the problem into two subproblems of\r\nsizes  and\r\n. By\r\nignoring floors, ceilings, and \r\nwe get the recurrence:  From the analysis of merge sort, the solution will be \r\nTowards an Average Case\r\nTo reach the average case, we assume that all elements of the array\r\nare distinct, and each split  was equally likely. This\r\nsituation occurs when the input is chosen uniformly at random\r\namongst all  possible orderings.\r\nThen  This is an average over all problem sizes for 2 subproblems\r\nplus . The solution of\r\nthis recurrence is  To prove this we use the substitution method:\r\nProof. We use induction on  to prove \r\n\r\nBase case: . And\r\nwe need to prove that : \r\nThis trivially holds. (e.g., for )\r\nInductive case: Assume fo every  we have  and prove for  that  The highlighted part is based on fact that when a summation\r\nhas the form ,\r\nwhere  is a monotonically\r\nincreasing function, then we can approximate it by integrals:  So we have  Q.E.D.\r\n\r\nImprovements to QuickSort\r\nQuick sort is often fast in practice because of the small constants\r\nin the asymptotic runtime. There are some improvements (not\r\ncomprehensive) we can apply on the quick sort:\r\n\r\nImprovements for handling equal values:\r\n\r\nPartition into smaller, equal, and larger elements.\r\nSo we only need to sort smaller and larger subarrays.\r\n\r\nChoose the pivot as median of 3 elements.\r\n\r\nSlightly faster in practice, but still has a quadratic worst-case\r\nruntime.\r\n\r\nDual-Pivot Quick Sort by Vladimir Yaroslavskiy.\r\n\r\nUse two pivots instead of one and partition array into 3 areas.\r\n\r\n\r\nA Randomized Version of\r\nQuickSort\r\nChoosing the right pivot element can be tricky since we have no idea\r\na priori which element can be a good pivot. A (surprisingly good)\r\nsolution is to leave it to chance:\r\n\r\n","tags":["Notes","Data Structures and Algorithms"]},{"title":"Probability and Statistics for Engineering Lecture 3-5 Notes","url":"/2025/10/15/pse-2/","content":"Random Variables and\r\nDistributions\r\nIntroduction\r\nDefinition of Random\r\nVariable\r\nTo simplify the complex problems into functional operations and unify\r\ntheir studys, we need to use the random variable.\r\n\n  \n    \n      Definition: Random Variable\r\n\n    \n    \n      A random variable (or r.v. for short) is a real-valued\r\nfunction defined on sample space (i.e. ), typically denoted\r\nby capital letters like .\r\n\n    \n  \r\n\n  \n    \n      Example\r\n\n    \n    \n      On Valentine’s Day a restaurant offers a Lucky Lovers discount. When\r\nthe waiter brings the check, he’ll also bring the four aces from a deck\r\nof cards. He’ll shuffle them and lay them out face down on the table.\r\nThe couple will then get to turn one card over.\r\n\r\nIf it’s a black ace, they’ll owe the full amount,\r\nbut if it’s the ace of hearts, the waiter will give them a $20 Lucky\r\nLovers discount.\r\nIf they first turn over the ace of diamonds, they’ll then get to\r\nturn over one of the remaining cards, earning a $10 discount for finding\r\nthe ace of hearts this time.\r\n\r\nHow to defined the Lucky Lovers discount as a random variable?\r\n\n    \n  \r\nSolution. It is not difficult to obtain the sample space of\r\nthe game of Lucky Lovers. Let H, D, C, S represent the aces of hearts,\r\ndiamonds, clubs, and spades, respectively, we have:  The Lucky Lovers discount can be defined as  It is a mapping from the outcomes in the sample space to\r\nnumbers on the real line.\r\nWe can observe that: with r.v.s defined, we simplify the study of\r\nrandom events to the study of r.v.s. The study of r.v.s. essentially\r\ninvolves examining all possible values that the r.v. can take and the\r\nprobability associated with each value, this is known as the\r\nprobability distribution. With the distribution, we can then\r\ngrasp the overall certainty of the random event, providing a foundation\r\nfor further study of its underlying regularity.\r\nBased on the possible values a r.v. can take, they can be classified\r\ninto:\r\n\r\nDiscrete random variable takes a finite or countable number\r\nof values.\r\nContinuous random variable takes continuous values.\r\n\r\nThe differences and similarities in how the probability distributions\r\nof these two types of r.v.s are described:\r\n\r\nDiscrete r.v. can be described using a probability mass\r\nfunction (PMF).\r\nContinuous r.v. can be described using a probability density\r\nfunction (PDF).\r\nBoth types of r.v.s can be described using a cumulative\r\ndistribution function (CDF).\r\n\r\n\n  \n    \n      Definition: Probability Mass Function\r\n\n    \n    \n      Let  be a discrete r.v.,  be the\r\nsupport of , i.e., the\r\nset of values that  can take. Then\r\nthe probability mass function (or PMF for short) of\r\n is  defined as  The\r\nfunction satisfies:\r\n\r\nNon-negativity: .\r\nNormalization: .\r\n\r\n\n    \n  \r\n\n  \n    \n      Example\r\n\n    \n    \n      Suppose that the support of a r.v.  is  and the PMF of  is\r\ngiven by ,\r\nwhere  is some positive\r\nconstant. Express the value of  in\r\nterms of .\r\n\n    \n  \r\nSolution. Using the normalization property of PMF, we have\r\n This implies that \r\nDescription of\r\nProbability Distribution\r\n\n  \n    \n      Definition: Discrete Random Variable and Probability Mass\r\nFunction\r\n\n    \n    \n       is called a continuous\r\nrandom variable if there exists a non-negative function , defined for all , satisfies that  for any\r\n. The\r\nfunction  is called the\r\nprobability density function (or PDF for short) of\r\n. Similar to PMF, the function\r\nsatisfies:\r\n\r\nNon-negativity: .\r\nNormalization: .\r\n\r\n\n    \n  \r\nIt’s obvious that for any , we have . Thus, different\r\nfrom the PMF,  does\r\nnot reflecct the probability of  taking the value of . Instead,  reflects the degress to which the\r\nprobability is concentrated around .\r\nIntuitively, the larger  is,\r\nthe greater the probability that \r\ntakes a value near .\r\n\n  \n    \n      Definition: Cumulative Distribution Function\r\n\n    \n    \n      For a r.v. , the cumulative\r\ndistribution function (or CDF for short) is defined as\r\n\r\n\r\nFor a discrete r.v., the CDF is a step function .\r\nFor a continuous r.v., the CDF is a continuous function .\r\nConsequently, .\r\nThe CDF is non-decreasing and\r\nright-continuous.\r\nThe maximum of the CDF is .\r\nThe minimum of the CDF is .\r\nFor any real numbers ,\r\n.\r\n\r\n\n    \n  \r\n\n  \n    \n      Example\r\n\n    \n    \n      Suppose that the lifespan in years of a certain household appliance\r\nis a r.v. with PDF given by  What is the\r\nprobability that an appliance will function between 1 and 2 years?\r\n\n    \n  \r\nSolution. By the normalization property of the PDF, we have\r\n Let  be the r.v.\r\nrepresenting the lifespan of a computer, then  ### Expectation and Variance Sometimes we need a simple,\r\nclear, distinctive description of a r.v. One of the most commonly used\r\nnumerical characteristics are the mathematical expectation and\r\nvariance.\r\n\n  \n    \n      Definition: Mathematical Expectation\r\n\n    \n    \n      The mathematical expectation (also known as the\r\nmean, expectation, or expected value) of a\r\nr.v.  is denoted as  and is defined as follows:\r\n\r\nIf  is a discrete r.v. with\r\nPMF , given\r\n,\r\nthen\r\n\r\n\r\n\r\nIf  is a continuous r.v. with\r\nPDF , given ,\r\nthen\r\n\r\n\r\n\n    \n  \r\nSimply put, the expectation is the weighted average of all\r\npossible values of a r.v. Expectation represents the average\r\nresult or long-term value that we can anticipate from a\r\nseries of random events.\r\n\n  \n    \n      Definition: Variance and Standard Deviation\r\n\n    \n    \n      If a r.v. satisfies that , then  is\r\ncalled the variance of ,\r\nand \r\nis called the standard deviation (or SD for short) of\r\n.\r\n\n    \n  \r\nSpecifically:\r\n\r\nIf  is discrete with PMF , then\r\n\r\nIf  is continuous with PDF\r\n, then  More generally, the expectation of any function of  is: \r\n\r\nThere are some basic properties of the expectation and variance:\r\n\r\nFor any constants , .\r\nFor any constants , , and\r\nthus .\r\n\r\n, for any functions  and .\r\n\r\nProof of property 3. w.l.o.g., show the case for a discrete\r\nr.v. with PMF .\r\nLet , then \r\nCommon Discrete\r\nDistributions\r\nBernoulli Distribution\r\n\n  \n    \n      Definition: Bernoulli Trial and Bernoulli Distribution\r\n\n    \n    \n      If a random experiment only have two possible outcomes  and , then the experiment is\r\ncalled a Bernoulli trial.\r\nIf a r.v.  only takes values\r\n and , and , , then  is called to follow a Bernoulli\r\ndistribution with parameter ,\r\ndenoted as  The PMF of\r\n can be\r\nexpressed as  The\r\nexpectation and variance of  is \r\n\n    \n  \r\nThe Bernoulli distribution is the foundation of many classical\r\nprobability distributions, such as the binomial distribution, the\r\ngeometric distribution, etc.\r\nBinomial Distribution\r\n\n  \n    \n      Definition: -Fold Bernoulli\r\nTrial and Binomial Distribution\r\n\n    \n    \n      An -fold Bernoulli\r\ntrial is an experiment that repeats a Bernoulli trial  times independently. Note:\r\n\r\nindependently suggests that the result of each Bernoulli\r\ntrial would not affect each other.\r\nrepeat suggests that the probability of event  in each Bernoulli trial, i.e., , remains the same.\r\n\r\nLet  be a r.v. that records the\r\nnumber of times event  happens in\r\nan -fold Bernoulli trial, then\r\n is called to follow a\r\nbinomial distribution with parameter  and , denoted as  The PMF of\r\n can be derived\r\nas \r\n\n    \n  \r\nThe expectation and variance of  can be derived\r\nto be \r\nProof. By the definition of expectation, we have: \r\nThe variance can also be derived similarly. More concise derivations\r\nare available once the independence between random variables is\r\nintroduced.\r\n\n  \n    \n      Example\r\n\n    \n    \n      A factory has 80 pieces of the same type of equipment, each operating\r\nindependently, with a failure probability of . A single maintainer can only repair\r\none piece of equipment at a time. The factory is considering two\r\nstrategies for allocating maintainers:\r\n\r\nAllocate 4 maintainers, with each responsible for maintaining 20\r\npieces of equipment.\r\nAllocate 3 maintainers, with them jointly responsible for\r\nmaintaining all 80 pieces of equipment.\r\n\r\nPlease compare these two strategies in terms of the probability that\r\na piece of equipment cannot be repaired in time when a failure\r\noccurs.\r\n\n    \n  \r\nSolution. For the first strategy, let  be the number of equipment that fail\r\nat the same time among the 20 pieces maintained by the four maintainers,\r\nrespectively. Then  Thus, the probability that a piece of equipment cannot be\r\nrepaired in time is  For the second strategy, let  be the number of equipment that fail at\r\nthe same time among the 80 pieces, then  Thus, the probability that a piece of equipement cannot be\r\nrepaired in time is  So the second strategy is more optimal.\r\nGeometric Distribution\r\n\n  \n    \n      Definition: Geometric Distribution\r\n\n    \n    \n      Suppose that a Bernoulli trial is repeated independently until  occurs, let  be a r.v. that records the number of\r\ntrials required, then  is called\r\nto follow a geometric distribution with parameter , denoted as  The PMF of\r\n can be derived\r\nas \r\nThe expectation and variance of  is \r\nThe geometric distribution is the only discrete distribution which has\r\nthe memoryless property, i.e., for  and any positive\r\nintegers , we have \r\n\n    \n  \r\nPoisson Distribution\r\n\n  \n    \n      Definition: Poisson Distribution\r\n\n    \n    \n      Let  be a discrete r.v. with\r\nsupport , if it PMF\r\nis \r\nwhere  is a constant,\r\nthen  is said to follow a\r\nPoisson distribution with parameter , denoted by \r\n\n    \n  \r\nPoisson distribution is used to describe the number of events\r\noccurring in a fixed interval of time/space if the event occur\r\nwith a constant rate and independently.\r\nThe expectation and variance of  can be\r\nderived to be  Proof. By the definition of expectation, we have:\r\n The parameter  is\r\nalso called the intensity of the Poisson distribution.\r\n\n  \n    \n      Example\r\n\n    \n    \n      A council is considering whether to base a recovery vehicle on a\r\nstretch of road to help clear incidents as quickly as possible. Records\r\nshow that, on average, the number of incidents during the moring rush\r\nhour is 5. The council won’t base a recovery vehicle on the road if the\r\nprobability of having more than 5 incidents during the morning rush hour\r\nis less than . Based on this\r\ninformation, should the council provide a vehicle?\r\n\n    \n  \r\nSolution. Let  be the\r\nnumber of incidents during the morning rush hour of a random day, then\r\n. The goal is\r\nto compute .\r\nSince:  Then \r\n\n  \n    \n      Tip\r\n\n    \n    \n      The Poisson distribution can be obtained derived as the limit of the\r\nbinomial distribution.\r\n\n    \n  \r\nProof. Consider the number of events within a unit time\r\ninterval  and divide it into\r\n subintervals:  Several assumptions are made:\r\n\r\nLet  be large so that each\r\nsubinterval is very short, making it impossible for two or more events\r\nto occur with the same subinterval.\r\nThe probability of an every occurring is proportional to the length\r\nof the subinterval, i.e., .\r\nWhether an event occurs in a subinterval is independent of the\r\nothers.\r\n\r\nLet  be the number of events\r\nwithin , by the assumption\r\nabove, we have .\r\nSo  Let , it is not\r\ndifficult to get  This is known as the Poisson theorem.\r\n\n  \n    \n      Tip\r\n\n    \n    \n      The theorem suggests that for an -fold Bernoulli trial with large  and small , the binomial distribution  can be approximated\r\nby the Poisson distribution .\r\nA rule of thumb is that when  and , the Poisson distribution\r\nprovides a good approximation to the binomial distribution.\r\n\n    \n  \r\n\n  \n    \n      Example\r\n\n    \n    \n      An insurance company has launched a life insurance policy where each\r\nparticipant is required to pay a premium of 12 dollars on January 1st.\r\nIf a participant dies within the year, his/her family can receive a\r\ncompensation of 2,000 dollars from the insurance company.\r\nSuppose 2,500 people participate in this insurance, and the\r\nprobability of death for each person within the year is 0.002.What is\r\nthe probability that the insurance company’s profit from this life\r\ninsurance policy is no less than 20,000 dollars?\r\n\n    \n  \r\nSolution. For the insurance company to make a profit no less\r\nthan 20000 dollars from the life insurance policy, the maximum number of\r\ndeaths is  Let  be the number of\r\ndeaths among the 2500 participants, then . We\r\nwould like to compute  Since  and , we apply the Poisson theorem:\r\n\r\nIn summary, we introduced the following discrete distributions:\r\nSummary\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nDistribution\r\nPMF\r\nExpectation\r\nVariance\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nCommon Continuous\r\nDistributions\r\nUniform Distribution\r\n\n  \n    \n      Definition: Uniform Distribution\r\n\n    \n    \n      If the PDF of a r.v.  is \r\nthen  is said to follow a uniform\r\ndistribution on , denoted as\r\n or simply\r\n.\r\nThe CDF of  is \r\n\n    \n  \r\nThe uniform distribution has a important property:  This only depends on the length but not the position of the\r\ninterval. This is known as the equal likelihood.\r\nThe expectation and variance of  can be derived to be  Proof: \r\nExponential Distribution\r\n\n  \n    \n      Definition: Exponential Distribution\r\n\n    \n    \n      If the PDF of a r.v.  is  then\r\n is said to follow an\r\nexponential distribution with parameter , denoted as . The CDF of\r\n is \r\n\n    \n  \r\nThe exponential distribution can be used to describe the distribution\r\nof the time intervals between events in a Poisson process:\r\nA Poisson process can be simply understood as a process where random\r\nevents occur independently and with a constant rate along the time axis.\r\nThe number of events occurring within a unit time interval follows , then the number\r\nof events occuring in  follows\r\n. Let\r\n be the time until the first event\r\noccurs, then  which means that . Similarly, we\r\ncan show that the time intervals between events independently\r\nfollow .\r\nThe expectation and variance of  can be derived\r\nto be  Proof: \r\nThe exponential distribution is the only continuous distribution with\r\nthe memoryless property, i.e., if , then for any\r\n we have  Proof: For any , since , it follows\r\nthat  Then, by the definition of conditional probability: \r\n\n  \n    \n      Tip\r\n\n    \n    \n      The ceiling of an exponential r.v. follows a geometric\r\ndistribution.\r\n\n    \n  \r\nNormal Distribution\r\n\n  \n    \n      Definition: Normal Distribution\r\n\n    \n    \n      If the PDF of a random variable  is \r\nthen  is said to follow a\r\nnormal distribution (also known as the Gaussian\r\ndistribution) with parameter  and  (), denoted as .\r\nSpecifically,  is the\r\nstandard normal distribution, with PDF \r\nThe CDF of  has no\r\nexplicit expression, however, it is used very often and thus expressed\r\nas : \r\n\n    \n  \r\nThe PDF of  is an\r\nelegant bell-shaped curve, symmetric about the parameter .\r\n\r\nThe larger the  is, the more\r\nright the PDF is located at.\r\nThe larger the  is, the\r\nflatter the PDF is.\r\n\r\nIf  and\r\ndefine r.v. ,\r\nthen . We call this the\r\nstandardization. Proof: Consider the CDF of :  By differentiation, the PDF of  is given by  which shows that .\r\nThe expectation and variance of  can be derived to be  Proof: Let , it suffices to\r\nprove , then\r\n\r\n\n  \n    \n      Example\r\n\n    \n    \n      A bus manufacturer is designing a bus. When determining the door\r\nheight, they must ensure that it is not too high but also allows 99% of\r\nmale passengers to pass through without bending. Assuming the height of\r\nall males follows a normal distribution , what should be the minimum\r\ndoor height to meet this requirement?\r\n\n    \n  \r\nSolution. Let  denote\r\nthe height of a randomly selected male,  be the door height of the bus. Then the\r\nrequirement can be expressed as . Checking the probability table of normal\r\ndistribution, we find that for , , so  So \r\nSummary\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nDistribution\r\nPDF\r\nExpectation\r\nVariance\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nTransformation of Random\r\nVariables\r\n\n  \n    \n      Definition: Transformation of Random Variables\r\n\n    \n    \n      For a discrete r.v.  with PMF\r\n, it is not difficult to\r\ndetermine the PMF of :  which consider both cases where  is bijective and not bijective.\r\nFor a continuous r.v.  with PDF\r\n, if  is a discrete r.v., then the PMF\r\nof  is  Otherwise, if  is\r\nalso a continuous r.v., then if  is a strictly monotonic\r\nfunction on the support of ,\r\nand it has a continuously-differentiable inverse function , then the PDF of  is \r\n\n    \n  \r\nProof for the continuous-to-continuous transformation.\r\nConsider the CDF of : . - If  is a strictly increasing\r\nfunction, then  and:\r\n - If  is a strictly\r\ndecreasing function, then  and: \r\n\n  \n    \n      Example\r\n\n    \n    \n      Consider the time it takes to transfer a file over a network depends\r\non the network speed , which vary\r\ndue to traffic and other conditions and .\r\nLet  denote the time required to\r\ntransfer a 100Mb file, please derive the PDF of .\r\n\n    \n  \r\nSolution. We have  and  Since  is a strictly\r\ndecreasing function on  and\r\nits inverse function  is continuously\r\ndifferentiable. So  Also, since ,\r\nthen . Then we have\r\n\r\nA famous application of r.v. transformation is based on the following\r\nresults:\r\n\n  \n    \n      Tip\r\n\n    \n    \n      If the CDF of a continuous r.v.  is  and its inverse function  exists. Define a r.v. , then .\r\nOn the other hand, if  is\r\nthe CDF of some r.v. and its inverse function  exists, let , then for  we have , i.e., the CDF of  is .\r\n\n    \n  \r\nProof. Consider the CDF of :  Since  is a\r\nnon-decreasing function and  exists, then for any :  This suggests that . The proof for\r\nthe second claim is similar.\r\nThe second result can be used in the inverse transform\r\nsampling, which is a widely used technique for generating random\r\nsamples from a complicated distribution.\r\nIf  is not a strictly\r\nmonotonic function on the support of , how to derive the PDF of ?\r\n\n  \n    \n      Example\r\n\n    \n    \n      Assume that r.v. , what is the PDF of ?\r\n\n    \n  \r\nSolution. Since , the PDF of  is  Although  is not\r\na monotonic function, it is strictly increasing on  and strictly decreasing on\r\n. For any , we have \r\nThe r.v.  follows a\r\ndistribution known as the chi-squared distribution.\r\n\n  \n    \n      Example\r\n\n    \n    \n      Consider the time it takes to transfer a file over a network depends\r\non the network speed , which vary\r\ndue to traffic and other conditions and .\r\nLet  denote the time required to\r\ntransfer a 100Mb file, calculate .\r\n\n    \n  \r\nSolution. By the definition of expectation, we have \r\nWe can also calculate the expectation of  using the PMF/PDF of  directly:\r\n\n  \n    \n      Tip\r\n\n    \n    \n      If  is a discrete r.v. with PMF\r\n, given\r\n,\r\nthen \r\nIf  is a continuous r.v. with PMF\r\n, given ,\r\nthen \r\n\n    \n  \r\nAlternative solution. \r\nWe generalize this kind of process to the relationship between  and . To describe such rule, we need a\r\nfew definition first.\r\n\n  \n    \n      Definition: Convex and Concave Function\r\n\n    \n    \n      A function  is\r\nsaid to be a convex function, if for any  and , we have  A\r\nfunction  is said to\r\nbe a concave function, if for any  and , we have \r\n\n    \n  \r\n\n  \n    \n      Definition: Jensen’s Inequality\r\n\n    \n    \n      Let  be a random variable,\r\nthen\r\n\r\nfor any convex function ,\r\n\r\n\r\n\r\nfor any concave function ,\r\n\r\n\r\n\n    \n  \r\nBy the Jensen’s inequality, we have\r\n\r\n\r\n\r\n\r\n\r\n\r\n","tags":["Notes","Probability and Statistics for Engineering"]}]